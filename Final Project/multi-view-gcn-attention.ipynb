{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch_geometric\nimport torch\nimport numpy as np\nfrom torch_geometric.datasets import Planetoid\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom torch_geometric.utils import dense_to_sparse\nimport networkx as nx\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-08-12T15:37:40.979320Z","iopub.execute_input":"2024-08-12T15:37:40.979733Z","iopub.status.idle":"2024-08-12T15:37:59.732235Z","shell.execute_reply.started":"2024-08-12T15:37:40.979693Z","shell.execute_reply":"2024-08-12T15:37:59.731076Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Step 1: Load the Dataset (Generalized for Cora, Citeseer, and Pubmed)\ndef load_dataset(name):\n    dataset = Planetoid(root=f'/tmp/{name}', name=name)\n    data = dataset[0]\n    return data\n\n# Step 2: Create the First View (Citation Graph)\ndef create_first_view(data):\n    # The first view is already provided by the dataset as the citation graph.\n    first_view_edge_index = data.edge_index\n    return first_view_edge_index\n\n# Step 3: Create the Second View (Feature Similarity Graph)\ndef create_second_view(data, threshold=0.7):\n    # Compute cosine similarity between node features\n    features = data.x.numpy()\n    similarity_matrix = cosine_similarity(features)\n    \n    # Apply a threshold to construct the graph\n    num_nodes = similarity_matrix.shape[0]\n    edge_index = []\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j and similarity_matrix[i, j] > threshold:\n                edge_index.append([i, j])\n    \n    second_view_edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    return second_view_edge_index\n\n# Step 4: Create the Third View (b-Matching Graph)\ndef create_third_view(data, k=10):\n    # Using NetworkX for b-Matching\n    G = nx.Graph()\n    features = data.x.numpy()\n    \n    # Add nodes\n    for i in range(features.shape[0]):\n        G.add_node(i)\n    \n    # Add edges based on similarity (without threshold, just top-k similar)\n    similarity_matrix = cosine_similarity(features)\n    for i in range(similarity_matrix.shape[0]):\n        similar_nodes = np.argsort(-similarity_matrix[i, :])[:k]\n        for j in similar_nodes:\n            if i != j:\n                G.add_edge(i, j, weight=similarity_matrix[i, j])\n    \n    # Convert to PyTorch geometric format\n    adj_matrix = nx.adjacency_matrix(G).todense()\n    third_view_edge_index, _ = dense_to_sparse(torch.tensor(adj_matrix, dtype=torch.float))\n    return third_view_edge_index\n\n# Step 5: Preprocess Dataset (Generalized for Cora, Citeseer, and Pubmed)\ndef preprocess_dataset(name):\n    data = load_dataset(name)\n    \n    # Generate the views\n    first_view = create_first_view(data)\n    second_view = create_second_view(data, threshold=0.7)\n    third_view = create_third_view(data, k=10)\n    \n    # Store processed data in a dictionary\n    processed_data = {\n        'first_view': first_view,\n        'second_view': second_view,\n        'third_view': third_view,\n        'features': data.x,\n        'labels': data.y,\n        'train_mask': data.train_mask,\n        'val_mask': data.val_mask,\n        'test_mask': data.test_mask\n    }\n\n    return processed_data\n\n# Step 6: Execute Preprocessing and Store in a Dictionary for All Datasets\nif __name__ == \"__main__\":\n    all_datasets = {}\n    for dataset_name in ['Cora', 'Citeseer', 'Pubmed']:\n        print(f\"Processing {dataset_name} dataset...\")\n        processed_data = preprocess_dataset(dataset_name)\n        all_datasets[dataset_name] = processed_data\n\n    print(\"All datasets have been processed and stored in the 'all_datasets' dictionary.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-12T15:37:59.735004Z","iopub.execute_input":"2024-08-12T15:37:59.735817Z","iopub.status.idle":"2024-08-12T15:51:10.422246Z","shell.execute_reply.started":"2024-08-12T15:37:59.735769Z","shell.execute_reply":"2024-08-12T15:51:10.421170Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Processing Cora dataset...\n","output_type":"stream"},{"name":"stderr","text":"Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\nProcessing...\nDone!\n","output_type":"stream"},{"name":"stdout","text":"Processing Citeseer dataset...\n","output_type":"stream"},{"name":"stderr","text":"Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\nProcessing...\nDone!\n","output_type":"stream"},{"name":"stdout","text":"Processing Pubmed dataset...\n","output_type":"stream"},{"name":"stderr","text":"Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\nProcessing...\nDone!\n","output_type":"stream"},{"name":"stdout","text":"All datasets have been processed and stored in the 'all_datasets' dictionary.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.data import Data\nimport torch_geometric\nfrom torch_geometric.utils import add_self_loops, degree","metadata":{"execution":{"iopub.status.busy":"2024-08-12T15:54:21.192929Z","iopub.execute_input":"2024-08-12T15:54:21.193370Z","iopub.status.idle":"2024-08-12T15:54:21.199472Z","shell.execute_reply.started":"2024-08-12T15:54:21.193341Z","shell.execute_reply":"2024-08-12T15:54:21.198197Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.utils import add_self_loops, degree\nfrom torch_geometric.utils import scatter\nimport torch.nn.init as init\nimport torch.nn as nn\n\nclass MAGCN(torch.nn.Module):\n    def __init__(self, num_features, num_classes, num_views, hidden_dim=16, dropout=0.5):\n        super(MAGCN, self).__init__()\n        self.num_views = num_views\n        self.dropout = dropout\n\n        self.gcn_unfold = torch.nn.ModuleList([\n            GCNConv(num_features, hidden_dim) for _ in range(num_views)\n        ])\n        self.relu = nn.ReLU()\n        self.dropout_layer = nn.Dropout(self.dropout)\n        \n        # MLP for attention mechanism with dropout\n        self.attention_mlp = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, 6),\n            torch.nn.ReLU(),\n            nn.Dropout(self.dropout),  # Apply dropout\n            torch.nn.Linear(6, 3),\n            torch.nn.ReLU(),\n            nn.Dropout(self.dropout),  # Apply dropout\n            torch.nn.Linear(3, num_views)\n        )\n        \n        # Final GCN layer after merging with dropout\n        self.gcn_merge = GCNConv(hidden_dim, num_classes)\n        self.final_dropout = nn.Dropout(self.dropout)  # Apply dropout after the final GCN layer\n\n        # Initialize weights for Linear and GCNConv layers using Glorot (Xavier) initialization\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, torch.nn.Linear):\n                init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    init.zeros_(m.bias)\n            elif isinstance(m, GCNConv):\n                init.xavier_uniform_(m.lin.weight)\n                if m.lin.bias is not None:\n                    init.zeros_(m.lin.bias)\n\n    def graph_gap(self, x, edge_index):\n        \"\"\"Graph Global Average Pooling operation\"\"\"\n        # Add self-loops to ensure self-connections\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        \n        # Get the degree of each node (including self-loops)\n        row, col = edge_index\n        deg = degree(row, x.size(0), dtype=x.dtype) + 1  # Degree + self-loop\n        \n        # Normalize node features by the degree (mean aggregation)\n        deg_inv = deg.pow(-1).view(-1, 1)\n        x = deg_inv * x\n        \n        # Sum the features from the neighbors\n        out = scatter(x[col], row, dim=0, dim_size=x.size(0), reduce='add')\n        \n        return out\n\n    def forward(self, data):\n        # Step 1: Apply GCN to each view\n        view_outputs = []\n        for i in range(self.num_views):\n            x = self.gcn_unfold[i](data.x, data.view_edge_index[i])\n            x = self.relu(x)\n            x = self.dropout_layer(x)\n            view_outputs.append(x)\n\n        # Step 2: Apply Graph GAP for each view\n        gap_outputs = []\n        for i in range(self.num_views):\n            gap_output = self.graph_gap(view_outputs[i], data.view_edge_index[i])\n            gap_outputs.append(gap_output)\n\n        gap_outputs = torch.stack(gap_outputs, dim=0)  # Shape: (num_views, num_nodes, hidden_dim)\n\n        # Step 3: Compute attention scores using MLP\n        pooled_gap = torch.mean(gap_outputs, dim=0)  # Shape: (num_nodes, hidden_dim)\n\n        attention_scores = self.attention_mlp(pooled_gap)  # Shape: (num_nodes, num_views)\n\n        # Normalize attention scores across views for each node\n        attention_scores = F.softmax(attention_scores, dim=1)  # Shape: (num_nodes, num_views)\n\n        # Step 4: Apply attention to the view outputs\n        attention_scores = attention_scores.unsqueeze(-1)  # Shape: (num_nodes, num_views, 1)\n        weighted_output = torch.sum(attention_scores * gap_outputs.permute(1, 0, 2), dim=1)  # Shape: (num_nodes, hidden_dim)\n\n        # Step 5: Merge the views with final GCN\n        out = self.gcn_merge(weighted_output, data.view_edge_index[0])  # Use first view's edges for merging\n        out = self.final_dropout(out)\n        return F.log_softmax(out, dim=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:17:22.738354Z","iopub.execute_input":"2024-08-12T17:17:22.738759Z","iopub.status.idle":"2024-08-12T17:17:22.758109Z","shell.execute_reply.started":"2024-08-12T17:17:22.738730Z","shell.execute_reply":"2024-08-12T17:17:22.756983Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data\nimport os\n\n# Function to train the MAGCN model with early stopping\ndef train_model(model, data, optimizer, epochs=2000, early_stopping_patience=100, model_save_path=\"best_model.pth\"):\n    model.train()\n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        out = model(data)\n        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n        loss.backward()\n        optimizer.step()\n\n        # Train accuracy\n        _, pred_train = out[data.train_mask].max(dim=1)\n        correct_train = int((pred_train == data.y[data.train_mask]).sum())\n        train_acc = correct_train / int(data.train_mask.sum())\n\n        # Validation step\n        model.eval()  # Switch to evaluation mode\n        with torch.no_grad():\n            val_out = model(data)\n            val_loss = F.nll_loss(val_out[data.val_mask], data.y[data.val_mask])\n\n            # Validation accuracy\n            _, pred_val = val_out[data.val_mask].max(dim=1)\n            correct_val = int((pred_val == data.y[data.val_mask]).sum())\n            val_acc = correct_val / int(data.val_mask.sum())\n        \n        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Acc: {train_acc:.4f}, Val_Loss: {val_loss.item():.4f}, Val_Acc: {val_acc:.4f}')\n\n        # Early stopping and save the best model\n        if val_loss.item() < best_val_loss:\n            best_val_loss = val_loss.item()\n            patience_counter = 0\n            best_model_state = model.state_dict()\n\n            # Save the best model state to a file\n            torch.save(best_model_state, model_save_path)\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= early_stopping_patience:\n            print(\"Early stopping triggered.\")\n            break\n\n        model.train()  # Switch back to training mode\n    \n    # Load the best model after training\n    model.load_state_dict(torch.load(model_save_path))\n    print(f\"Best model loaded from {model_save_path}\")\n\n# Function to test the MAGCN model\ndef test_model(model, data):\n    model.eval()\n    _, pred = model(data).max(dim=1)\n    correct = int((pred[data.test_mask] == data.y[data.test_mask]).sum())\n    acc = correct / int(data.test_mask.sum())\n    print(f'Test Accuracy: {acc:.4f}')\n\n# Modified data loading to read from dictionary and structure it for MAGCN\ndef load_processed_data_from_dict(dataset_dict):\n    return Data(\n        x=dataset_dict['features'],\n        y=dataset_dict['labels'],\n        train_mask=dataset_dict['train_mask'],\n        val_mask=dataset_dict['val_mask'],\n        test_mask=dataset_dict['test_mask'],\n        view_edge_index=[dataset_dict['first_view'], dataset_dict['second_view'], dataset_dict['third_view']]\n    )\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:17:23.376813Z","iopub.execute_input":"2024-08-12T17:17:23.377200Z","iopub.status.idle":"2024-08-12T17:17:23.391952Z","shell.execute_reply.started":"2024-08-12T17:17:23.377173Z","shell.execute_reply":"2024-08-12T17:17:23.390800Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n    # Assume all_datasets dictionary is already populated as in the previous steps\n    dataset_name = 'Cora'  # Change this to Citeseer or Pubmed as needed\n    data = load_processed_data_from_dict(all_datasets[dataset_name])\n\n    # Model configuration based on the paper's setup\n    num_features = data.x.size(1)\n    num_classes = int(data.y.max()) + 1\n    hidden_dim = 16  # Set the hidden dimension to 16 as per the paper\n    num_views = 3\n\n    # Initialize the model with the specified architecture and dimensions\n    model = MAGCN(num_features, num_classes, num_views, hidden_dim)\n    \n    # Use Adam optimizer with the specified learning rate and weight decay\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n\n    # Train and test the model\n    train_model(model, data, optimizer, epochs=2000, early_stopping_patience=1000)\n    test_model(model, data)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:17:57.785663Z","iopub.execute_input":"2024-08-12T17:17:57.786083Z","iopub.status.idle":"2024-08-12T17:19:04.277370Z","shell.execute_reply.started":"2024-08-12T17:17:57.786050Z","shell.execute_reply":"2024-08-12T17:19:04.276192Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.9446, Acc: 0.1714, Val_Loss: 1.9183, Val_Acc: 0.3000\nEpoch 2, Loss: 1.9132, Acc: 0.2500, Val_Loss: 1.8755, Val_Acc: 0.5720\nEpoch 3, Loss: 1.8242, Acc: 0.4214, Val_Loss: 1.8209, Val_Acc: 0.6720\nEpoch 4, Loss: 1.7822, Acc: 0.4214, Val_Loss: 1.7661, Val_Acc: 0.6900\nEpoch 5, Loss: 1.7421, Acc: 0.3643, Val_Loss: 1.7084, Val_Acc: 0.7240\nEpoch 6, Loss: 1.6455, Acc: 0.4714, Val_Loss: 1.6558, Val_Acc: 0.7540\nEpoch 7, Loss: 1.6250, Acc: 0.4286, Val_Loss: 1.6135, Val_Acc: 0.7660\nEpoch 8, Loss: 1.5155, Acc: 0.4714, Val_Loss: 1.5650, Val_Acc: 0.7600\nEpoch 9, Loss: 1.4300, Acc: 0.5071, Val_Loss: 1.5218, Val_Acc: 0.7360\nEpoch 10, Loss: 1.3775, Acc: 0.5357, Val_Loss: 1.4697, Val_Acc: 0.7360\nEpoch 11, Loss: 1.3929, Acc: 0.4857, Val_Loss: 1.4155, Val_Acc: 0.7520\nEpoch 12, Loss: 1.2936, Acc: 0.5000, Val_Loss: 1.3511, Val_Acc: 0.7720\nEpoch 13, Loss: 1.2589, Acc: 0.5000, Val_Loss: 1.2871, Val_Acc: 0.7840\nEpoch 14, Loss: 1.2286, Acc: 0.4714, Val_Loss: 1.2314, Val_Acc: 0.7900\nEpoch 15, Loss: 1.0751, Acc: 0.5714, Val_Loss: 1.1772, Val_Acc: 0.7860\nEpoch 16, Loss: 1.0941, Acc: 0.5857, Val_Loss: 1.1266, Val_Acc: 0.7940\nEpoch 17, Loss: 1.0486, Acc: 0.5643, Val_Loss: 1.0812, Val_Acc: 0.7900\nEpoch 18, Loss: 1.0622, Acc: 0.5286, Val_Loss: 1.0423, Val_Acc: 0.7880\nEpoch 19, Loss: 1.0147, Acc: 0.5714, Val_Loss: 1.0140, Val_Acc: 0.7840\nEpoch 20, Loss: 1.0100, Acc: 0.5571, Val_Loss: 0.9942, Val_Acc: 0.7800\nEpoch 21, Loss: 0.9420, Acc: 0.5786, Val_Loss: 0.9730, Val_Acc: 0.7740\nEpoch 22, Loss: 1.0790, Acc: 0.5500, Val_Loss: 0.9417, Val_Acc: 0.7780\nEpoch 23, Loss: 0.9447, Acc: 0.6000, Val_Loss: 0.9034, Val_Acc: 0.7800\nEpoch 24, Loss: 0.9131, Acc: 0.5643, Val_Loss: 0.8704, Val_Acc: 0.7800\nEpoch 25, Loss: 0.8395, Acc: 0.6357, Val_Loss: 0.8380, Val_Acc: 0.7820\nEpoch 26, Loss: 0.8638, Acc: 0.5786, Val_Loss: 0.8113, Val_Acc: 0.7760\nEpoch 27, Loss: 0.8608, Acc: 0.6286, Val_Loss: 0.7938, Val_Acc: 0.7660\nEpoch 28, Loss: 0.8658, Acc: 0.6071, Val_Loss: 0.7787, Val_Acc: 0.7700\nEpoch 29, Loss: 0.8983, Acc: 0.5857, Val_Loss: 0.7681, Val_Acc: 0.7680\nEpoch 30, Loss: 0.9192, Acc: 0.5643, Val_Loss: 0.7645, Val_Acc: 0.7760\nEpoch 31, Loss: 0.8452, Acc: 0.6214, Val_Loss: 0.7637, Val_Acc: 0.7800\nEpoch 32, Loss: 0.7429, Acc: 0.7000, Val_Loss: 0.7642, Val_Acc: 0.7780\nEpoch 33, Loss: 0.8561, Acc: 0.5643, Val_Loss: 0.7659, Val_Acc: 0.7760\nEpoch 34, Loss: 0.8049, Acc: 0.6286, Val_Loss: 0.7635, Val_Acc: 0.7800\nEpoch 35, Loss: 0.6963, Acc: 0.6643, Val_Loss: 0.7659, Val_Acc: 0.7780\nEpoch 36, Loss: 0.8451, Acc: 0.6071, Val_Loss: 0.7607, Val_Acc: 0.7740\nEpoch 37, Loss: 0.7910, Acc: 0.6286, Val_Loss: 0.7518, Val_Acc: 0.7720\nEpoch 38, Loss: 0.8577, Acc: 0.5857, Val_Loss: 0.7438, Val_Acc: 0.7720\nEpoch 39, Loss: 0.7857, Acc: 0.6000, Val_Loss: 0.7361, Val_Acc: 0.7700\nEpoch 40, Loss: 0.8359, Acc: 0.6000, Val_Loss: 0.7281, Val_Acc: 0.7760\nEpoch 41, Loss: 0.7547, Acc: 0.6214, Val_Loss: 0.7219, Val_Acc: 0.7680\nEpoch 42, Loss: 0.7363, Acc: 0.6071, Val_Loss: 0.7201, Val_Acc: 0.7700\nEpoch 43, Loss: 0.7644, Acc: 0.6286, Val_Loss: 0.7221, Val_Acc: 0.7700\nEpoch 44, Loss: 0.7580, Acc: 0.6357, Val_Loss: 0.7264, Val_Acc: 0.7720\nEpoch 45, Loss: 0.7298, Acc: 0.6429, Val_Loss: 0.7368, Val_Acc: 0.7720\nEpoch 46, Loss: 0.7615, Acc: 0.6143, Val_Loss: 0.7508, Val_Acc: 0.7740\nEpoch 47, Loss: 0.7657, Acc: 0.6000, Val_Loss: 0.7659, Val_Acc: 0.7780\nEpoch 48, Loss: 0.7661, Acc: 0.6357, Val_Loss: 0.7730, Val_Acc: 0.7780\nEpoch 49, Loss: 0.8052, Acc: 0.6000, Val_Loss: 0.7695, Val_Acc: 0.7740\nEpoch 50, Loss: 0.7326, Acc: 0.6286, Val_Loss: 0.7668, Val_Acc: 0.7720\nEpoch 51, Loss: 0.6625, Acc: 0.6714, Val_Loss: 0.7594, Val_Acc: 0.7680\nEpoch 52, Loss: 0.6551, Acc: 0.6643, Val_Loss: 0.7569, Val_Acc: 0.7720\nEpoch 53, Loss: 0.7015, Acc: 0.6643, Val_Loss: 0.7551, Val_Acc: 0.7720\nEpoch 54, Loss: 0.7855, Acc: 0.5857, Val_Loss: 0.7542, Val_Acc: 0.7700\nEpoch 55, Loss: 0.7636, Acc: 0.6071, Val_Loss: 0.7550, Val_Acc: 0.7660\nEpoch 56, Loss: 0.7165, Acc: 0.6429, Val_Loss: 0.7531, Val_Acc: 0.7720\nEpoch 57, Loss: 0.7154, Acc: 0.6500, Val_Loss: 0.7454, Val_Acc: 0.7700\nEpoch 58, Loss: 0.7375, Acc: 0.6286, Val_Loss: 0.7352, Val_Acc: 0.7720\nEpoch 59, Loss: 0.6957, Acc: 0.6786, Val_Loss: 0.7241, Val_Acc: 0.7740\nEpoch 60, Loss: 0.7545, Acc: 0.5929, Val_Loss: 0.7160, Val_Acc: 0.7780\nEpoch 61, Loss: 0.6834, Acc: 0.6857, Val_Loss: 0.7119, Val_Acc: 0.7800\nEpoch 62, Loss: 0.8570, Acc: 0.5214, Val_Loss: 0.7104, Val_Acc: 0.7820\nEpoch 63, Loss: 0.7137, Acc: 0.6000, Val_Loss: 0.7091, Val_Acc: 0.7860\nEpoch 64, Loss: 0.6869, Acc: 0.6286, Val_Loss: 0.7099, Val_Acc: 0.7840\nEpoch 65, Loss: 0.8776, Acc: 0.5643, Val_Loss: 0.7165, Val_Acc: 0.7860\nEpoch 66, Loss: 0.7419, Acc: 0.6714, Val_Loss: 0.7241, Val_Acc: 0.7860\nEpoch 67, Loss: 0.7512, Acc: 0.6071, Val_Loss: 0.7347, Val_Acc: 0.7840\nEpoch 68, Loss: 0.7747, Acc: 0.5857, Val_Loss: 0.7483, Val_Acc: 0.7800\nEpoch 69, Loss: 0.6013, Acc: 0.7000, Val_Loss: 0.7608, Val_Acc: 0.7780\nEpoch 70, Loss: 0.7671, Acc: 0.6357, Val_Loss: 0.7632, Val_Acc: 0.7780\nEpoch 71, Loss: 0.6602, Acc: 0.7071, Val_Loss: 0.7588, Val_Acc: 0.7760\nEpoch 72, Loss: 0.7428, Acc: 0.6643, Val_Loss: 0.7581, Val_Acc: 0.7740\nEpoch 73, Loss: 0.8087, Acc: 0.6143, Val_Loss: 0.7539, Val_Acc: 0.7660\nEpoch 74, Loss: 0.7632, Acc: 0.6357, Val_Loss: 0.7544, Val_Acc: 0.7620\nEpoch 75, Loss: 0.7058, Acc: 0.5786, Val_Loss: 0.7542, Val_Acc: 0.7660\nEpoch 76, Loss: 0.7169, Acc: 0.6429, Val_Loss: 0.7492, Val_Acc: 0.7680\nEpoch 77, Loss: 0.7309, Acc: 0.6714, Val_Loss: 0.7495, Val_Acc: 0.7680\nEpoch 78, Loss: 0.6721, Acc: 0.6357, Val_Loss: 0.7492, Val_Acc: 0.7680\nEpoch 79, Loss: 0.7410, Acc: 0.6000, Val_Loss: 0.7459, Val_Acc: 0.7700\nEpoch 80, Loss: 0.7086, Acc: 0.6929, Val_Loss: 0.7411, Val_Acc: 0.7700\nEpoch 81, Loss: 0.7180, Acc: 0.6286, Val_Loss: 0.7397, Val_Acc: 0.7700\nEpoch 82, Loss: 0.6878, Acc: 0.6571, Val_Loss: 0.7409, Val_Acc: 0.7760\nEpoch 83, Loss: 0.7392, Acc: 0.6000, Val_Loss: 0.7424, Val_Acc: 0.7780\nEpoch 84, Loss: 0.7868, Acc: 0.5714, Val_Loss: 0.7422, Val_Acc: 0.7820\nEpoch 85, Loss: 0.7882, Acc: 0.5929, Val_Loss: 0.7410, Val_Acc: 0.7820\nEpoch 86, Loss: 0.6938, Acc: 0.6286, Val_Loss: 0.7401, Val_Acc: 0.7820\nEpoch 87, Loss: 0.7578, Acc: 0.6429, Val_Loss: 0.7390, Val_Acc: 0.7820\nEpoch 88, Loss: 0.6283, Acc: 0.7000, Val_Loss: 0.7370, Val_Acc: 0.7800\nEpoch 89, Loss: 0.7745, Acc: 0.5786, Val_Loss: 0.7327, Val_Acc: 0.7780\nEpoch 90, Loss: 0.6468, Acc: 0.6857, Val_Loss: 0.7261, Val_Acc: 0.7720\nEpoch 91, Loss: 0.6888, Acc: 0.6500, Val_Loss: 0.7233, Val_Acc: 0.7700\nEpoch 92, Loss: 0.7411, Acc: 0.6143, Val_Loss: 0.7243, Val_Acc: 0.7700\nEpoch 93, Loss: 0.7520, Acc: 0.6143, Val_Loss: 0.7235, Val_Acc: 0.7680\nEpoch 94, Loss: 0.8084, Acc: 0.5786, Val_Loss: 0.7228, Val_Acc: 0.7680\nEpoch 95, Loss: 0.7533, Acc: 0.6214, Val_Loss: 0.7267, Val_Acc: 0.7700\nEpoch 96, Loss: 0.8064, Acc: 0.6000, Val_Loss: 0.7331, Val_Acc: 0.7740\nEpoch 97, Loss: 0.6794, Acc: 0.6500, Val_Loss: 0.7378, Val_Acc: 0.7760\nEpoch 98, Loss: 0.7559, Acc: 0.6643, Val_Loss: 0.7430, Val_Acc: 0.7760\nEpoch 99, Loss: 0.6659, Acc: 0.6929, Val_Loss: 0.7434, Val_Acc: 0.7720\nEpoch 100, Loss: 0.7964, Acc: 0.6000, Val_Loss: 0.7477, Val_Acc: 0.7740\nEpoch 101, Loss: 0.7192, Acc: 0.6143, Val_Loss: 0.7529, Val_Acc: 0.7740\nEpoch 102, Loss: 0.7190, Acc: 0.6286, Val_Loss: 0.7556, Val_Acc: 0.7760\nEpoch 103, Loss: 0.7270, Acc: 0.6357, Val_Loss: 0.7507, Val_Acc: 0.7760\nEpoch 104, Loss: 0.8112, Acc: 0.5643, Val_Loss: 0.7491, Val_Acc: 0.7720\nEpoch 105, Loss: 0.7635, Acc: 0.6500, Val_Loss: 0.7488, Val_Acc: 0.7680\nEpoch 106, Loss: 0.7946, Acc: 0.5786, Val_Loss: 0.7452, Val_Acc: 0.7720\nEpoch 107, Loss: 0.8509, Acc: 0.5929, Val_Loss: 0.7446, Val_Acc: 0.7680\nEpoch 108, Loss: 0.6879, Acc: 0.6929, Val_Loss: 0.7464, Val_Acc: 0.7680\nEpoch 109, Loss: 0.7545, Acc: 0.6071, Val_Loss: 0.7478, Val_Acc: 0.7640\nEpoch 110, Loss: 0.6994, Acc: 0.6643, Val_Loss: 0.7496, Val_Acc: 0.7640\nEpoch 111, Loss: 0.6948, Acc: 0.6214, Val_Loss: 0.7512, Val_Acc: 0.7620\nEpoch 112, Loss: 0.7364, Acc: 0.6000, Val_Loss: 0.7601, Val_Acc: 0.7620\nEpoch 113, Loss: 0.7701, Acc: 0.6143, Val_Loss: 0.7713, Val_Acc: 0.7600\nEpoch 114, Loss: 0.7389, Acc: 0.5857, Val_Loss: 0.7776, Val_Acc: 0.7700\nEpoch 115, Loss: 0.7510, Acc: 0.6286, Val_Loss: 0.7802, Val_Acc: 0.7640\nEpoch 116, Loss: 0.7536, Acc: 0.5929, Val_Loss: 0.7788, Val_Acc: 0.7660\nEpoch 117, Loss: 0.5820, Acc: 0.7286, Val_Loss: 0.7760, Val_Acc: 0.7620\nEpoch 118, Loss: 0.7670, Acc: 0.6071, Val_Loss: 0.7687, Val_Acc: 0.7640\nEpoch 119, Loss: 0.6676, Acc: 0.6357, Val_Loss: 0.7590, Val_Acc: 0.7600\nEpoch 120, Loss: 0.6711, Acc: 0.6429, Val_Loss: 0.7478, Val_Acc: 0.7600\nEpoch 121, Loss: 0.6928, Acc: 0.7143, Val_Loss: 0.7379, Val_Acc: 0.7640\nEpoch 122, Loss: 0.7371, Acc: 0.6286, Val_Loss: 0.7328, Val_Acc: 0.7560\nEpoch 123, Loss: 0.6364, Acc: 0.6643, Val_Loss: 0.7316, Val_Acc: 0.7600\nEpoch 124, Loss: 0.6305, Acc: 0.6500, Val_Loss: 0.7307, Val_Acc: 0.7600\nEpoch 125, Loss: 0.6973, Acc: 0.6429, Val_Loss: 0.7305, Val_Acc: 0.7660\nEpoch 126, Loss: 0.7298, Acc: 0.6214, Val_Loss: 0.7335, Val_Acc: 0.7640\nEpoch 127, Loss: 0.7560, Acc: 0.6714, Val_Loss: 0.7370, Val_Acc: 0.7720\nEpoch 128, Loss: 0.6730, Acc: 0.6714, Val_Loss: 0.7397, Val_Acc: 0.7700\nEpoch 129, Loss: 0.6490, Acc: 0.6500, Val_Loss: 0.7418, Val_Acc: 0.7740\nEpoch 130, Loss: 0.7852, Acc: 0.5786, Val_Loss: 0.7408, Val_Acc: 0.7700\nEpoch 131, Loss: 0.7843, Acc: 0.5929, Val_Loss: 0.7432, Val_Acc: 0.7680\nEpoch 132, Loss: 0.6513, Acc: 0.6714, Val_Loss: 0.7453, Val_Acc: 0.7640\nEpoch 133, Loss: 0.6554, Acc: 0.6714, Val_Loss: 0.7468, Val_Acc: 0.7540\nEpoch 134, Loss: 0.6431, Acc: 0.7286, Val_Loss: 0.7511, Val_Acc: 0.7620\nEpoch 135, Loss: 0.6485, Acc: 0.6929, Val_Loss: 0.7556, Val_Acc: 0.7640\nEpoch 136, Loss: 0.6517, Acc: 0.6500, Val_Loss: 0.7617, Val_Acc: 0.7560\nEpoch 137, Loss: 0.5220, Acc: 0.7214, Val_Loss: 0.7604, Val_Acc: 0.7600\nEpoch 138, Loss: 0.7554, Acc: 0.6143, Val_Loss: 0.7572, Val_Acc: 0.7620\nEpoch 139, Loss: 0.7373, Acc: 0.6429, Val_Loss: 0.7529, Val_Acc: 0.7660\nEpoch 140, Loss: 0.7275, Acc: 0.6571, Val_Loss: 0.7493, Val_Acc: 0.7720\nEpoch 141, Loss: 0.7413, Acc: 0.6357, Val_Loss: 0.7398, Val_Acc: 0.7780\nEpoch 142, Loss: 0.8192, Acc: 0.6143, Val_Loss: 0.7283, Val_Acc: 0.7780\nEpoch 143, Loss: 0.7193, Acc: 0.6000, Val_Loss: 0.7233, Val_Acc: 0.7760\nEpoch 144, Loss: 0.5633, Acc: 0.7143, Val_Loss: 0.7193, Val_Acc: 0.7760\nEpoch 145, Loss: 0.6499, Acc: 0.6357, Val_Loss: 0.7197, Val_Acc: 0.7720\nEpoch 146, Loss: 0.7599, Acc: 0.6286, Val_Loss: 0.7242, Val_Acc: 0.7760\nEpoch 147, Loss: 0.6540, Acc: 0.6643, Val_Loss: 0.7279, Val_Acc: 0.7720\nEpoch 148, Loss: 0.7034, Acc: 0.6571, Val_Loss: 0.7315, Val_Acc: 0.7700\nEpoch 149, Loss: 0.5644, Acc: 0.6786, Val_Loss: 0.7373, Val_Acc: 0.7680\nEpoch 150, Loss: 0.6899, Acc: 0.6214, Val_Loss: 0.7451, Val_Acc: 0.7600\nEpoch 151, Loss: 0.7341, Acc: 0.6429, Val_Loss: 0.7539, Val_Acc: 0.7620\nEpoch 152, Loss: 0.7398, Acc: 0.6143, Val_Loss: 0.7635, Val_Acc: 0.7640\nEpoch 153, Loss: 0.8022, Acc: 0.6143, Val_Loss: 0.7729, Val_Acc: 0.7660\nEpoch 154, Loss: 0.6087, Acc: 0.6857, Val_Loss: 0.7763, Val_Acc: 0.7660\nEpoch 155, Loss: 0.6495, Acc: 0.6714, Val_Loss: 0.7743, Val_Acc: 0.7680\nEpoch 156, Loss: 0.6927, Acc: 0.6429, Val_Loss: 0.7698, Val_Acc: 0.7700\nEpoch 157, Loss: 0.6911, Acc: 0.6643, Val_Loss: 0.7657, Val_Acc: 0.7660\nEpoch 158, Loss: 0.6903, Acc: 0.6286, Val_Loss: 0.7603, Val_Acc: 0.7760\nEpoch 159, Loss: 0.7048, Acc: 0.6500, Val_Loss: 0.7586, Val_Acc: 0.7780\nEpoch 160, Loss: 0.6751, Acc: 0.6429, Val_Loss: 0.7574, Val_Acc: 0.7740\nEpoch 161, Loss: 0.7072, Acc: 0.6143, Val_Loss: 0.7553, Val_Acc: 0.7780\nEpoch 162, Loss: 0.7093, Acc: 0.6500, Val_Loss: 0.7539, Val_Acc: 0.7760\nEpoch 163, Loss: 0.6823, Acc: 0.6286, Val_Loss: 0.7516, Val_Acc: 0.7760\nEpoch 164, Loss: 0.6547, Acc: 0.6643, Val_Loss: 0.7452, Val_Acc: 0.7760\nEpoch 165, Loss: 0.7115, Acc: 0.6429, Val_Loss: 0.7367, Val_Acc: 0.7720\nEpoch 166, Loss: 0.6754, Acc: 0.6143, Val_Loss: 0.7330, Val_Acc: 0.7660\nEpoch 167, Loss: 0.6263, Acc: 0.6500, Val_Loss: 0.7316, Val_Acc: 0.7640\nEpoch 168, Loss: 0.6950, Acc: 0.6429, Val_Loss: 0.7351, Val_Acc: 0.7620\nEpoch 169, Loss: 0.7626, Acc: 0.5857, Val_Loss: 0.7379, Val_Acc: 0.7640\nEpoch 170, Loss: 0.7306, Acc: 0.6357, Val_Loss: 0.7397, Val_Acc: 0.7640\nEpoch 171, Loss: 0.7533, Acc: 0.6071, Val_Loss: 0.7426, Val_Acc: 0.7720\nEpoch 172, Loss: 0.6889, Acc: 0.5929, Val_Loss: 0.7482, Val_Acc: 0.7740\nEpoch 173, Loss: 0.7211, Acc: 0.6214, Val_Loss: 0.7534, Val_Acc: 0.7740\nEpoch 174, Loss: 0.7908, Acc: 0.6000, Val_Loss: 0.7603, Val_Acc: 0.7760\nEpoch 175, Loss: 0.6708, Acc: 0.6571, Val_Loss: 0.7663, Val_Acc: 0.7720\nEpoch 176, Loss: 0.8184, Acc: 0.5429, Val_Loss: 0.7731, Val_Acc: 0.7580\nEpoch 177, Loss: 0.6157, Acc: 0.7071, Val_Loss: 0.7804, Val_Acc: 0.7520\nEpoch 178, Loss: 0.8092, Acc: 0.5929, Val_Loss: 0.7819, Val_Acc: 0.7560\nEpoch 179, Loss: 0.7814, Acc: 0.6143, Val_Loss: 0.7913, Val_Acc: 0.7480\nEpoch 180, Loss: 0.6933, Acc: 0.6500, Val_Loss: 0.7970, Val_Acc: 0.7460\nEpoch 181, Loss: 0.7385, Acc: 0.6357, Val_Loss: 0.8027, Val_Acc: 0.7400\nEpoch 182, Loss: 0.6941, Acc: 0.5929, Val_Loss: 0.8006, Val_Acc: 0.7400\nEpoch 183, Loss: 0.7237, Acc: 0.6214, Val_Loss: 0.7949, Val_Acc: 0.7440\nEpoch 184, Loss: 0.7709, Acc: 0.6000, Val_Loss: 0.7841, Val_Acc: 0.7460\nEpoch 185, Loss: 0.6867, Acc: 0.6429, Val_Loss: 0.7715, Val_Acc: 0.7600\nEpoch 186, Loss: 0.6979, Acc: 0.6071, Val_Loss: 0.7656, Val_Acc: 0.7720\nEpoch 187, Loss: 0.7018, Acc: 0.6357, Val_Loss: 0.7598, Val_Acc: 0.7720\nEpoch 188, Loss: 0.7419, Acc: 0.6143, Val_Loss: 0.7542, Val_Acc: 0.7700\nEpoch 189, Loss: 0.5713, Acc: 0.7143, Val_Loss: 0.7487, Val_Acc: 0.7700\nEpoch 190, Loss: 0.8017, Acc: 0.5857, Val_Loss: 0.7485, Val_Acc: 0.7660\nEpoch 191, Loss: 0.5911, Acc: 0.7071, Val_Loss: 0.7497, Val_Acc: 0.7600\nEpoch 192, Loss: 0.7442, Acc: 0.6286, Val_Loss: 0.7505, Val_Acc: 0.7660\nEpoch 193, Loss: 0.5230, Acc: 0.7500, Val_Loss: 0.7511, Val_Acc: 0.7660\nEpoch 194, Loss: 0.7443, Acc: 0.6071, Val_Loss: 0.7494, Val_Acc: 0.7680\nEpoch 195, Loss: 0.6683, Acc: 0.6857, Val_Loss: 0.7451, Val_Acc: 0.7660\nEpoch 196, Loss: 0.7260, Acc: 0.6286, Val_Loss: 0.7437, Val_Acc: 0.7660\nEpoch 197, Loss: 0.6565, Acc: 0.6571, Val_Loss: 0.7495, Val_Acc: 0.7720\nEpoch 198, Loss: 0.6740, Acc: 0.6214, Val_Loss: 0.7621, Val_Acc: 0.7740\nEpoch 199, Loss: 0.7583, Acc: 0.6286, Val_Loss: 0.7767, Val_Acc: 0.7760\nEpoch 200, Loss: 0.7375, Acc: 0.6143, Val_Loss: 0.7831, Val_Acc: 0.7700\nEpoch 201, Loss: 0.6948, Acc: 0.6429, Val_Loss: 0.7935, Val_Acc: 0.7660\nEpoch 202, Loss: 0.6467, Acc: 0.6714, Val_Loss: 0.8005, Val_Acc: 0.7600\nEpoch 203, Loss: 0.6495, Acc: 0.6786, Val_Loss: 0.8041, Val_Acc: 0.7660\nEpoch 204, Loss: 0.7339, Acc: 0.6214, Val_Loss: 0.8034, Val_Acc: 0.7580\nEpoch 205, Loss: 0.6757, Acc: 0.6143, Val_Loss: 0.7935, Val_Acc: 0.7580\nEpoch 206, Loss: 0.6664, Acc: 0.6571, Val_Loss: 0.7770, Val_Acc: 0.7600\nEpoch 207, Loss: 0.6070, Acc: 0.7214, Val_Loss: 0.7598, Val_Acc: 0.7620\nEpoch 208, Loss: 0.7693, Acc: 0.5929, Val_Loss: 0.7457, Val_Acc: 0.7660\nEpoch 209, Loss: 0.7644, Acc: 0.5714, Val_Loss: 0.7363, Val_Acc: 0.7580\nEpoch 210, Loss: 0.6685, Acc: 0.6786, Val_Loss: 0.7301, Val_Acc: 0.7640\nEpoch 211, Loss: 0.7447, Acc: 0.6214, Val_Loss: 0.7268, Val_Acc: 0.7660\nEpoch 212, Loss: 0.7096, Acc: 0.6286, Val_Loss: 0.7248, Val_Acc: 0.7680\nEpoch 213, Loss: 0.6226, Acc: 0.6571, Val_Loss: 0.7241, Val_Acc: 0.7660\nEpoch 214, Loss: 0.7146, Acc: 0.6357, Val_Loss: 0.7260, Val_Acc: 0.7660\nEpoch 215, Loss: 0.7266, Acc: 0.6000, Val_Loss: 0.7370, Val_Acc: 0.7680\nEpoch 216, Loss: 0.7073, Acc: 0.6071, Val_Loss: 0.7531, Val_Acc: 0.7600\nEpoch 217, Loss: 0.6864, Acc: 0.6571, Val_Loss: 0.7696, Val_Acc: 0.7700\nEpoch 218, Loss: 0.7570, Acc: 0.5857, Val_Loss: 0.7871, Val_Acc: 0.7680\nEpoch 219, Loss: 0.7445, Acc: 0.6071, Val_Loss: 0.7956, Val_Acc: 0.7680\nEpoch 220, Loss: 0.6567, Acc: 0.6286, Val_Loss: 0.7993, Val_Acc: 0.7680\nEpoch 221, Loss: 0.6998, Acc: 0.6214, Val_Loss: 0.8039, Val_Acc: 0.7700\nEpoch 222, Loss: 0.7061, Acc: 0.6643, Val_Loss: 0.7948, Val_Acc: 0.7680\nEpoch 223, Loss: 0.7403, Acc: 0.6214, Val_Loss: 0.7965, Val_Acc: 0.7660\nEpoch 224, Loss: 0.6737, Acc: 0.6500, Val_Loss: 0.7920, Val_Acc: 0.7680\nEpoch 225, Loss: 0.7332, Acc: 0.6214, Val_Loss: 0.7844, Val_Acc: 0.7660\nEpoch 226, Loss: 0.6384, Acc: 0.6571, Val_Loss: 0.7717, Val_Acc: 0.7580\nEpoch 227, Loss: 0.5968, Acc: 0.6786, Val_Loss: 0.7621, Val_Acc: 0.7600\nEpoch 228, Loss: 0.6457, Acc: 0.6500, Val_Loss: 0.7501, Val_Acc: 0.7620\nEpoch 229, Loss: 0.6919, Acc: 0.6786, Val_Loss: 0.7369, Val_Acc: 0.7660\nEpoch 230, Loss: 0.6878, Acc: 0.6286, Val_Loss: 0.7310, Val_Acc: 0.7660\nEpoch 231, Loss: 0.6101, Acc: 0.6929, Val_Loss: 0.7275, Val_Acc: 0.7680\nEpoch 232, Loss: 0.6787, Acc: 0.6643, Val_Loss: 0.7257, Val_Acc: 0.7660\nEpoch 233, Loss: 0.7004, Acc: 0.6571, Val_Loss: 0.7261, Val_Acc: 0.7680\nEpoch 234, Loss: 0.7284, Acc: 0.6071, Val_Loss: 0.7280, Val_Acc: 0.7680\nEpoch 235, Loss: 0.7238, Acc: 0.6643, Val_Loss: 0.7329, Val_Acc: 0.7700\nEpoch 236, Loss: 0.7152, Acc: 0.6357, Val_Loss: 0.7440, Val_Acc: 0.7740\nEpoch 237, Loss: 0.6765, Acc: 0.6429, Val_Loss: 0.7560, Val_Acc: 0.7760\nEpoch 238, Loss: 0.7195, Acc: 0.5929, Val_Loss: 0.7689, Val_Acc: 0.7740\nEpoch 239, Loss: 0.7165, Acc: 0.6000, Val_Loss: 0.7755, Val_Acc: 0.7700\nEpoch 240, Loss: 0.7744, Acc: 0.6000, Val_Loss: 0.7753, Val_Acc: 0.7720\nEpoch 241, Loss: 0.8049, Acc: 0.6429, Val_Loss: 0.7738, Val_Acc: 0.7720\nEpoch 242, Loss: 0.5822, Acc: 0.6786, Val_Loss: 0.7688, Val_Acc: 0.7720\nEpoch 243, Loss: 0.5966, Acc: 0.7000, Val_Loss: 0.7642, Val_Acc: 0.7680\nEpoch 244, Loss: 0.6566, Acc: 0.6286, Val_Loss: 0.7605, Val_Acc: 0.7660\nEpoch 245, Loss: 0.7611, Acc: 0.5929, Val_Loss: 0.7538, Val_Acc: 0.7720\nEpoch 246, Loss: 0.6395, Acc: 0.6786, Val_Loss: 0.7504, Val_Acc: 0.7700\nEpoch 247, Loss: 0.7046, Acc: 0.6429, Val_Loss: 0.7531, Val_Acc: 0.7620\nEpoch 248, Loss: 0.6785, Acc: 0.6643, Val_Loss: 0.7592, Val_Acc: 0.7520\nEpoch 249, Loss: 0.6729, Acc: 0.6429, Val_Loss: 0.7630, Val_Acc: 0.7500\nEpoch 250, Loss: 0.7422, Acc: 0.5857, Val_Loss: 0.7684, Val_Acc: 0.7500\nEpoch 251, Loss: 0.7565, Acc: 0.6500, Val_Loss: 0.7650, Val_Acc: 0.7540\nEpoch 252, Loss: 0.7352, Acc: 0.6143, Val_Loss: 0.7542, Val_Acc: 0.7620\nEpoch 253, Loss: 0.7069, Acc: 0.5857, Val_Loss: 0.7477, Val_Acc: 0.7700\nEpoch 254, Loss: 0.7073, Acc: 0.6357, Val_Loss: 0.7449, Val_Acc: 0.7700\nEpoch 255, Loss: 0.6645, Acc: 0.6214, Val_Loss: 0.7418, Val_Acc: 0.7720\nEpoch 256, Loss: 0.7001, Acc: 0.6429, Val_Loss: 0.7405, Val_Acc: 0.7700\nEpoch 257, Loss: 0.6373, Acc: 0.6000, Val_Loss: 0.7410, Val_Acc: 0.7680\nEpoch 258, Loss: 0.7173, Acc: 0.6286, Val_Loss: 0.7438, Val_Acc: 0.7680\nEpoch 259, Loss: 0.6244, Acc: 0.6714, Val_Loss: 0.7479, Val_Acc: 0.7740\nEpoch 260, Loss: 0.6851, Acc: 0.6643, Val_Loss: 0.7535, Val_Acc: 0.7740\nEpoch 261, Loss: 0.6279, Acc: 0.6786, Val_Loss: 0.7539, Val_Acc: 0.7740\nEpoch 262, Loss: 0.7425, Acc: 0.6357, Val_Loss: 0.7581, Val_Acc: 0.7640\nEpoch 263, Loss: 0.7547, Acc: 0.6357, Val_Loss: 0.7665, Val_Acc: 0.7620\nEpoch 264, Loss: 0.6847, Acc: 0.6571, Val_Loss: 0.7746, Val_Acc: 0.7620\nEpoch 265, Loss: 0.7374, Acc: 0.5929, Val_Loss: 0.7780, Val_Acc: 0.7580\nEpoch 266, Loss: 0.7367, Acc: 0.5857, Val_Loss: 0.7816, Val_Acc: 0.7600\nEpoch 267, Loss: 0.7288, Acc: 0.6071, Val_Loss: 0.7857, Val_Acc: 0.7560\nEpoch 268, Loss: 0.7097, Acc: 0.6357, Val_Loss: 0.7854, Val_Acc: 0.7560\nEpoch 269, Loss: 0.6350, Acc: 0.6857, Val_Loss: 0.7837, Val_Acc: 0.7580\nEpoch 270, Loss: 0.7569, Acc: 0.5500, Val_Loss: 0.7770, Val_Acc: 0.7680\nEpoch 271, Loss: 0.5702, Acc: 0.6714, Val_Loss: 0.7700, Val_Acc: 0.7760\nEpoch 272, Loss: 0.7491, Acc: 0.6071, Val_Loss: 0.7644, Val_Acc: 0.7760\nEpoch 273, Loss: 0.7428, Acc: 0.5643, Val_Loss: 0.7578, Val_Acc: 0.7780\nEpoch 274, Loss: 0.6404, Acc: 0.6643, Val_Loss: 0.7625, Val_Acc: 0.7760\nEpoch 275, Loss: 0.7181, Acc: 0.6143, Val_Loss: 0.7682, Val_Acc: 0.7740\nEpoch 276, Loss: 0.6337, Acc: 0.6857, Val_Loss: 0.7715, Val_Acc: 0.7800\nEpoch 277, Loss: 0.6956, Acc: 0.6571, Val_Loss: 0.7812, Val_Acc: 0.7680\nEpoch 278, Loss: 0.6070, Acc: 0.6786, Val_Loss: 0.7960, Val_Acc: 0.7640\nEpoch 279, Loss: 0.6838, Acc: 0.6500, Val_Loss: 0.8053, Val_Acc: 0.7540\nEpoch 280, Loss: 0.7225, Acc: 0.6071, Val_Loss: 0.8031, Val_Acc: 0.7580\nEpoch 281, Loss: 0.7993, Acc: 0.5929, Val_Loss: 0.7921, Val_Acc: 0.7620\nEpoch 282, Loss: 0.6507, Acc: 0.6071, Val_Loss: 0.7850, Val_Acc: 0.7600\nEpoch 283, Loss: 0.6656, Acc: 0.6786, Val_Loss: 0.7842, Val_Acc: 0.7600\nEpoch 284, Loss: 0.6857, Acc: 0.6714, Val_Loss: 0.7838, Val_Acc: 0.7620\nEpoch 285, Loss: 0.7525, Acc: 0.6000, Val_Loss: 0.7861, Val_Acc: 0.7620\nEpoch 286, Loss: 0.6967, Acc: 0.6357, Val_Loss: 0.7856, Val_Acc: 0.7600\nEpoch 287, Loss: 0.6372, Acc: 0.6857, Val_Loss: 0.7845, Val_Acc: 0.7640\nEpoch 288, Loss: 0.6330, Acc: 0.6357, Val_Loss: 0.7850, Val_Acc: 0.7680\nEpoch 289, Loss: 0.6179, Acc: 0.6714, Val_Loss: 0.7879, Val_Acc: 0.7640\nEpoch 290, Loss: 0.6753, Acc: 0.6429, Val_Loss: 0.7990, Val_Acc: 0.7600\nEpoch 291, Loss: 0.6691, Acc: 0.6357, Val_Loss: 0.8100, Val_Acc: 0.7620\nEpoch 292, Loss: 0.5864, Acc: 0.7000, Val_Loss: 0.8166, Val_Acc: 0.7540\nEpoch 293, Loss: 0.7131, Acc: 0.6429, Val_Loss: 0.8151, Val_Acc: 0.7660\nEpoch 294, Loss: 0.7132, Acc: 0.6357, Val_Loss: 0.8036, Val_Acc: 0.7720\nEpoch 295, Loss: 0.6836, Acc: 0.6357, Val_Loss: 0.7924, Val_Acc: 0.7780\nEpoch 296, Loss: 0.6886, Acc: 0.6143, Val_Loss: 0.7775, Val_Acc: 0.7800\nEpoch 297, Loss: 0.6649, Acc: 0.6429, Val_Loss: 0.7573, Val_Acc: 0.7760\nEpoch 298, Loss: 0.6356, Acc: 0.6571, Val_Loss: 0.7435, Val_Acc: 0.7780\nEpoch 299, Loss: 0.6631, Acc: 0.6571, Val_Loss: 0.7357, Val_Acc: 0.7740\nEpoch 300, Loss: 0.5721, Acc: 0.7071, Val_Loss: 0.7314, Val_Acc: 0.7720\nEpoch 301, Loss: 0.7537, Acc: 0.6143, Val_Loss: 0.7339, Val_Acc: 0.7740\nEpoch 302, Loss: 0.7796, Acc: 0.5929, Val_Loss: 0.7435, Val_Acc: 0.7640\nEpoch 303, Loss: 0.6817, Acc: 0.6286, Val_Loss: 0.7634, Val_Acc: 0.7640\nEpoch 304, Loss: 0.7590, Acc: 0.5929, Val_Loss: 0.7873, Val_Acc: 0.7600\nEpoch 305, Loss: 0.6909, Acc: 0.6786, Val_Loss: 0.8174, Val_Acc: 0.7540\nEpoch 306, Loss: 0.6711, Acc: 0.6429, Val_Loss: 0.8445, Val_Acc: 0.7440\nEpoch 307, Loss: 0.6984, Acc: 0.6357, Val_Loss: 0.8622, Val_Acc: 0.7440\nEpoch 308, Loss: 0.6756, Acc: 0.6429, Val_Loss: 0.8712, Val_Acc: 0.7440\nEpoch 309, Loss: 0.6375, Acc: 0.6786, Val_Loss: 0.8652, Val_Acc: 0.7500\nEpoch 310, Loss: 0.8539, Acc: 0.5643, Val_Loss: 0.8517, Val_Acc: 0.7540\nEpoch 311, Loss: 0.5826, Acc: 0.7000, Val_Loss: 0.8271, Val_Acc: 0.7560\nEpoch 312, Loss: 0.6053, Acc: 0.6857, Val_Loss: 0.8032, Val_Acc: 0.7640\nEpoch 313, Loss: 0.7866, Acc: 0.6071, Val_Loss: 0.7841, Val_Acc: 0.7660\nEpoch 314, Loss: 0.6703, Acc: 0.7071, Val_Loss: 0.7720, Val_Acc: 0.7640\nEpoch 315, Loss: 0.7681, Acc: 0.6286, Val_Loss: 0.7605, Val_Acc: 0.7640\nEpoch 316, Loss: 0.6460, Acc: 0.6786, Val_Loss: 0.7518, Val_Acc: 0.7660\nEpoch 317, Loss: 0.7834, Acc: 0.6000, Val_Loss: 0.7462, Val_Acc: 0.7640\nEpoch 318, Loss: 0.7009, Acc: 0.6143, Val_Loss: 0.7473, Val_Acc: 0.7620\nEpoch 319, Loss: 0.8087, Acc: 0.6214, Val_Loss: 0.7514, Val_Acc: 0.7620\nEpoch 320, Loss: 0.7484, Acc: 0.6000, Val_Loss: 0.7574, Val_Acc: 0.7660\nEpoch 321, Loss: 0.6897, Acc: 0.6429, Val_Loss: 0.7669, Val_Acc: 0.7620\nEpoch 322, Loss: 0.6521, Acc: 0.7071, Val_Loss: 0.7773, Val_Acc: 0.7620\nEpoch 323, Loss: 0.6733, Acc: 0.6643, Val_Loss: 0.7854, Val_Acc: 0.7600\nEpoch 324, Loss: 0.6772, Acc: 0.6571, Val_Loss: 0.7918, Val_Acc: 0.7580\nEpoch 325, Loss: 0.7219, Acc: 0.6214, Val_Loss: 0.7964, Val_Acc: 0.7560\nEpoch 326, Loss: 0.6159, Acc: 0.7000, Val_Loss: 0.7970, Val_Acc: 0.7560\nEpoch 327, Loss: 0.7265, Acc: 0.6429, Val_Loss: 0.7929, Val_Acc: 0.7600\nEpoch 328, Loss: 0.6859, Acc: 0.6429, Val_Loss: 0.7901, Val_Acc: 0.7600\nEpoch 329, Loss: 0.6787, Acc: 0.6643, Val_Loss: 0.7823, Val_Acc: 0.7600\nEpoch 330, Loss: 0.7431, Acc: 0.5857, Val_Loss: 0.7713, Val_Acc: 0.7640\nEpoch 331, Loss: 0.6639, Acc: 0.6714, Val_Loss: 0.7634, Val_Acc: 0.7720\nEpoch 332, Loss: 0.6962, Acc: 0.6143, Val_Loss: 0.7591, Val_Acc: 0.7740\nEpoch 333, Loss: 0.7231, Acc: 0.6000, Val_Loss: 0.7607, Val_Acc: 0.7780\nEpoch 334, Loss: 0.7341, Acc: 0.6000, Val_Loss: 0.7615, Val_Acc: 0.7720\nEpoch 335, Loss: 0.7497, Acc: 0.6286, Val_Loss: 0.7631, Val_Acc: 0.7720\nEpoch 336, Loss: 0.7856, Acc: 0.6143, Val_Loss: 0.7618, Val_Acc: 0.7640\nEpoch 337, Loss: 0.6597, Acc: 0.6429, Val_Loss: 0.7683, Val_Acc: 0.7600\nEpoch 338, Loss: 0.7156, Acc: 0.6643, Val_Loss: 0.7772, Val_Acc: 0.7540\nEpoch 339, Loss: 0.7838, Acc: 0.5786, Val_Loss: 0.7862, Val_Acc: 0.7500\nEpoch 340, Loss: 0.6968, Acc: 0.6357, Val_Loss: 0.7926, Val_Acc: 0.7480\nEpoch 341, Loss: 0.7166, Acc: 0.6071, Val_Loss: 0.7946, Val_Acc: 0.7480\nEpoch 342, Loss: 0.6941, Acc: 0.6286, Val_Loss: 0.7885, Val_Acc: 0.7480\nEpoch 343, Loss: 0.6477, Acc: 0.6929, Val_Loss: 0.7803, Val_Acc: 0.7500\nEpoch 344, Loss: 0.7521, Acc: 0.6286, Val_Loss: 0.7726, Val_Acc: 0.7620\nEpoch 345, Loss: 0.7235, Acc: 0.6143, Val_Loss: 0.7702, Val_Acc: 0.7640\nEpoch 346, Loss: 0.6773, Acc: 0.6929, Val_Loss: 0.7653, Val_Acc: 0.7600\nEpoch 347, Loss: 0.8217, Acc: 0.5571, Val_Loss: 0.7604, Val_Acc: 0.7620\nEpoch 348, Loss: 0.6609, Acc: 0.6714, Val_Loss: 0.7579, Val_Acc: 0.7640\nEpoch 349, Loss: 0.6950, Acc: 0.6500, Val_Loss: 0.7640, Val_Acc: 0.7640\nEpoch 350, Loss: 0.7394, Acc: 0.5643, Val_Loss: 0.7696, Val_Acc: 0.7560\nEpoch 351, Loss: 0.6112, Acc: 0.6857, Val_Loss: 0.7790, Val_Acc: 0.7520\nEpoch 352, Loss: 0.7727, Acc: 0.6071, Val_Loss: 0.7851, Val_Acc: 0.7500\nEpoch 353, Loss: 0.6764, Acc: 0.6500, Val_Loss: 0.7901, Val_Acc: 0.7500\nEpoch 354, Loss: 0.6897, Acc: 0.6500, Val_Loss: 0.7915, Val_Acc: 0.7520\nEpoch 355, Loss: 0.8275, Acc: 0.5357, Val_Loss: 0.7915, Val_Acc: 0.7520\nEpoch 356, Loss: 0.7463, Acc: 0.5786, Val_Loss: 0.7907, Val_Acc: 0.7580\nEpoch 357, Loss: 0.7161, Acc: 0.6143, Val_Loss: 0.7904, Val_Acc: 0.7620\nEpoch 358, Loss: 0.7609, Acc: 0.6357, Val_Loss: 0.7874, Val_Acc: 0.7700\nEpoch 359, Loss: 0.7341, Acc: 0.6714, Val_Loss: 0.7833, Val_Acc: 0.7680\nEpoch 360, Loss: 0.6948, Acc: 0.6286, Val_Loss: 0.7860, Val_Acc: 0.7680\nEpoch 361, Loss: 0.6852, Acc: 0.6143, Val_Loss: 0.7846, Val_Acc: 0.7680\nEpoch 362, Loss: 0.6749, Acc: 0.6500, Val_Loss: 0.7798, Val_Acc: 0.7640\nEpoch 363, Loss: 0.7180, Acc: 0.5929, Val_Loss: 0.7780, Val_Acc: 0.7560\nEpoch 364, Loss: 0.6478, Acc: 0.6643, Val_Loss: 0.7805, Val_Acc: 0.7520\nEpoch 365, Loss: 0.6702, Acc: 0.6714, Val_Loss: 0.7819, Val_Acc: 0.7480\nEpoch 366, Loss: 0.8130, Acc: 0.5857, Val_Loss: 0.7848, Val_Acc: 0.7500\nEpoch 367, Loss: 0.6438, Acc: 0.6857, Val_Loss: 0.7848, Val_Acc: 0.7540\nEpoch 368, Loss: 0.6655, Acc: 0.6857, Val_Loss: 0.7819, Val_Acc: 0.7560\nEpoch 369, Loss: 0.7983, Acc: 0.5643, Val_Loss: 0.7765, Val_Acc: 0.7540\nEpoch 370, Loss: 0.6873, Acc: 0.6143, Val_Loss: 0.7719, Val_Acc: 0.7600\nEpoch 371, Loss: 0.7381, Acc: 0.6071, Val_Loss: 0.7698, Val_Acc: 0.7640\nEpoch 372, Loss: 0.7311, Acc: 0.6357, Val_Loss: 0.7684, Val_Acc: 0.7660\nEpoch 373, Loss: 0.5878, Acc: 0.6857, Val_Loss: 0.7661, Val_Acc: 0.7640\nEpoch 374, Loss: 0.6826, Acc: 0.6571, Val_Loss: 0.7665, Val_Acc: 0.7680\nEpoch 375, Loss: 0.6915, Acc: 0.6571, Val_Loss: 0.7640, Val_Acc: 0.7680\nEpoch 376, Loss: 0.6891, Acc: 0.6286, Val_Loss: 0.7648, Val_Acc: 0.7680\nEpoch 377, Loss: 0.7192, Acc: 0.6429, Val_Loss: 0.7639, Val_Acc: 0.7680\nEpoch 378, Loss: 0.5854, Acc: 0.7357, Val_Loss: 0.7674, Val_Acc: 0.7700\nEpoch 379, Loss: 0.7011, Acc: 0.6429, Val_Loss: 0.7699, Val_Acc: 0.7700\nEpoch 380, Loss: 0.6860, Acc: 0.6357, Val_Loss: 0.7716, Val_Acc: 0.7660\nEpoch 381, Loss: 0.7013, Acc: 0.6571, Val_Loss: 0.7720, Val_Acc: 0.7600\nEpoch 382, Loss: 0.6291, Acc: 0.7000, Val_Loss: 0.7762, Val_Acc: 0.7560\nEpoch 383, Loss: 0.7274, Acc: 0.6214, Val_Loss: 0.7773, Val_Acc: 0.7580\nEpoch 384, Loss: 0.6595, Acc: 0.6071, Val_Loss: 0.7769, Val_Acc: 0.7560\nEpoch 385, Loss: 0.7442, Acc: 0.6071, Val_Loss: 0.7774, Val_Acc: 0.7560\nEpoch 386, Loss: 0.7561, Acc: 0.6214, Val_Loss: 0.7765, Val_Acc: 0.7580\nEpoch 387, Loss: 0.7940, Acc: 0.5786, Val_Loss: 0.7737, Val_Acc: 0.7540\nEpoch 388, Loss: 0.7116, Acc: 0.6357, Val_Loss: 0.7683, Val_Acc: 0.7560\nEpoch 389, Loss: 0.6390, Acc: 0.6500, Val_Loss: 0.7630, Val_Acc: 0.7660\nEpoch 390, Loss: 0.7388, Acc: 0.6071, Val_Loss: 0.7573, Val_Acc: 0.7680\nEpoch 391, Loss: 0.6584, Acc: 0.7071, Val_Loss: 0.7527, Val_Acc: 0.7700\nEpoch 392, Loss: 0.6759, Acc: 0.6786, Val_Loss: 0.7515, Val_Acc: 0.7740\nEpoch 393, Loss: 0.7321, Acc: 0.6214, Val_Loss: 0.7509, Val_Acc: 0.7660\nEpoch 394, Loss: 0.7327, Acc: 0.6143, Val_Loss: 0.7542, Val_Acc: 0.7620\nEpoch 395, Loss: 0.7456, Acc: 0.6000, Val_Loss: 0.7632, Val_Acc: 0.7540\nEpoch 396, Loss: 0.6586, Acc: 0.6786, Val_Loss: 0.7752, Val_Acc: 0.7500\nEpoch 397, Loss: 0.7002, Acc: 0.6143, Val_Loss: 0.7859, Val_Acc: 0.7600\nEpoch 398, Loss: 0.7508, Acc: 0.5786, Val_Loss: 0.7910, Val_Acc: 0.7560\nEpoch 399, Loss: 0.6036, Acc: 0.6500, Val_Loss: 0.7952, Val_Acc: 0.7560\nEpoch 400, Loss: 0.6211, Acc: 0.6643, Val_Loss: 0.7976, Val_Acc: 0.7540\nEpoch 401, Loss: 0.6643, Acc: 0.6571, Val_Loss: 0.7869, Val_Acc: 0.7560\nEpoch 402, Loss: 0.7181, Acc: 0.6429, Val_Loss: 0.7797, Val_Acc: 0.7620\nEpoch 403, Loss: 0.7018, Acc: 0.6714, Val_Loss: 0.7729, Val_Acc: 0.7620\nEpoch 404, Loss: 0.7093, Acc: 0.6357, Val_Loss: 0.7697, Val_Acc: 0.7640\nEpoch 405, Loss: 0.7159, Acc: 0.6500, Val_Loss: 0.7666, Val_Acc: 0.7640\nEpoch 406, Loss: 0.7177, Acc: 0.6571, Val_Loss: 0.7643, Val_Acc: 0.7600\nEpoch 407, Loss: 0.5964, Acc: 0.6357, Val_Loss: 0.7641, Val_Acc: 0.7620\nEpoch 408, Loss: 0.6437, Acc: 0.6857, Val_Loss: 0.7606, Val_Acc: 0.7640\nEpoch 409, Loss: 0.6235, Acc: 0.7000, Val_Loss: 0.7628, Val_Acc: 0.7700\nEpoch 410, Loss: 0.6529, Acc: 0.6500, Val_Loss: 0.7658, Val_Acc: 0.7680\nEpoch 411, Loss: 0.6829, Acc: 0.6714, Val_Loss: 0.7686, Val_Acc: 0.7780\nEpoch 412, Loss: 0.6893, Acc: 0.6357, Val_Loss: 0.7687, Val_Acc: 0.7740\nEpoch 413, Loss: 0.6820, Acc: 0.6357, Val_Loss: 0.7708, Val_Acc: 0.7740\nEpoch 414, Loss: 0.7078, Acc: 0.6214, Val_Loss: 0.7731, Val_Acc: 0.7740\nEpoch 415, Loss: 0.6230, Acc: 0.6786, Val_Loss: 0.7743, Val_Acc: 0.7740\nEpoch 416, Loss: 0.7744, Acc: 0.5857, Val_Loss: 0.7769, Val_Acc: 0.7720\nEpoch 417, Loss: 0.5913, Acc: 0.6714, Val_Loss: 0.7725, Val_Acc: 0.7740\nEpoch 418, Loss: 0.6751, Acc: 0.6643, Val_Loss: 0.7638, Val_Acc: 0.7680\nEpoch 419, Loss: 0.6940, Acc: 0.6214, Val_Loss: 0.7561, Val_Acc: 0.7640\nEpoch 420, Loss: 0.6710, Acc: 0.6571, Val_Loss: 0.7499, Val_Acc: 0.7640\nEpoch 421, Loss: 0.5962, Acc: 0.7429, Val_Loss: 0.7465, Val_Acc: 0.7640\nEpoch 422, Loss: 0.6328, Acc: 0.6643, Val_Loss: 0.7418, Val_Acc: 0.7600\nEpoch 423, Loss: 0.6408, Acc: 0.6357, Val_Loss: 0.7445, Val_Acc: 0.7620\nEpoch 424, Loss: 0.7115, Acc: 0.6214, Val_Loss: 0.7481, Val_Acc: 0.7600\nEpoch 425, Loss: 0.7490, Acc: 0.5929, Val_Loss: 0.7544, Val_Acc: 0.7560\nEpoch 426, Loss: 0.6650, Acc: 0.6643, Val_Loss: 0.7623, Val_Acc: 0.7520\nEpoch 427, Loss: 0.5805, Acc: 0.6857, Val_Loss: 0.7719, Val_Acc: 0.7500\nEpoch 428, Loss: 0.6792, Acc: 0.6429, Val_Loss: 0.7834, Val_Acc: 0.7580\nEpoch 429, Loss: 0.6459, Acc: 0.6357, Val_Loss: 0.7926, Val_Acc: 0.7580\nEpoch 430, Loss: 0.6652, Acc: 0.6571, Val_Loss: 0.8053, Val_Acc: 0.7560\nEpoch 431, Loss: 0.7180, Acc: 0.6071, Val_Loss: 0.8127, Val_Acc: 0.7600\nEpoch 432, Loss: 0.7183, Acc: 0.6357, Val_Loss: 0.8200, Val_Acc: 0.7600\nEpoch 433, Loss: 0.6202, Acc: 0.7000, Val_Loss: 0.8249, Val_Acc: 0.7640\nEpoch 434, Loss: 0.5582, Acc: 0.7357, Val_Loss: 0.8121, Val_Acc: 0.7660\nEpoch 435, Loss: 0.6229, Acc: 0.7000, Val_Loss: 0.7933, Val_Acc: 0.7640\nEpoch 436, Loss: 0.6603, Acc: 0.6500, Val_Loss: 0.7713, Val_Acc: 0.7680\nEpoch 437, Loss: 0.7238, Acc: 0.5929, Val_Loss: 0.7597, Val_Acc: 0.7680\nEpoch 438, Loss: 0.7252, Acc: 0.6286, Val_Loss: 0.7520, Val_Acc: 0.7700\nEpoch 439, Loss: 0.8046, Acc: 0.5571, Val_Loss: 0.7482, Val_Acc: 0.7700\nEpoch 440, Loss: 0.5892, Acc: 0.7214, Val_Loss: 0.7492, Val_Acc: 0.7680\nEpoch 441, Loss: 0.6979, Acc: 0.6429, Val_Loss: 0.7532, Val_Acc: 0.7680\nEpoch 442, Loss: 0.7290, Acc: 0.6143, Val_Loss: 0.7605, Val_Acc: 0.7620\nEpoch 443, Loss: 0.7083, Acc: 0.6357, Val_Loss: 0.7722, Val_Acc: 0.7600\nEpoch 444, Loss: 0.6608, Acc: 0.6643, Val_Loss: 0.7960, Val_Acc: 0.7500\nEpoch 445, Loss: 0.5462, Acc: 0.7286, Val_Loss: 0.8247, Val_Acc: 0.7460\nEpoch 446, Loss: 0.7009, Acc: 0.6571, Val_Loss: 0.8534, Val_Acc: 0.7540\nEpoch 447, Loss: 0.7281, Acc: 0.6143, Val_Loss: 0.8815, Val_Acc: 0.7540\nEpoch 448, Loss: 0.7067, Acc: 0.6071, Val_Loss: 0.8889, Val_Acc: 0.7500\nEpoch 449, Loss: 0.7476, Acc: 0.6143, Val_Loss: 0.8795, Val_Acc: 0.7540\nEpoch 450, Loss: 0.7484, Acc: 0.6071, Val_Loss: 0.8622, Val_Acc: 0.7580\nEpoch 451, Loss: 0.6862, Acc: 0.6429, Val_Loss: 0.8280, Val_Acc: 0.7580\nEpoch 452, Loss: 0.7578, Acc: 0.6071, Val_Loss: 0.7995, Val_Acc: 0.7620\nEpoch 453, Loss: 0.5994, Acc: 0.6857, Val_Loss: 0.7816, Val_Acc: 0.7700\nEpoch 454, Loss: 0.5910, Acc: 0.6929, Val_Loss: 0.7678, Val_Acc: 0.7640\nEpoch 455, Loss: 0.7537, Acc: 0.6214, Val_Loss: 0.7592, Val_Acc: 0.7760\nEpoch 456, Loss: 0.7014, Acc: 0.6857, Val_Loss: 0.7557, Val_Acc: 0.7760\nEpoch 457, Loss: 0.7577, Acc: 0.5714, Val_Loss: 0.7543, Val_Acc: 0.7740\nEpoch 458, Loss: 0.7377, Acc: 0.6071, Val_Loss: 0.7545, Val_Acc: 0.7680\nEpoch 459, Loss: 0.6552, Acc: 0.6929, Val_Loss: 0.7607, Val_Acc: 0.7640\nEpoch 460, Loss: 0.6662, Acc: 0.6643, Val_Loss: 0.7844, Val_Acc: 0.7660\nEpoch 461, Loss: 0.7195, Acc: 0.5857, Val_Loss: 0.8162, Val_Acc: 0.7560\nEpoch 462, Loss: 0.6770, Acc: 0.6643, Val_Loss: 0.8498, Val_Acc: 0.7600\nEpoch 463, Loss: 0.6472, Acc: 0.6857, Val_Loss: 0.8867, Val_Acc: 0.7460\nEpoch 464, Loss: 0.6660, Acc: 0.6786, Val_Loss: 0.9137, Val_Acc: 0.7480\nEpoch 465, Loss: 0.6098, Acc: 0.6643, Val_Loss: 0.9078, Val_Acc: 0.7440\nEpoch 466, Loss: 0.7235, Acc: 0.6500, Val_Loss: 0.8807, Val_Acc: 0.7360\nEpoch 467, Loss: 0.6189, Acc: 0.6857, Val_Loss: 0.8564, Val_Acc: 0.7440\nEpoch 468, Loss: 0.7655, Acc: 0.6143, Val_Loss: 0.8350, Val_Acc: 0.7560\nEpoch 469, Loss: 0.7322, Acc: 0.6214, Val_Loss: 0.8117, Val_Acc: 0.7520\nEpoch 470, Loss: 0.7028, Acc: 0.5786, Val_Loss: 0.7916, Val_Acc: 0.7540\nEpoch 471, Loss: 0.6993, Acc: 0.6643, Val_Loss: 0.7714, Val_Acc: 0.7660\nEpoch 472, Loss: 0.7536, Acc: 0.6143, Val_Loss: 0.7570, Val_Acc: 0.7640\nEpoch 473, Loss: 0.7757, Acc: 0.6000, Val_Loss: 0.7492, Val_Acc: 0.7600\nEpoch 474, Loss: 0.7952, Acc: 0.6000, Val_Loss: 0.7471, Val_Acc: 0.7680\nEpoch 475, Loss: 0.8402, Acc: 0.5643, Val_Loss: 0.7506, Val_Acc: 0.7740\nEpoch 476, Loss: 0.8023, Acc: 0.6071, Val_Loss: 0.7582, Val_Acc: 0.7720\nEpoch 477, Loss: 0.7118, Acc: 0.6500, Val_Loss: 0.7705, Val_Acc: 0.7760\nEpoch 478, Loss: 0.9161, Acc: 0.5357, Val_Loss: 0.7868, Val_Acc: 0.7700\nEpoch 479, Loss: 0.7149, Acc: 0.6214, Val_Loss: 0.8075, Val_Acc: 0.7620\nEpoch 480, Loss: 0.5900, Acc: 0.6929, Val_Loss: 0.8274, Val_Acc: 0.7560\nEpoch 481, Loss: 0.7531, Acc: 0.6214, Val_Loss: 0.8419, Val_Acc: 0.7560\nEpoch 482, Loss: 0.7206, Acc: 0.6357, Val_Loss: 0.8414, Val_Acc: 0.7580\nEpoch 483, Loss: 0.7372, Acc: 0.6071, Val_Loss: 0.8364, Val_Acc: 0.7560\nEpoch 484, Loss: 0.6735, Acc: 0.6500, Val_Loss: 0.8276, Val_Acc: 0.7540\nEpoch 485, Loss: 0.6917, Acc: 0.6643, Val_Loss: 0.8065, Val_Acc: 0.7620\nEpoch 486, Loss: 0.6563, Acc: 0.6000, Val_Loss: 0.7905, Val_Acc: 0.7620\nEpoch 487, Loss: 0.6913, Acc: 0.6429, Val_Loss: 0.7797, Val_Acc: 0.7540\nEpoch 488, Loss: 0.6172, Acc: 0.6500, Val_Loss: 0.7730, Val_Acc: 0.7540\nEpoch 489, Loss: 0.7339, Acc: 0.6286, Val_Loss: 0.7687, Val_Acc: 0.7560\nEpoch 490, Loss: 0.7628, Acc: 0.6000, Val_Loss: 0.7622, Val_Acc: 0.7620\nEpoch 491, Loss: 0.6966, Acc: 0.6286, Val_Loss: 0.7666, Val_Acc: 0.7680\nEpoch 492, Loss: 0.7422, Acc: 0.5929, Val_Loss: 0.7755, Val_Acc: 0.7700\nEpoch 493, Loss: 0.6469, Acc: 0.6857, Val_Loss: 0.7867, Val_Acc: 0.7660\nEpoch 494, Loss: 0.7192, Acc: 0.6000, Val_Loss: 0.7983, Val_Acc: 0.7700\nEpoch 495, Loss: 0.6756, Acc: 0.6286, Val_Loss: 0.8125, Val_Acc: 0.7620\nEpoch 496, Loss: 0.7157, Acc: 0.6000, Val_Loss: 0.8270, Val_Acc: 0.7620\nEpoch 497, Loss: 0.6491, Acc: 0.6643, Val_Loss: 0.8352, Val_Acc: 0.7600\nEpoch 498, Loss: 0.7371, Acc: 0.6429, Val_Loss: 0.8400, Val_Acc: 0.7600\nEpoch 499, Loss: 0.6694, Acc: 0.6500, Val_Loss: 0.8409, Val_Acc: 0.7620\nEpoch 500, Loss: 0.7031, Acc: 0.6143, Val_Loss: 0.8382, Val_Acc: 0.7640\nEpoch 501, Loss: 0.6473, Acc: 0.6286, Val_Loss: 0.8265, Val_Acc: 0.7640\nEpoch 502, Loss: 0.6780, Acc: 0.6857, Val_Loss: 0.8149, Val_Acc: 0.7560\nEpoch 503, Loss: 0.6524, Acc: 0.6286, Val_Loss: 0.8059, Val_Acc: 0.7580\nEpoch 504, Loss: 0.7141, Acc: 0.6214, Val_Loss: 0.7989, Val_Acc: 0.7580\nEpoch 505, Loss: 0.6868, Acc: 0.6643, Val_Loss: 0.7952, Val_Acc: 0.7580\nEpoch 506, Loss: 0.6365, Acc: 0.6857, Val_Loss: 0.7890, Val_Acc: 0.7620\nEpoch 507, Loss: 0.6998, Acc: 0.6357, Val_Loss: 0.7850, Val_Acc: 0.7600\nEpoch 508, Loss: 0.6129, Acc: 0.6357, Val_Loss: 0.7802, Val_Acc: 0.7560\nEpoch 509, Loss: 0.7487, Acc: 0.6214, Val_Loss: 0.7837, Val_Acc: 0.7600\nEpoch 510, Loss: 0.6477, Acc: 0.6500, Val_Loss: 0.7877, Val_Acc: 0.7620\nEpoch 511, Loss: 0.6328, Acc: 0.7071, Val_Loss: 0.7933, Val_Acc: 0.7620\nEpoch 512, Loss: 0.6623, Acc: 0.6429, Val_Loss: 0.7998, Val_Acc: 0.7680\nEpoch 513, Loss: 0.7335, Acc: 0.6143, Val_Loss: 0.8051, Val_Acc: 0.7620\nEpoch 514, Loss: 0.7410, Acc: 0.6143, Val_Loss: 0.8101, Val_Acc: 0.7660\nEpoch 515, Loss: 0.7603, Acc: 0.6357, Val_Loss: 0.8096, Val_Acc: 0.7640\nEpoch 516, Loss: 0.6526, Acc: 0.6571, Val_Loss: 0.8143, Val_Acc: 0.7620\nEpoch 517, Loss: 0.6749, Acc: 0.6286, Val_Loss: 0.8271, Val_Acc: 0.7560\nEpoch 518, Loss: 0.6361, Acc: 0.7357, Val_Loss: 0.8441, Val_Acc: 0.7480\nEpoch 519, Loss: 0.6168, Acc: 0.6357, Val_Loss: 0.8525, Val_Acc: 0.7480\nEpoch 520, Loss: 0.7227, Acc: 0.6429, Val_Loss: 0.8461, Val_Acc: 0.7460\nEpoch 521, Loss: 0.7627, Acc: 0.6214, Val_Loss: 0.8370, Val_Acc: 0.7580\nEpoch 522, Loss: 0.6166, Acc: 0.6857, Val_Loss: 0.8208, Val_Acc: 0.7540\nEpoch 523, Loss: 0.6366, Acc: 0.7000, Val_Loss: 0.8079, Val_Acc: 0.7560\nEpoch 524, Loss: 0.7625, Acc: 0.6214, Val_Loss: 0.7990, Val_Acc: 0.7600\nEpoch 525, Loss: 0.6751, Acc: 0.6286, Val_Loss: 0.7931, Val_Acc: 0.7600\nEpoch 526, Loss: 0.7586, Acc: 0.6143, Val_Loss: 0.7952, Val_Acc: 0.7640\nEpoch 527, Loss: 0.6653, Acc: 0.6429, Val_Loss: 0.7991, Val_Acc: 0.7660\nEpoch 528, Loss: 0.7041, Acc: 0.6071, Val_Loss: 0.8041, Val_Acc: 0.7660\nEpoch 529, Loss: 0.7461, Acc: 0.5857, Val_Loss: 0.8045, Val_Acc: 0.7640\nEpoch 530, Loss: 0.7169, Acc: 0.6357, Val_Loss: 0.7991, Val_Acc: 0.7640\nEpoch 531, Loss: 0.6235, Acc: 0.6857, Val_Loss: 0.7957, Val_Acc: 0.7580\nEpoch 532, Loss: 0.6248, Acc: 0.7071, Val_Loss: 0.7935, Val_Acc: 0.7540\nEpoch 533, Loss: 0.6840, Acc: 0.6929, Val_Loss: 0.7935, Val_Acc: 0.7580\nEpoch 534, Loss: 0.6715, Acc: 0.6857, Val_Loss: 0.7888, Val_Acc: 0.7620\nEpoch 535, Loss: 0.8197, Acc: 0.5500, Val_Loss: 0.7858, Val_Acc: 0.7600\nEpoch 536, Loss: 0.6996, Acc: 0.6357, Val_Loss: 0.7846, Val_Acc: 0.7600\nEpoch 537, Loss: 0.6292, Acc: 0.7000, Val_Loss: 0.7886, Val_Acc: 0.7580\nEpoch 538, Loss: 0.6181, Acc: 0.6643, Val_Loss: 0.7932, Val_Acc: 0.7600\nEpoch 539, Loss: 0.6107, Acc: 0.6643, Val_Loss: 0.7980, Val_Acc: 0.7600\nEpoch 540, Loss: 0.7020, Acc: 0.6429, Val_Loss: 0.8008, Val_Acc: 0.7600\nEpoch 541, Loss: 0.7765, Acc: 0.5857, Val_Loss: 0.8049, Val_Acc: 0.7660\nEpoch 542, Loss: 0.6579, Acc: 0.6429, Val_Loss: 0.8102, Val_Acc: 0.7600\nEpoch 543, Loss: 0.6194, Acc: 0.6857, Val_Loss: 0.8129, Val_Acc: 0.7620\nEpoch 544, Loss: 0.7974, Acc: 0.5857, Val_Loss: 0.8133, Val_Acc: 0.7620\nEpoch 545, Loss: 0.7679, Acc: 0.6000, Val_Loss: 0.8128, Val_Acc: 0.7660\nEpoch 546, Loss: 0.7785, Acc: 0.5857, Val_Loss: 0.8113, Val_Acc: 0.7600\nEpoch 547, Loss: 0.6395, Acc: 0.6857, Val_Loss: 0.8098, Val_Acc: 0.7580\nEpoch 548, Loss: 0.5908, Acc: 0.6786, Val_Loss: 0.7990, Val_Acc: 0.7580\nEpoch 549, Loss: 0.7076, Acc: 0.6071, Val_Loss: 0.7922, Val_Acc: 0.7620\nEpoch 550, Loss: 0.6601, Acc: 0.6786, Val_Loss: 0.7853, Val_Acc: 0.7660\nEpoch 551, Loss: 0.6392, Acc: 0.6714, Val_Loss: 0.7844, Val_Acc: 0.7680\nEpoch 552, Loss: 0.7131, Acc: 0.6500, Val_Loss: 0.7796, Val_Acc: 0.7700\nEpoch 553, Loss: 0.6404, Acc: 0.6429, Val_Loss: 0.7775, Val_Acc: 0.7740\nEpoch 554, Loss: 0.6896, Acc: 0.6357, Val_Loss: 0.7783, Val_Acc: 0.7740\nEpoch 555, Loss: 0.7329, Acc: 0.6214, Val_Loss: 0.7798, Val_Acc: 0.7680\nEpoch 556, Loss: 0.5957, Acc: 0.6929, Val_Loss: 0.7798, Val_Acc: 0.7600\nEpoch 557, Loss: 0.6319, Acc: 0.6714, Val_Loss: 0.7828, Val_Acc: 0.7620\nEpoch 558, Loss: 0.6804, Acc: 0.6214, Val_Loss: 0.7865, Val_Acc: 0.7680\nEpoch 559, Loss: 0.6811, Acc: 0.6286, Val_Loss: 0.7882, Val_Acc: 0.7640\nEpoch 560, Loss: 0.8023, Acc: 0.5643, Val_Loss: 0.7920, Val_Acc: 0.7580\nEpoch 561, Loss: 0.5668, Acc: 0.7071, Val_Loss: 0.8003, Val_Acc: 0.7580\nEpoch 562, Loss: 0.7242, Acc: 0.5857, Val_Loss: 0.8031, Val_Acc: 0.7560\nEpoch 563, Loss: 0.6445, Acc: 0.6714, Val_Loss: 0.8078, Val_Acc: 0.7600\nEpoch 564, Loss: 0.6561, Acc: 0.6929, Val_Loss: 0.8068, Val_Acc: 0.7560\nEpoch 565, Loss: 0.7435, Acc: 0.6357, Val_Loss: 0.8028, Val_Acc: 0.7560\nEpoch 566, Loss: 0.6616, Acc: 0.6571, Val_Loss: 0.8015, Val_Acc: 0.7540\nEpoch 567, Loss: 0.7588, Acc: 0.6286, Val_Loss: 0.8054, Val_Acc: 0.7540\nEpoch 568, Loss: 0.7445, Acc: 0.6000, Val_Loss: 0.8098, Val_Acc: 0.7620\nEpoch 569, Loss: 0.6410, Acc: 0.6643, Val_Loss: 0.8092, Val_Acc: 0.7640\nEpoch 570, Loss: 0.6173, Acc: 0.6786, Val_Loss: 0.8022, Val_Acc: 0.7640\nEpoch 571, Loss: 0.7430, Acc: 0.6214, Val_Loss: 0.7892, Val_Acc: 0.7620\nEpoch 572, Loss: 0.6608, Acc: 0.6643, Val_Loss: 0.7777, Val_Acc: 0.7580\nEpoch 573, Loss: 0.7628, Acc: 0.6357, Val_Loss: 0.7697, Val_Acc: 0.7620\nEpoch 574, Loss: 0.6370, Acc: 0.6643, Val_Loss: 0.7658, Val_Acc: 0.7620\nEpoch 575, Loss: 0.7703, Acc: 0.5929, Val_Loss: 0.7645, Val_Acc: 0.7620\nEpoch 576, Loss: 0.7935, Acc: 0.5857, Val_Loss: 0.7697, Val_Acc: 0.7560\nEpoch 577, Loss: 0.6585, Acc: 0.6571, Val_Loss: 0.7796, Val_Acc: 0.7600\nEpoch 578, Loss: 0.7613, Acc: 0.5429, Val_Loss: 0.7947, Val_Acc: 0.7580\nEpoch 579, Loss: 0.8787, Acc: 0.5500, Val_Loss: 0.8093, Val_Acc: 0.7540\nEpoch 580, Loss: 0.6418, Acc: 0.7000, Val_Loss: 0.8215, Val_Acc: 0.7580\nEpoch 581, Loss: 0.6137, Acc: 0.6500, Val_Loss: 0.8286, Val_Acc: 0.7640\nEpoch 582, Loss: 0.7029, Acc: 0.6643, Val_Loss: 0.8297, Val_Acc: 0.7660\nEpoch 583, Loss: 0.7722, Acc: 0.5929, Val_Loss: 0.8267, Val_Acc: 0.7640\nEpoch 584, Loss: 0.7620, Acc: 0.5857, Val_Loss: 0.8140, Val_Acc: 0.7640\nEpoch 585, Loss: 0.5997, Acc: 0.6429, Val_Loss: 0.8016, Val_Acc: 0.7640\nEpoch 586, Loss: 0.7265, Acc: 0.6143, Val_Loss: 0.7935, Val_Acc: 0.7600\nEpoch 587, Loss: 0.6235, Acc: 0.6786, Val_Loss: 0.7864, Val_Acc: 0.7620\nEpoch 588, Loss: 0.7187, Acc: 0.6357, Val_Loss: 0.7797, Val_Acc: 0.7640\nEpoch 589, Loss: 0.7067, Acc: 0.6857, Val_Loss: 0.7765, Val_Acc: 0.7680\nEpoch 590, Loss: 0.7427, Acc: 0.6214, Val_Loss: 0.7712, Val_Acc: 0.7760\nEpoch 591, Loss: 0.7450, Acc: 0.5500, Val_Loss: 0.7657, Val_Acc: 0.7720\nEpoch 592, Loss: 0.7320, Acc: 0.5929, Val_Loss: 0.7650, Val_Acc: 0.7740\nEpoch 593, Loss: 0.8833, Acc: 0.5786, Val_Loss: 0.7650, Val_Acc: 0.7700\nEpoch 594, Loss: 0.7218, Acc: 0.6071, Val_Loss: 0.7682, Val_Acc: 0.7660\nEpoch 595, Loss: 0.7193, Acc: 0.6071, Val_Loss: 0.7745, Val_Acc: 0.7620\nEpoch 596, Loss: 0.7681, Acc: 0.5857, Val_Loss: 0.7830, Val_Acc: 0.7500\nEpoch 597, Loss: 0.6386, Acc: 0.6500, Val_Loss: 0.7961, Val_Acc: 0.7480\nEpoch 598, Loss: 0.6500, Acc: 0.6571, Val_Loss: 0.8102, Val_Acc: 0.7560\nEpoch 599, Loss: 0.6749, Acc: 0.6286, Val_Loss: 0.8210, Val_Acc: 0.7500\nEpoch 600, Loss: 0.5974, Acc: 0.6857, Val_Loss: 0.8295, Val_Acc: 0.7520\nEpoch 601, Loss: 0.6944, Acc: 0.6143, Val_Loss: 0.8378, Val_Acc: 0.7520\nEpoch 602, Loss: 0.6620, Acc: 0.6429, Val_Loss: 0.8401, Val_Acc: 0.7540\nEpoch 603, Loss: 0.7204, Acc: 0.6071, Val_Loss: 0.8378, Val_Acc: 0.7560\nEpoch 604, Loss: 0.8177, Acc: 0.6071, Val_Loss: 0.8375, Val_Acc: 0.7560\nEpoch 605, Loss: 0.6552, Acc: 0.6571, Val_Loss: 0.8367, Val_Acc: 0.7580\nEpoch 606, Loss: 0.6593, Acc: 0.6857, Val_Loss: 0.8293, Val_Acc: 0.7540\nEpoch 607, Loss: 0.8902, Acc: 0.5000, Val_Loss: 0.8186, Val_Acc: 0.7560\nEpoch 608, Loss: 0.7362, Acc: 0.6429, Val_Loss: 0.8104, Val_Acc: 0.7640\nEpoch 609, Loss: 0.6840, Acc: 0.6714, Val_Loss: 0.8027, Val_Acc: 0.7680\nEpoch 610, Loss: 0.6420, Acc: 0.6357, Val_Loss: 0.7945, Val_Acc: 0.7680\nEpoch 611, Loss: 0.6842, Acc: 0.6429, Val_Loss: 0.7927, Val_Acc: 0.7600\nEpoch 612, Loss: 0.7069, Acc: 0.6357, Val_Loss: 0.7928, Val_Acc: 0.7580\nEpoch 613, Loss: 0.7625, Acc: 0.6214, Val_Loss: 0.7960, Val_Acc: 0.7540\nEpoch 614, Loss: 0.7766, Acc: 0.5714, Val_Loss: 0.7961, Val_Acc: 0.7500\nEpoch 615, Loss: 0.7519, Acc: 0.6071, Val_Loss: 0.7964, Val_Acc: 0.7560\nEpoch 616, Loss: 0.6259, Acc: 0.6929, Val_Loss: 0.7947, Val_Acc: 0.7680\nEpoch 617, Loss: 0.6516, Acc: 0.6857, Val_Loss: 0.7948, Val_Acc: 0.7660\nEpoch 618, Loss: 0.5905, Acc: 0.6929, Val_Loss: 0.7849, Val_Acc: 0.7720\nEpoch 619, Loss: 0.6145, Acc: 0.7214, Val_Loss: 0.7731, Val_Acc: 0.7780\nEpoch 620, Loss: 0.6963, Acc: 0.6786, Val_Loss: 0.7644, Val_Acc: 0.7780\nEpoch 621, Loss: 0.8014, Acc: 0.5714, Val_Loss: 0.7576, Val_Acc: 0.7760\nEpoch 622, Loss: 0.6975, Acc: 0.6143, Val_Loss: 0.7512, Val_Acc: 0.7740\nEpoch 623, Loss: 0.6849, Acc: 0.6286, Val_Loss: 0.7465, Val_Acc: 0.7680\nEpoch 624, Loss: 0.7246, Acc: 0.6643, Val_Loss: 0.7466, Val_Acc: 0.7660\nEpoch 625, Loss: 0.6739, Acc: 0.6429, Val_Loss: 0.7477, Val_Acc: 0.7640\nEpoch 626, Loss: 0.6949, Acc: 0.6286, Val_Loss: 0.7477, Val_Acc: 0.7640\nEpoch 627, Loss: 0.6645, Acc: 0.6714, Val_Loss: 0.7509, Val_Acc: 0.7600\nEpoch 628, Loss: 0.7192, Acc: 0.6000, Val_Loss: 0.7569, Val_Acc: 0.7580\nEpoch 629, Loss: 0.6240, Acc: 0.6786, Val_Loss: 0.7638, Val_Acc: 0.7620\nEpoch 630, Loss: 0.6481, Acc: 0.6857, Val_Loss: 0.7733, Val_Acc: 0.7680\nEpoch 631, Loss: 0.7117, Acc: 0.6929, Val_Loss: 0.7846, Val_Acc: 0.7740\nEpoch 632, Loss: 0.6969, Acc: 0.6286, Val_Loss: 0.7979, Val_Acc: 0.7720\nEpoch 633, Loss: 0.6371, Acc: 0.7000, Val_Loss: 0.8094, Val_Acc: 0.7700\nEpoch 634, Loss: 0.7894, Acc: 0.6143, Val_Loss: 0.8197, Val_Acc: 0.7720\nEpoch 635, Loss: 0.6149, Acc: 0.7000, Val_Loss: 0.8246, Val_Acc: 0.7700\nEpoch 636, Loss: 0.7211, Acc: 0.6000, Val_Loss: 0.8219, Val_Acc: 0.7700\nEpoch 637, Loss: 0.6616, Acc: 0.6786, Val_Loss: 0.8120, Val_Acc: 0.7620\nEpoch 638, Loss: 0.7263, Acc: 0.6429, Val_Loss: 0.8029, Val_Acc: 0.7620\nEpoch 639, Loss: 0.7035, Acc: 0.6143, Val_Loss: 0.7960, Val_Acc: 0.7640\nEpoch 640, Loss: 0.7256, Acc: 0.6143, Val_Loss: 0.7924, Val_Acc: 0.7620\nEpoch 641, Loss: 0.6597, Acc: 0.6357, Val_Loss: 0.7905, Val_Acc: 0.7580\nEpoch 642, Loss: 0.6647, Acc: 0.6714, Val_Loss: 0.7851, Val_Acc: 0.7600\nEpoch 643, Loss: 0.6901, Acc: 0.6571, Val_Loss: 0.7739, Val_Acc: 0.7620\nEpoch 644, Loss: 0.7423, Acc: 0.6286, Val_Loss: 0.7621, Val_Acc: 0.7700\nEpoch 645, Loss: 0.7413, Acc: 0.6429, Val_Loss: 0.7553, Val_Acc: 0.7780\nEpoch 646, Loss: 0.6000, Acc: 0.6786, Val_Loss: 0.7605, Val_Acc: 0.7740\nEpoch 647, Loss: 0.6027, Acc: 0.7143, Val_Loss: 0.7691, Val_Acc: 0.7760\nEpoch 648, Loss: 0.6835, Acc: 0.6500, Val_Loss: 0.7818, Val_Acc: 0.7760\nEpoch 649, Loss: 0.7237, Acc: 0.6071, Val_Loss: 0.7915, Val_Acc: 0.7740\nEpoch 650, Loss: 0.6536, Acc: 0.6429, Val_Loss: 0.7991, Val_Acc: 0.7760\nEpoch 651, Loss: 0.7307, Acc: 0.6357, Val_Loss: 0.8038, Val_Acc: 0.7760\nEpoch 652, Loss: 0.6778, Acc: 0.6214, Val_Loss: 0.8052, Val_Acc: 0.7740\nEpoch 653, Loss: 0.6629, Acc: 0.6429, Val_Loss: 0.8035, Val_Acc: 0.7780\nEpoch 654, Loss: 0.7808, Acc: 0.6000, Val_Loss: 0.8000, Val_Acc: 0.7740\nEpoch 655, Loss: 0.5565, Acc: 0.6929, Val_Loss: 0.7869, Val_Acc: 0.7720\nEpoch 656, Loss: 0.5871, Acc: 0.6929, Val_Loss: 0.7773, Val_Acc: 0.7740\nEpoch 657, Loss: 0.7495, Acc: 0.5929, Val_Loss: 0.7719, Val_Acc: 0.7740\nEpoch 658, Loss: 0.7118, Acc: 0.6214, Val_Loss: 0.7660, Val_Acc: 0.7760\nEpoch 659, Loss: 0.6839, Acc: 0.6143, Val_Loss: 0.7631, Val_Acc: 0.7760\nEpoch 660, Loss: 0.7727, Acc: 0.5857, Val_Loss: 0.7613, Val_Acc: 0.7740\nEpoch 661, Loss: 0.6763, Acc: 0.6429, Val_Loss: 0.7644, Val_Acc: 0.7700\nEpoch 662, Loss: 0.6948, Acc: 0.6429, Val_Loss: 0.7696, Val_Acc: 0.7660\nEpoch 663, Loss: 0.6635, Acc: 0.6571, Val_Loss: 0.7835, Val_Acc: 0.7640\nEpoch 664, Loss: 0.7369, Acc: 0.6571, Val_Loss: 0.8043, Val_Acc: 0.7560\nEpoch 665, Loss: 0.5849, Acc: 0.7357, Val_Loss: 0.8228, Val_Acc: 0.7560\nEpoch 666, Loss: 0.7275, Acc: 0.6214, Val_Loss: 0.8365, Val_Acc: 0.7580\nEpoch 667, Loss: 0.7128, Acc: 0.6143, Val_Loss: 0.8408, Val_Acc: 0.7580\nEpoch 668, Loss: 0.7723, Acc: 0.5857, Val_Loss: 0.8399, Val_Acc: 0.7620\nEpoch 669, Loss: 0.6447, Acc: 0.6571, Val_Loss: 0.8330, Val_Acc: 0.7620\nEpoch 670, Loss: 0.7738, Acc: 0.5643, Val_Loss: 0.8075, Val_Acc: 0.7620\nEpoch 671, Loss: 0.7294, Acc: 0.5929, Val_Loss: 0.7812, Val_Acc: 0.7680\nEpoch 672, Loss: 0.6905, Acc: 0.6571, Val_Loss: 0.7685, Val_Acc: 0.7700\nEpoch 673, Loss: 0.6896, Acc: 0.6214, Val_Loss: 0.7601, Val_Acc: 0.7740\nEpoch 674, Loss: 0.6760, Acc: 0.6071, Val_Loss: 0.7539, Val_Acc: 0.7820\nEpoch 675, Loss: 0.5921, Acc: 0.6714, Val_Loss: 0.7531, Val_Acc: 0.7720\nEpoch 676, Loss: 0.7742, Acc: 0.5429, Val_Loss: 0.7656, Val_Acc: 0.7620\nEpoch 677, Loss: 0.6611, Acc: 0.6571, Val_Loss: 0.7863, Val_Acc: 0.7580\nEpoch 678, Loss: 0.7088, Acc: 0.6000, Val_Loss: 0.7971, Val_Acc: 0.7560\nEpoch 679, Loss: 0.7073, Acc: 0.6500, Val_Loss: 0.8010, Val_Acc: 0.7540\nEpoch 680, Loss: 0.7621, Acc: 0.6143, Val_Loss: 0.7992, Val_Acc: 0.7600\nEpoch 681, Loss: 0.6796, Acc: 0.6357, Val_Loss: 0.7989, Val_Acc: 0.7720\nEpoch 682, Loss: 0.7325, Acc: 0.6071, Val_Loss: 0.7999, Val_Acc: 0.7720\nEpoch 683, Loss: 0.6562, Acc: 0.6571, Val_Loss: 0.8031, Val_Acc: 0.7720\nEpoch 684, Loss: 0.7426, Acc: 0.6500, Val_Loss: 0.8054, Val_Acc: 0.7700\nEpoch 685, Loss: 0.7387, Acc: 0.5786, Val_Loss: 0.8036, Val_Acc: 0.7720\nEpoch 686, Loss: 0.6785, Acc: 0.7000, Val_Loss: 0.7997, Val_Acc: 0.7720\nEpoch 687, Loss: 0.7954, Acc: 0.5929, Val_Loss: 0.7931, Val_Acc: 0.7720\nEpoch 688, Loss: 0.7067, Acc: 0.6214, Val_Loss: 0.7957, Val_Acc: 0.7600\nEpoch 689, Loss: 0.6213, Acc: 0.6786, Val_Loss: 0.8033, Val_Acc: 0.7620\nEpoch 690, Loss: 0.7507, Acc: 0.5929, Val_Loss: 0.8007, Val_Acc: 0.7600\nEpoch 691, Loss: 0.7718, Acc: 0.5786, Val_Loss: 0.7903, Val_Acc: 0.7620\nEpoch 692, Loss: 0.7173, Acc: 0.6357, Val_Loss: 0.7800, Val_Acc: 0.7620\nEpoch 693, Loss: 0.7010, Acc: 0.6429, Val_Loss: 0.7674, Val_Acc: 0.7640\nEpoch 694, Loss: 0.6246, Acc: 0.6571, Val_Loss: 0.7639, Val_Acc: 0.7780\nEpoch 695, Loss: 0.7468, Acc: 0.6071, Val_Loss: 0.7681, Val_Acc: 0.7860\nEpoch 696, Loss: 0.6707, Acc: 0.6286, Val_Loss: 0.7801, Val_Acc: 0.7760\nEpoch 697, Loss: 0.7137, Acc: 0.6143, Val_Loss: 0.7908, Val_Acc: 0.7700\nEpoch 698, Loss: 0.7329, Acc: 0.6071, Val_Loss: 0.8050, Val_Acc: 0.7680\nEpoch 699, Loss: 0.6628, Acc: 0.6571, Val_Loss: 0.8220, Val_Acc: 0.7660\nEpoch 700, Loss: 0.7837, Acc: 0.5357, Val_Loss: 0.8360, Val_Acc: 0.7580\nEpoch 701, Loss: 0.6740, Acc: 0.6357, Val_Loss: 0.8501, Val_Acc: 0.7580\nEpoch 702, Loss: 0.7604, Acc: 0.5786, Val_Loss: 0.8657, Val_Acc: 0.7580\nEpoch 703, Loss: 0.7344, Acc: 0.6000, Val_Loss: 0.8585, Val_Acc: 0.7600\nEpoch 704, Loss: 0.7566, Acc: 0.5929, Val_Loss: 0.8466, Val_Acc: 0.7600\nEpoch 705, Loss: 0.6146, Acc: 0.7071, Val_Loss: 0.8300, Val_Acc: 0.7620\nEpoch 706, Loss: 0.6457, Acc: 0.6786, Val_Loss: 0.8136, Val_Acc: 0.7600\nEpoch 707, Loss: 0.8339, Acc: 0.5643, Val_Loss: 0.7993, Val_Acc: 0.7620\nEpoch 708, Loss: 0.6084, Acc: 0.6714, Val_Loss: 0.7869, Val_Acc: 0.7760\nEpoch 709, Loss: 0.6236, Acc: 0.6857, Val_Loss: 0.7749, Val_Acc: 0.7800\nEpoch 710, Loss: 0.8299, Acc: 0.5571, Val_Loss: 0.7657, Val_Acc: 0.7820\nEpoch 711, Loss: 0.6454, Acc: 0.6571, Val_Loss: 0.7596, Val_Acc: 0.7800\nEpoch 712, Loss: 0.7304, Acc: 0.6357, Val_Loss: 0.7569, Val_Acc: 0.7800\nEpoch 713, Loss: 0.8122, Acc: 0.5786, Val_Loss: 0.7608, Val_Acc: 0.7760\nEpoch 714, Loss: 0.7133, Acc: 0.6429, Val_Loss: 0.7662, Val_Acc: 0.7700\nEpoch 715, Loss: 0.6632, Acc: 0.6429, Val_Loss: 0.7770, Val_Acc: 0.7720\nEpoch 716, Loss: 0.7339, Acc: 0.6214, Val_Loss: 0.7883, Val_Acc: 0.7740\nEpoch 717, Loss: 0.6895, Acc: 0.6429, Val_Loss: 0.8077, Val_Acc: 0.7720\nEpoch 718, Loss: 0.6797, Acc: 0.6571, Val_Loss: 0.8270, Val_Acc: 0.7700\nEpoch 719, Loss: 0.7187, Acc: 0.6143, Val_Loss: 0.8442, Val_Acc: 0.7680\nEpoch 720, Loss: 0.6757, Acc: 0.6286, Val_Loss: 0.8553, Val_Acc: 0.7680\nEpoch 721, Loss: 0.7728, Acc: 0.5857, Val_Loss: 0.8562, Val_Acc: 0.7640\nEpoch 722, Loss: 0.7371, Acc: 0.6143, Val_Loss: 0.8454, Val_Acc: 0.7680\nEpoch 723, Loss: 0.7319, Acc: 0.6214, Val_Loss: 0.8291, Val_Acc: 0.7760\nEpoch 724, Loss: 0.7291, Acc: 0.6500, Val_Loss: 0.8152, Val_Acc: 0.7800\nEpoch 725, Loss: 0.7216, Acc: 0.6214, Val_Loss: 0.8016, Val_Acc: 0.7780\nEpoch 726, Loss: 0.7512, Acc: 0.5857, Val_Loss: 0.7939, Val_Acc: 0.7760\nEpoch 727, Loss: 0.7129, Acc: 0.6357, Val_Loss: 0.7901, Val_Acc: 0.7740\nEpoch 728, Loss: 0.5925, Acc: 0.7357, Val_Loss: 0.7902, Val_Acc: 0.7740\nEpoch 729, Loss: 0.7359, Acc: 0.6429, Val_Loss: 0.7890, Val_Acc: 0.7740\nEpoch 730, Loss: 0.7436, Acc: 0.6143, Val_Loss: 0.7868, Val_Acc: 0.7700\nEpoch 731, Loss: 0.6580, Acc: 0.6786, Val_Loss: 0.7844, Val_Acc: 0.7700\nEpoch 732, Loss: 0.5785, Acc: 0.6929, Val_Loss: 0.7839, Val_Acc: 0.7720\nEpoch 733, Loss: 0.6355, Acc: 0.6929, Val_Loss: 0.7849, Val_Acc: 0.7740\nEpoch 734, Loss: 0.6790, Acc: 0.6571, Val_Loss: 0.7824, Val_Acc: 0.7780\nEpoch 735, Loss: 0.6984, Acc: 0.6500, Val_Loss: 0.7813, Val_Acc: 0.7780\nEpoch 736, Loss: 0.6882, Acc: 0.6500, Val_Loss: 0.7796, Val_Acc: 0.7780\nEpoch 737, Loss: 0.7105, Acc: 0.6714, Val_Loss: 0.7757, Val_Acc: 0.7820\nEpoch 738, Loss: 0.6769, Acc: 0.6500, Val_Loss: 0.7713, Val_Acc: 0.7840\nEpoch 739, Loss: 0.6723, Acc: 0.6500, Val_Loss: 0.7698, Val_Acc: 0.7840\nEpoch 740, Loss: 0.6396, Acc: 0.6571, Val_Loss: 0.7720, Val_Acc: 0.7740\nEpoch 741, Loss: 0.6744, Acc: 0.6571, Val_Loss: 0.7811, Val_Acc: 0.7680\nEpoch 742, Loss: 0.7067, Acc: 0.6429, Val_Loss: 0.7929, Val_Acc: 0.7640\nEpoch 743, Loss: 0.6969, Acc: 0.6357, Val_Loss: 0.8075, Val_Acc: 0.7600\nEpoch 744, Loss: 0.7024, Acc: 0.6214, Val_Loss: 0.8196, Val_Acc: 0.7600\nEpoch 745, Loss: 0.7590, Acc: 0.6143, Val_Loss: 0.8293, Val_Acc: 0.7640\nEpoch 746, Loss: 0.5830, Acc: 0.7071, Val_Loss: 0.8383, Val_Acc: 0.7580\nEpoch 747, Loss: 0.7681, Acc: 0.5929, Val_Loss: 0.8451, Val_Acc: 0.7700\nEpoch 748, Loss: 0.6653, Acc: 0.6357, Val_Loss: 0.8477, Val_Acc: 0.7700\nEpoch 749, Loss: 0.6723, Acc: 0.6643, Val_Loss: 0.8453, Val_Acc: 0.7660\nEpoch 750, Loss: 0.7799, Acc: 0.5786, Val_Loss: 0.8348, Val_Acc: 0.7720\nEpoch 751, Loss: 0.7013, Acc: 0.6429, Val_Loss: 0.8165, Val_Acc: 0.7720\nEpoch 752, Loss: 0.6268, Acc: 0.7214, Val_Loss: 0.8002, Val_Acc: 0.7760\nEpoch 753, Loss: 0.6128, Acc: 0.7143, Val_Loss: 0.7829, Val_Acc: 0.7760\nEpoch 754, Loss: 0.6370, Acc: 0.6857, Val_Loss: 0.7785, Val_Acc: 0.7620\nEpoch 755, Loss: 0.6032, Acc: 0.7143, Val_Loss: 0.7845, Val_Acc: 0.7620\nEpoch 756, Loss: 0.7603, Acc: 0.5786, Val_Loss: 0.8001, Val_Acc: 0.7560\nEpoch 757, Loss: 0.7406, Acc: 0.6071, Val_Loss: 0.7997, Val_Acc: 0.7560\nEpoch 758, Loss: 0.7216, Acc: 0.6071, Val_Loss: 0.7936, Val_Acc: 0.7560\nEpoch 759, Loss: 0.7040, Acc: 0.6143, Val_Loss: 0.7864, Val_Acc: 0.7560\nEpoch 760, Loss: 0.7378, Acc: 0.5857, Val_Loss: 0.7921, Val_Acc: 0.7600\nEpoch 761, Loss: 0.7426, Acc: 0.5857, Val_Loss: 0.8020, Val_Acc: 0.7680\nEpoch 762, Loss: 0.7942, Acc: 0.5786, Val_Loss: 0.8115, Val_Acc: 0.7660\nEpoch 763, Loss: 0.6951, Acc: 0.6357, Val_Loss: 0.8212, Val_Acc: 0.7600\nEpoch 764, Loss: 0.7304, Acc: 0.5929, Val_Loss: 0.8238, Val_Acc: 0.7620\nEpoch 765, Loss: 0.6968, Acc: 0.6500, Val_Loss: 0.8226, Val_Acc: 0.7580\nEpoch 766, Loss: 0.8020, Acc: 0.5643, Val_Loss: 0.8210, Val_Acc: 0.7600\nEpoch 767, Loss: 0.7628, Acc: 0.5786, Val_Loss: 0.8201, Val_Acc: 0.7620\nEpoch 768, Loss: 0.7283, Acc: 0.6214, Val_Loss: 0.8122, Val_Acc: 0.7600\nEpoch 769, Loss: 0.7024, Acc: 0.6286, Val_Loss: 0.8060, Val_Acc: 0.7540\nEpoch 770, Loss: 0.6404, Acc: 0.6857, Val_Loss: 0.8011, Val_Acc: 0.7520\nEpoch 771, Loss: 0.7211, Acc: 0.6214, Val_Loss: 0.7971, Val_Acc: 0.7560\nEpoch 772, Loss: 0.6393, Acc: 0.6714, Val_Loss: 0.7935, Val_Acc: 0.7480\nEpoch 773, Loss: 0.6955, Acc: 0.6500, Val_Loss: 0.7901, Val_Acc: 0.7500\nEpoch 774, Loss: 0.6490, Acc: 0.6357, Val_Loss: 0.7860, Val_Acc: 0.7560\nEpoch 775, Loss: 0.6979, Acc: 0.6357, Val_Loss: 0.7889, Val_Acc: 0.7560\nEpoch 776, Loss: 0.6775, Acc: 0.6500, Val_Loss: 0.8012, Val_Acc: 0.7520\nEpoch 777, Loss: 0.6269, Acc: 0.6786, Val_Loss: 0.8160, Val_Acc: 0.7540\nEpoch 778, Loss: 0.7004, Acc: 0.6643, Val_Loss: 0.8254, Val_Acc: 0.7580\nEpoch 779, Loss: 0.7712, Acc: 0.5929, Val_Loss: 0.8374, Val_Acc: 0.7660\nEpoch 780, Loss: 0.6837, Acc: 0.6286, Val_Loss: 0.8511, Val_Acc: 0.7640\nEpoch 781, Loss: 0.7247, Acc: 0.6071, Val_Loss: 0.8584, Val_Acc: 0.7660\nEpoch 782, Loss: 0.7154, Acc: 0.6357, Val_Loss: 0.8603, Val_Acc: 0.7640\nEpoch 783, Loss: 0.6856, Acc: 0.6143, Val_Loss: 0.8554, Val_Acc: 0.7620\nEpoch 784, Loss: 0.7724, Acc: 0.5429, Val_Loss: 0.8465, Val_Acc: 0.7560\nEpoch 785, Loss: 0.6406, Acc: 0.6500, Val_Loss: 0.8162, Val_Acc: 0.7600\nEpoch 786, Loss: 0.6089, Acc: 0.6643, Val_Loss: 0.7915, Val_Acc: 0.7640\nEpoch 787, Loss: 0.7515, Acc: 0.6000, Val_Loss: 0.7757, Val_Acc: 0.7640\nEpoch 788, Loss: 0.6444, Acc: 0.6929, Val_Loss: 0.7725, Val_Acc: 0.7600\nEpoch 789, Loss: 0.6035, Acc: 0.6643, Val_Loss: 0.7718, Val_Acc: 0.7540\nEpoch 790, Loss: 0.6446, Acc: 0.6500, Val_Loss: 0.7740, Val_Acc: 0.7540\nEpoch 791, Loss: 0.7110, Acc: 0.6286, Val_Loss: 0.7750, Val_Acc: 0.7580\nEpoch 792, Loss: 0.5439, Acc: 0.7357, Val_Loss: 0.7780, Val_Acc: 0.7580\nEpoch 793, Loss: 0.6152, Acc: 0.6857, Val_Loss: 0.7831, Val_Acc: 0.7600\nEpoch 794, Loss: 0.7766, Acc: 0.6000, Val_Loss: 0.7905, Val_Acc: 0.7640\nEpoch 795, Loss: 0.7095, Acc: 0.6429, Val_Loss: 0.7979, Val_Acc: 0.7640\nEpoch 796, Loss: 0.7282, Acc: 0.5857, Val_Loss: 0.8053, Val_Acc: 0.7620\nEpoch 797, Loss: 0.6273, Acc: 0.7000, Val_Loss: 0.8084, Val_Acc: 0.7620\nEpoch 798, Loss: 0.7164, Acc: 0.6500, Val_Loss: 0.8153, Val_Acc: 0.7620\nEpoch 799, Loss: 0.6252, Acc: 0.6786, Val_Loss: 0.8204, Val_Acc: 0.7600\nEpoch 800, Loss: 0.7264, Acc: 0.6500, Val_Loss: 0.8239, Val_Acc: 0.7640\nEpoch 801, Loss: 0.6763, Acc: 0.6643, Val_Loss: 0.8244, Val_Acc: 0.7620\nEpoch 802, Loss: 0.6813, Acc: 0.6143, Val_Loss: 0.8228, Val_Acc: 0.7620\nEpoch 803, Loss: 0.6148, Acc: 0.6714, Val_Loss: 0.8164, Val_Acc: 0.7640\nEpoch 804, Loss: 0.6862, Acc: 0.6571, Val_Loss: 0.8077, Val_Acc: 0.7620\nEpoch 805, Loss: 0.8242, Acc: 0.5857, Val_Loss: 0.7929, Val_Acc: 0.7620\nEpoch 806, Loss: 0.7790, Acc: 0.6000, Val_Loss: 0.7854, Val_Acc: 0.7580\nEpoch 807, Loss: 0.6367, Acc: 0.6571, Val_Loss: 0.7894, Val_Acc: 0.7580\nEpoch 808, Loss: 0.6852, Acc: 0.6429, Val_Loss: 0.7898, Val_Acc: 0.7600\nEpoch 809, Loss: 0.7866, Acc: 0.5786, Val_Loss: 0.7915, Val_Acc: 0.7680\nEpoch 810, Loss: 0.6555, Acc: 0.6357, Val_Loss: 0.7940, Val_Acc: 0.7680\nEpoch 811, Loss: 0.6493, Acc: 0.6929, Val_Loss: 0.7957, Val_Acc: 0.7720\nEpoch 812, Loss: 0.7638, Acc: 0.5714, Val_Loss: 0.7995, Val_Acc: 0.7720\nEpoch 813, Loss: 0.6292, Acc: 0.6714, Val_Loss: 0.8087, Val_Acc: 0.7660\nEpoch 814, Loss: 0.7713, Acc: 0.6000, Val_Loss: 0.8199, Val_Acc: 0.7600\nEpoch 815, Loss: 0.6825, Acc: 0.6429, Val_Loss: 0.8280, Val_Acc: 0.7560\nEpoch 816, Loss: 0.6223, Acc: 0.6786, Val_Loss: 0.8304, Val_Acc: 0.7580\nEpoch 817, Loss: 0.7736, Acc: 0.5929, Val_Loss: 0.8302, Val_Acc: 0.7640\nEpoch 818, Loss: 0.7573, Acc: 0.6357, Val_Loss: 0.8207, Val_Acc: 0.7680\nEpoch 819, Loss: 0.6595, Acc: 0.6571, Val_Loss: 0.8118, Val_Acc: 0.7640\nEpoch 820, Loss: 0.6726, Acc: 0.6214, Val_Loss: 0.7987, Val_Acc: 0.7680\nEpoch 821, Loss: 0.7334, Acc: 0.6214, Val_Loss: 0.7918, Val_Acc: 0.7720\nEpoch 822, Loss: 0.6446, Acc: 0.6500, Val_Loss: 0.7891, Val_Acc: 0.7760\nEpoch 823, Loss: 0.6213, Acc: 0.6714, Val_Loss: 0.7869, Val_Acc: 0.7780\nEpoch 824, Loss: 0.6972, Acc: 0.6357, Val_Loss: 0.7873, Val_Acc: 0.7760\nEpoch 825, Loss: 0.7216, Acc: 0.6500, Val_Loss: 0.7926, Val_Acc: 0.7760\nEpoch 826, Loss: 0.6755, Acc: 0.6571, Val_Loss: 0.7965, Val_Acc: 0.7740\nEpoch 827, Loss: 0.7064, Acc: 0.6357, Val_Loss: 0.8020, Val_Acc: 0.7700\nEpoch 828, Loss: 0.7375, Acc: 0.5929, Val_Loss: 0.8037, Val_Acc: 0.7680\nEpoch 829, Loss: 0.5540, Acc: 0.7357, Val_Loss: 0.8064, Val_Acc: 0.7640\nEpoch 830, Loss: 0.7541, Acc: 0.5929, Val_Loss: 0.8087, Val_Acc: 0.7620\nEpoch 831, Loss: 0.7058, Acc: 0.6286, Val_Loss: 0.8070, Val_Acc: 0.7640\nEpoch 832, Loss: 0.6448, Acc: 0.6786, Val_Loss: 0.8073, Val_Acc: 0.7600\nEpoch 833, Loss: 0.7008, Acc: 0.6214, Val_Loss: 0.8052, Val_Acc: 0.7660\nEpoch 834, Loss: 0.7669, Acc: 0.6143, Val_Loss: 0.8001, Val_Acc: 0.7600\nEpoch 835, Loss: 0.6918, Acc: 0.6643, Val_Loss: 0.7965, Val_Acc: 0.7580\nEpoch 836, Loss: 0.6223, Acc: 0.6571, Val_Loss: 0.7936, Val_Acc: 0.7580\nEpoch 837, Loss: 0.6562, Acc: 0.6786, Val_Loss: 0.7899, Val_Acc: 0.7640\nEpoch 838, Loss: 0.6922, Acc: 0.6357, Val_Loss: 0.7907, Val_Acc: 0.7660\nEpoch 839, Loss: 0.7184, Acc: 0.6143, Val_Loss: 0.7960, Val_Acc: 0.7560\nEpoch 840, Loss: 0.7044, Acc: 0.5857, Val_Loss: 0.8013, Val_Acc: 0.7600\nEpoch 841, Loss: 0.6659, Acc: 0.6286, Val_Loss: 0.8004, Val_Acc: 0.7580\nEpoch 842, Loss: 0.6670, Acc: 0.6286, Val_Loss: 0.7970, Val_Acc: 0.7580\nEpoch 843, Loss: 0.6799, Acc: 0.6214, Val_Loss: 0.7962, Val_Acc: 0.7600\nEpoch 844, Loss: 0.6293, Acc: 0.6786, Val_Loss: 0.8043, Val_Acc: 0.7620\nEpoch 845, Loss: 0.6455, Acc: 0.6786, Val_Loss: 0.7947, Val_Acc: 0.7560\nEpoch 846, Loss: 0.6944, Acc: 0.6357, Val_Loss: 0.7899, Val_Acc: 0.7620\nEpoch 847, Loss: 0.7345, Acc: 0.6143, Val_Loss: 0.7826, Val_Acc: 0.7640\nEpoch 848, Loss: 0.7513, Acc: 0.5857, Val_Loss: 0.7866, Val_Acc: 0.7640\nEpoch 849, Loss: 0.7531, Acc: 0.6214, Val_Loss: 0.7912, Val_Acc: 0.7700\nEpoch 850, Loss: 0.7378, Acc: 0.6000, Val_Loss: 0.7898, Val_Acc: 0.7680\nEpoch 851, Loss: 0.7440, Acc: 0.6000, Val_Loss: 0.7905, Val_Acc: 0.7700\nEpoch 852, Loss: 0.6703, Acc: 0.6571, Val_Loss: 0.7970, Val_Acc: 0.7640\nEpoch 853, Loss: 0.6799, Acc: 0.6429, Val_Loss: 0.8063, Val_Acc: 0.7580\nEpoch 854, Loss: 0.6176, Acc: 0.6857, Val_Loss: 0.8100, Val_Acc: 0.7600\nEpoch 855, Loss: 0.6694, Acc: 0.6857, Val_Loss: 0.8135, Val_Acc: 0.7600\nEpoch 856, Loss: 0.6388, Acc: 0.6571, Val_Loss: 0.8179, Val_Acc: 0.7640\nEpoch 857, Loss: 0.6451, Acc: 0.6500, Val_Loss: 0.8185, Val_Acc: 0.7620\nEpoch 858, Loss: 0.6157, Acc: 0.6714, Val_Loss: 0.8181, Val_Acc: 0.7580\nEpoch 859, Loss: 0.7162, Acc: 0.6643, Val_Loss: 0.7983, Val_Acc: 0.7580\nEpoch 860, Loss: 0.6135, Acc: 0.7000, Val_Loss: 0.7770, Val_Acc: 0.7680\nEpoch 861, Loss: 0.6727, Acc: 0.6143, Val_Loss: 0.7611, Val_Acc: 0.7760\nEpoch 862, Loss: 0.6462, Acc: 0.6000, Val_Loss: 0.7548, Val_Acc: 0.7780\nEpoch 863, Loss: 0.7154, Acc: 0.5929, Val_Loss: 0.7529, Val_Acc: 0.7760\nEpoch 864, Loss: 0.7311, Acc: 0.5929, Val_Loss: 0.7524, Val_Acc: 0.7760\nEpoch 865, Loss: 0.6917, Acc: 0.5857, Val_Loss: 0.7536, Val_Acc: 0.7840\nEpoch 866, Loss: 0.7265, Acc: 0.6000, Val_Loss: 0.7614, Val_Acc: 0.7740\nEpoch 867, Loss: 0.8156, Acc: 0.5714, Val_Loss: 0.7766, Val_Acc: 0.7740\nEpoch 868, Loss: 0.7366, Acc: 0.6000, Val_Loss: 0.7979, Val_Acc: 0.7620\nEpoch 869, Loss: 0.7061, Acc: 0.6357, Val_Loss: 0.8274, Val_Acc: 0.7620\nEpoch 870, Loss: 0.8035, Acc: 0.5714, Val_Loss: 0.8560, Val_Acc: 0.7560\nEpoch 871, Loss: 0.7012, Acc: 0.6500, Val_Loss: 0.8656, Val_Acc: 0.7520\nEpoch 872, Loss: 0.7487, Acc: 0.5857, Val_Loss: 0.8686, Val_Acc: 0.7500\nEpoch 873, Loss: 0.6904, Acc: 0.6857, Val_Loss: 0.8586, Val_Acc: 0.7520\nEpoch 874, Loss: 0.6782, Acc: 0.6571, Val_Loss: 0.8426, Val_Acc: 0.7500\nEpoch 875, Loss: 0.7531, Acc: 0.5714, Val_Loss: 0.8249, Val_Acc: 0.7580\nEpoch 876, Loss: 0.7351, Acc: 0.6071, Val_Loss: 0.8078, Val_Acc: 0.7640\nEpoch 877, Loss: 0.7555, Acc: 0.5786, Val_Loss: 0.7935, Val_Acc: 0.7680\nEpoch 878, Loss: 0.5922, Acc: 0.6857, Val_Loss: 0.7843, Val_Acc: 0.7680\nEpoch 879, Loss: 0.6468, Acc: 0.6929, Val_Loss: 0.7778, Val_Acc: 0.7700\nEpoch 880, Loss: 0.7047, Acc: 0.6143, Val_Loss: 0.7745, Val_Acc: 0.7680\nEpoch 881, Loss: 0.6954, Acc: 0.6571, Val_Loss: 0.7746, Val_Acc: 0.7660\nEpoch 882, Loss: 0.6023, Acc: 0.7071, Val_Loss: 0.7769, Val_Acc: 0.7660\nEpoch 883, Loss: 0.7204, Acc: 0.6357, Val_Loss: 0.7800, Val_Acc: 0.7700\nEpoch 884, Loss: 0.7045, Acc: 0.6071, Val_Loss: 0.7870, Val_Acc: 0.7660\nEpoch 885, Loss: 0.6627, Acc: 0.6714, Val_Loss: 0.7977, Val_Acc: 0.7620\nEpoch 886, Loss: 0.6046, Acc: 0.6571, Val_Loss: 0.8129, Val_Acc: 0.7600\nEpoch 887, Loss: 0.6899, Acc: 0.6714, Val_Loss: 0.8278, Val_Acc: 0.7580\nEpoch 888, Loss: 0.6963, Acc: 0.6286, Val_Loss: 0.8426, Val_Acc: 0.7580\nEpoch 889, Loss: 0.6769, Acc: 0.6286, Val_Loss: 0.8383, Val_Acc: 0.7620\nEpoch 890, Loss: 0.5598, Acc: 0.7071, Val_Loss: 0.8297, Val_Acc: 0.7600\nEpoch 891, Loss: 0.6076, Acc: 0.7000, Val_Loss: 0.8179, Val_Acc: 0.7600\nEpoch 892, Loss: 0.8107, Acc: 0.6071, Val_Loss: 0.8064, Val_Acc: 0.7700\nEpoch 893, Loss: 0.7658, Acc: 0.5786, Val_Loss: 0.8003, Val_Acc: 0.7720\nEpoch 894, Loss: 0.6955, Acc: 0.6429, Val_Loss: 0.7956, Val_Acc: 0.7700\nEpoch 895, Loss: 0.6800, Acc: 0.6571, Val_Loss: 0.7865, Val_Acc: 0.7660\nEpoch 896, Loss: 0.6321, Acc: 0.7000, Val_Loss: 0.7759, Val_Acc: 0.7720\nEpoch 897, Loss: 0.6667, Acc: 0.6214, Val_Loss: 0.7671, Val_Acc: 0.7700\nEpoch 898, Loss: 0.7098, Acc: 0.6000, Val_Loss: 0.7603, Val_Acc: 0.7740\nEpoch 899, Loss: 0.6795, Acc: 0.6357, Val_Loss: 0.7602, Val_Acc: 0.7720\nEpoch 900, Loss: 0.6639, Acc: 0.6643, Val_Loss: 0.7628, Val_Acc: 0.7740\nEpoch 901, Loss: 0.6783, Acc: 0.6000, Val_Loss: 0.7647, Val_Acc: 0.7740\nEpoch 902, Loss: 0.7171, Acc: 0.6286, Val_Loss: 0.7673, Val_Acc: 0.7760\nEpoch 903, Loss: 0.7677, Acc: 0.5857, Val_Loss: 0.7701, Val_Acc: 0.7780\nEpoch 904, Loss: 0.6943, Acc: 0.6714, Val_Loss: 0.7634, Val_Acc: 0.7780\nEpoch 905, Loss: 0.5793, Acc: 0.7000, Val_Loss: 0.7597, Val_Acc: 0.7780\nEpoch 906, Loss: 0.6704, Acc: 0.6214, Val_Loss: 0.7541, Val_Acc: 0.7820\nEpoch 907, Loss: 0.7144, Acc: 0.6500, Val_Loss: 0.7538, Val_Acc: 0.7840\nEpoch 908, Loss: 0.6162, Acc: 0.7071, Val_Loss: 0.7549, Val_Acc: 0.7820\nEpoch 909, Loss: 0.6725, Acc: 0.6429, Val_Loss: 0.7665, Val_Acc: 0.7800\nEpoch 910, Loss: 0.8107, Acc: 0.5500, Val_Loss: 0.7839, Val_Acc: 0.7740\nEpoch 911, Loss: 0.6849, Acc: 0.6357, Val_Loss: 0.8006, Val_Acc: 0.7680\nEpoch 912, Loss: 0.6538, Acc: 0.6500, Val_Loss: 0.8137, Val_Acc: 0.7720\nEpoch 913, Loss: 0.7502, Acc: 0.6143, Val_Loss: 0.8216, Val_Acc: 0.7720\nEpoch 914, Loss: 0.7414, Acc: 0.5857, Val_Loss: 0.8062, Val_Acc: 0.7720\nEpoch 915, Loss: 0.7209, Acc: 0.6143, Val_Loss: 0.7906, Val_Acc: 0.7640\nEpoch 916, Loss: 0.6659, Acc: 0.6429, Val_Loss: 0.7815, Val_Acc: 0.7600\nEpoch 917, Loss: 0.7227, Acc: 0.6000, Val_Loss: 0.7786, Val_Acc: 0.7600\nEpoch 918, Loss: 0.7696, Acc: 0.6000, Val_Loss: 0.7760, Val_Acc: 0.7600\nEpoch 919, Loss: 0.7126, Acc: 0.6214, Val_Loss: 0.7669, Val_Acc: 0.7620\nEpoch 920, Loss: 0.6417, Acc: 0.6786, Val_Loss: 0.7608, Val_Acc: 0.7660\nEpoch 921, Loss: 0.6311, Acc: 0.6500, Val_Loss: 0.7585, Val_Acc: 0.7740\nEpoch 922, Loss: 0.7610, Acc: 0.6071, Val_Loss: 0.7635, Val_Acc: 0.7780\nEpoch 923, Loss: 0.6926, Acc: 0.6286, Val_Loss: 0.7769, Val_Acc: 0.7740\nEpoch 924, Loss: 0.7572, Acc: 0.5929, Val_Loss: 0.7879, Val_Acc: 0.7760\nEpoch 925, Loss: 0.6580, Acc: 0.6357, Val_Loss: 0.7921, Val_Acc: 0.7700\nEpoch 926, Loss: 0.6617, Acc: 0.6643, Val_Loss: 0.7986, Val_Acc: 0.7700\nEpoch 927, Loss: 0.5911, Acc: 0.6929, Val_Loss: 0.8070, Val_Acc: 0.7620\nEpoch 928, Loss: 0.7011, Acc: 0.6071, Val_Loss: 0.8080, Val_Acc: 0.7540\nEpoch 929, Loss: 0.7230, Acc: 0.6429, Val_Loss: 0.8076, Val_Acc: 0.7580\nEpoch 930, Loss: 0.5521, Acc: 0.7429, Val_Loss: 0.8035, Val_Acc: 0.7580\nEpoch 931, Loss: 0.6551, Acc: 0.6429, Val_Loss: 0.7992, Val_Acc: 0.7620\nEpoch 932, Loss: 0.7540, Acc: 0.6000, Val_Loss: 0.7988, Val_Acc: 0.7660\nEpoch 933, Loss: 0.7593, Acc: 0.6500, Val_Loss: 0.8019, Val_Acc: 0.7560\nEpoch 934, Loss: 0.7637, Acc: 0.6214, Val_Loss: 0.8082, Val_Acc: 0.7580\nEpoch 935, Loss: 0.5798, Acc: 0.7214, Val_Loss: 0.8175, Val_Acc: 0.7620\nEpoch 936, Loss: 0.6828, Acc: 0.6286, Val_Loss: 0.8266, Val_Acc: 0.7580\nEpoch 937, Loss: 0.7586, Acc: 0.6357, Val_Loss: 0.8332, Val_Acc: 0.7600\nEpoch 938, Loss: 0.6782, Acc: 0.6286, Val_Loss: 0.8264, Val_Acc: 0.7600\nEpoch 939, Loss: 0.7377, Acc: 0.5857, Val_Loss: 0.8273, Val_Acc: 0.7600\nEpoch 940, Loss: 0.6520, Acc: 0.6857, Val_Loss: 0.8251, Val_Acc: 0.7580\nEpoch 941, Loss: 0.6867, Acc: 0.6286, Val_Loss: 0.8186, Val_Acc: 0.7580\nEpoch 942, Loss: 0.6568, Acc: 0.6714, Val_Loss: 0.8145, Val_Acc: 0.7620\nEpoch 943, Loss: 0.7558, Acc: 0.5857, Val_Loss: 0.8048, Val_Acc: 0.7620\nEpoch 944, Loss: 0.6863, Acc: 0.6786, Val_Loss: 0.7991, Val_Acc: 0.7700\nEpoch 945, Loss: 0.8295, Acc: 0.5786, Val_Loss: 0.7941, Val_Acc: 0.7780\nEpoch 946, Loss: 0.6739, Acc: 0.6786, Val_Loss: 0.7939, Val_Acc: 0.7780\nEpoch 947, Loss: 0.7394, Acc: 0.5786, Val_Loss: 0.7960, Val_Acc: 0.7720\nEpoch 948, Loss: 0.6315, Acc: 0.6429, Val_Loss: 0.8035, Val_Acc: 0.7700\nEpoch 949, Loss: 0.7249, Acc: 0.6500, Val_Loss: 0.8152, Val_Acc: 0.7700\nEpoch 950, Loss: 0.6574, Acc: 0.6214, Val_Loss: 0.8339, Val_Acc: 0.7680\nEpoch 951, Loss: 0.7336, Acc: 0.6143, Val_Loss: 0.8444, Val_Acc: 0.7660\nEpoch 952, Loss: 0.7060, Acc: 0.6357, Val_Loss: 0.8530, Val_Acc: 0.7620\nEpoch 953, Loss: 0.6085, Acc: 0.6571, Val_Loss: 0.8579, Val_Acc: 0.7560\nEpoch 954, Loss: 0.6950, Acc: 0.6357, Val_Loss: 0.8579, Val_Acc: 0.7580\nEpoch 955, Loss: 0.6880, Acc: 0.6429, Val_Loss: 0.8475, Val_Acc: 0.7560\nEpoch 956, Loss: 0.8230, Acc: 0.5357, Val_Loss: 0.8328, Val_Acc: 0.7500\nEpoch 957, Loss: 0.6606, Acc: 0.6786, Val_Loss: 0.8184, Val_Acc: 0.7520\nEpoch 958, Loss: 0.7297, Acc: 0.6071, Val_Loss: 0.8078, Val_Acc: 0.7600\nEpoch 959, Loss: 0.7869, Acc: 0.5786, Val_Loss: 0.7973, Val_Acc: 0.7680\nEpoch 960, Loss: 0.8043, Acc: 0.5571, Val_Loss: 0.7860, Val_Acc: 0.7720\nEpoch 961, Loss: 0.7358, Acc: 0.6071, Val_Loss: 0.7808, Val_Acc: 0.7720\nEpoch 962, Loss: 0.7133, Acc: 0.6286, Val_Loss: 0.7773, Val_Acc: 0.7700\nEpoch 963, Loss: 0.7112, Acc: 0.6143, Val_Loss: 0.7759, Val_Acc: 0.7660\nEpoch 964, Loss: 0.6385, Acc: 0.7071, Val_Loss: 0.7776, Val_Acc: 0.7660\nEpoch 965, Loss: 0.7576, Acc: 0.5857, Val_Loss: 0.7797, Val_Acc: 0.7640\nEpoch 966, Loss: 0.6339, Acc: 0.6643, Val_Loss: 0.7834, Val_Acc: 0.7580\nEpoch 967, Loss: 0.6382, Acc: 0.6714, Val_Loss: 0.7883, Val_Acc: 0.7620\nEpoch 968, Loss: 0.6719, Acc: 0.6714, Val_Loss: 0.8014, Val_Acc: 0.7600\nEpoch 969, Loss: 0.6179, Acc: 0.6786, Val_Loss: 0.8047, Val_Acc: 0.7640\nEpoch 970, Loss: 0.6148, Acc: 0.6500, Val_Loss: 0.8082, Val_Acc: 0.7640\nEpoch 971, Loss: 0.6185, Acc: 0.6429, Val_Loss: 0.8234, Val_Acc: 0.7680\nEpoch 972, Loss: 0.5241, Acc: 0.7500, Val_Loss: 0.8423, Val_Acc: 0.7640\nEpoch 973, Loss: 0.6792, Acc: 0.6571, Val_Loss: 0.8451, Val_Acc: 0.7580\nEpoch 974, Loss: 0.7007, Acc: 0.6500, Val_Loss: 0.8379, Val_Acc: 0.7540\nEpoch 975, Loss: 0.7128, Acc: 0.6429, Val_Loss: 0.8381, Val_Acc: 0.7580\nEpoch 976, Loss: 0.6451, Acc: 0.6143, Val_Loss: 0.8391, Val_Acc: 0.7620\nEpoch 977, Loss: 0.6540, Acc: 0.6286, Val_Loss: 0.8400, Val_Acc: 0.7600\nEpoch 978, Loss: 0.7402, Acc: 0.6214, Val_Loss: 0.8518, Val_Acc: 0.7580\nEpoch 979, Loss: 0.6942, Acc: 0.6214, Val_Loss: 0.8605, Val_Acc: 0.7580\nEpoch 980, Loss: 0.7733, Acc: 0.6357, Val_Loss: 0.8665, Val_Acc: 0.7500\nEpoch 981, Loss: 0.7028, Acc: 0.6357, Val_Loss: 0.8703, Val_Acc: 0.7500\nEpoch 982, Loss: 0.6219, Acc: 0.6500, Val_Loss: 0.8682, Val_Acc: 0.7520\nEpoch 983, Loss: 0.7765, Acc: 0.6357, Val_Loss: 0.8606, Val_Acc: 0.7500\nEpoch 984, Loss: 0.6738, Acc: 0.6571, Val_Loss: 0.8466, Val_Acc: 0.7540\nEpoch 985, Loss: 0.6182, Acc: 0.6571, Val_Loss: 0.8331, Val_Acc: 0.7600\nEpoch 986, Loss: 0.5781, Acc: 0.7357, Val_Loss: 0.8118, Val_Acc: 0.7720\nEpoch 987, Loss: 0.6991, Acc: 0.6286, Val_Loss: 0.7988, Val_Acc: 0.7660\nEpoch 988, Loss: 0.7540, Acc: 0.5786, Val_Loss: 0.7896, Val_Acc: 0.7640\nEpoch 989, Loss: 0.7389, Acc: 0.5714, Val_Loss: 0.7843, Val_Acc: 0.7600\nEpoch 990, Loss: 0.5998, Acc: 0.6786, Val_Loss: 0.7782, Val_Acc: 0.7620\nEpoch 991, Loss: 0.7497, Acc: 0.5786, Val_Loss: 0.7765, Val_Acc: 0.7580\nEpoch 992, Loss: 0.6485, Acc: 0.6286, Val_Loss: 0.7794, Val_Acc: 0.7580\nEpoch 993, Loss: 0.7105, Acc: 0.6429, Val_Loss: 0.7800, Val_Acc: 0.7560\nEpoch 994, Loss: 0.6144, Acc: 0.6857, Val_Loss: 0.7822, Val_Acc: 0.7580\nEpoch 995, Loss: 0.5740, Acc: 0.6929, Val_Loss: 0.7907, Val_Acc: 0.7680\nEpoch 996, Loss: 0.7128, Acc: 0.6714, Val_Loss: 0.8030, Val_Acc: 0.7700\nEpoch 997, Loss: 0.7258, Acc: 0.6000, Val_Loss: 0.8201, Val_Acc: 0.7660\nEpoch 998, Loss: 0.6449, Acc: 0.6643, Val_Loss: 0.8388, Val_Acc: 0.7640\nEpoch 999, Loss: 0.7479, Acc: 0.6214, Val_Loss: 0.8611, Val_Acc: 0.7640\nEpoch 1000, Loss: 0.5695, Acc: 0.7143, Val_Loss: 0.8706, Val_Acc: 0.7580\nEpoch 1001, Loss: 0.6785, Acc: 0.6214, Val_Loss: 0.8737, Val_Acc: 0.7580\nEpoch 1002, Loss: 0.7150, Acc: 0.6214, Val_Loss: 0.8712, Val_Acc: 0.7600\nEpoch 1003, Loss: 0.6027, Acc: 0.6857, Val_Loss: 0.8534, Val_Acc: 0.7620\nEpoch 1004, Loss: 0.6608, Acc: 0.6571, Val_Loss: 0.8396, Val_Acc: 0.7660\nEpoch 1005, Loss: 0.5175, Acc: 0.7214, Val_Loss: 0.8305, Val_Acc: 0.7660\nEpoch 1006, Loss: 0.6163, Acc: 0.6857, Val_Loss: 0.8235, Val_Acc: 0.7640\nEpoch 1007, Loss: 0.7554, Acc: 0.6071, Val_Loss: 0.8211, Val_Acc: 0.7600\nEpoch 1008, Loss: 0.7499, Acc: 0.5786, Val_Loss: 0.8179, Val_Acc: 0.7620\nEpoch 1009, Loss: 0.7675, Acc: 0.6071, Val_Loss: 0.8088, Val_Acc: 0.7540\nEpoch 1010, Loss: 0.6157, Acc: 0.6786, Val_Loss: 0.7982, Val_Acc: 0.7580\nEpoch 1011, Loss: 0.6893, Acc: 0.6357, Val_Loss: 0.7849, Val_Acc: 0.7660\nEpoch 1012, Loss: 0.6130, Acc: 0.7000, Val_Loss: 0.7733, Val_Acc: 0.7740\nEpoch 1013, Loss: 0.6877, Acc: 0.6071, Val_Loss: 0.7664, Val_Acc: 0.7700\nEpoch 1014, Loss: 0.7408, Acc: 0.5786, Val_Loss: 0.7651, Val_Acc: 0.7720\nEpoch 1015, Loss: 0.5926, Acc: 0.6857, Val_Loss: 0.7659, Val_Acc: 0.7720\nEpoch 1016, Loss: 0.6503, Acc: 0.6714, Val_Loss: 0.7669, Val_Acc: 0.7740\nEpoch 1017, Loss: 0.7441, Acc: 0.6429, Val_Loss: 0.7689, Val_Acc: 0.7740\nEpoch 1018, Loss: 0.6875, Acc: 0.6286, Val_Loss: 0.7731, Val_Acc: 0.7740\nEpoch 1019, Loss: 0.7257, Acc: 0.6357, Val_Loss: 0.7859, Val_Acc: 0.7740\nEpoch 1020, Loss: 0.7312, Acc: 0.6000, Val_Loss: 0.7993, Val_Acc: 0.7720\nEpoch 1021, Loss: 0.6865, Acc: 0.6286, Val_Loss: 0.8188, Val_Acc: 0.7680\nEpoch 1022, Loss: 0.6988, Acc: 0.6071, Val_Loss: 0.8348, Val_Acc: 0.7700\nEpoch 1023, Loss: 0.6882, Acc: 0.6071, Val_Loss: 0.8412, Val_Acc: 0.7660\nEpoch 1024, Loss: 0.6516, Acc: 0.6214, Val_Loss: 0.8403, Val_Acc: 0.7680\nEpoch 1025, Loss: 0.5698, Acc: 0.7286, Val_Loss: 0.8386, Val_Acc: 0.7700\nEpoch 1026, Loss: 0.6693, Acc: 0.6357, Val_Loss: 0.8296, Val_Acc: 0.7700\nEpoch 1027, Loss: 0.7663, Acc: 0.6000, Val_Loss: 0.8178, Val_Acc: 0.7720\nEpoch 1028, Loss: 0.7662, Acc: 0.5929, Val_Loss: 0.8069, Val_Acc: 0.7680\nEpoch 1029, Loss: 0.7285, Acc: 0.6143, Val_Loss: 0.7977, Val_Acc: 0.7680\nEpoch 1030, Loss: 0.7041, Acc: 0.6643, Val_Loss: 0.7887, Val_Acc: 0.7720\nEpoch 1031, Loss: 0.7152, Acc: 0.6286, Val_Loss: 0.7849, Val_Acc: 0.7760\nEpoch 1032, Loss: 0.7119, Acc: 0.6357, Val_Loss: 0.7814, Val_Acc: 0.7740\nEpoch 1033, Loss: 0.6946, Acc: 0.6286, Val_Loss: 0.7774, Val_Acc: 0.7720\nEpoch 1034, Loss: 0.6574, Acc: 0.6786, Val_Loss: 0.7767, Val_Acc: 0.7800\nEpoch 1035, Loss: 0.6081, Acc: 0.7000, Val_Loss: 0.7849, Val_Acc: 0.7840\nEpoch 1036, Loss: 0.7375, Acc: 0.6071, Val_Loss: 0.7932, Val_Acc: 0.7820\nEpoch 1037, Loss: 0.7381, Acc: 0.6429, Val_Loss: 0.8039, Val_Acc: 0.7760\nEpoch 1038, Loss: 0.7292, Acc: 0.6214, Val_Loss: 0.8152, Val_Acc: 0.7780\nEpoch 1039, Loss: 0.6533, Acc: 0.6643, Val_Loss: 0.8171, Val_Acc: 0.7740\nEpoch 1040, Loss: 0.7686, Acc: 0.5714, Val_Loss: 0.8162, Val_Acc: 0.7660\nEpoch 1041, Loss: 0.7426, Acc: 0.6357, Val_Loss: 0.8125, Val_Acc: 0.7620\nEpoch 1042, Loss: 0.7066, Acc: 0.6643, Val_Loss: 0.8096, Val_Acc: 0.7580\nEpoch 1043, Loss: 0.6494, Acc: 0.6571, Val_Loss: 0.8040, Val_Acc: 0.7560\nEpoch 1044, Loss: 0.7165, Acc: 0.6214, Val_Loss: 0.7975, Val_Acc: 0.7540\nEpoch 1045, Loss: 0.7326, Acc: 0.6571, Val_Loss: 0.7941, Val_Acc: 0.7600\nEpoch 1046, Loss: 0.6831, Acc: 0.6643, Val_Loss: 0.7948, Val_Acc: 0.7640\nEpoch 1047, Loss: 0.6690, Acc: 0.6000, Val_Loss: 0.7976, Val_Acc: 0.7620\nEpoch 1048, Loss: 0.8194, Acc: 0.5929, Val_Loss: 0.7975, Val_Acc: 0.7620\nEpoch 1049, Loss: 0.7013, Acc: 0.6000, Val_Loss: 0.7929, Val_Acc: 0.7620\nEpoch 1050, Loss: 0.6248, Acc: 0.6786, Val_Loss: 0.7893, Val_Acc: 0.7680\nEpoch 1051, Loss: 0.7479, Acc: 0.6357, Val_Loss: 0.7888, Val_Acc: 0.7660\nEpoch 1052, Loss: 0.7229, Acc: 0.6143, Val_Loss: 0.7894, Val_Acc: 0.7640\nEpoch 1053, Loss: 0.7243, Acc: 0.6143, Val_Loss: 0.7885, Val_Acc: 0.7640\nEpoch 1054, Loss: 0.6490, Acc: 0.6571, Val_Loss: 0.7903, Val_Acc: 0.7660\nEpoch 1055, Loss: 0.6628, Acc: 0.6643, Val_Loss: 0.7943, Val_Acc: 0.7620\nEpoch 1056, Loss: 0.6064, Acc: 0.6786, Val_Loss: 0.7979, Val_Acc: 0.7640\nEpoch 1057, Loss: 0.6893, Acc: 0.6643, Val_Loss: 0.8016, Val_Acc: 0.7640\nEpoch 1058, Loss: 0.6951, Acc: 0.6071, Val_Loss: 0.8069, Val_Acc: 0.7640\nEpoch 1059, Loss: 0.6284, Acc: 0.6643, Val_Loss: 0.8178, Val_Acc: 0.7680\nEpoch 1060, Loss: 0.7829, Acc: 0.5929, Val_Loss: 0.8241, Val_Acc: 0.7660\nEpoch 1061, Loss: 0.5884, Acc: 0.7071, Val_Loss: 0.8194, Val_Acc: 0.7740\nEpoch 1062, Loss: 0.7333, Acc: 0.6214, Val_Loss: 0.8207, Val_Acc: 0.7680\nEpoch 1063, Loss: 0.6294, Acc: 0.6571, Val_Loss: 0.8206, Val_Acc: 0.7680\nEarly stopping triggered.\nBest model loaded from best_model.pth\nTest Accuracy: 0.7920\n","output_type":"stream"}]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Assume all_datasets dictionary is already populated as in the previous steps\n    dataset_name = 'Citeseer'  # Change this to Citeseer or Pubmed as needed\n    data = load_processed_data_from_dict(all_datasets[dataset_name])\n\n    # Model configuration based on the paper's setup\n    num_features = data.x.size(1)\n    num_classes = int(data.y.max()) + 1\n    hidden_dim = 16  # Set the hidden dimension to 16 as per the paper\n    num_views = 3\n\n    # Initialize the model with the specified architecture and dimensions\n    model = MAGCN(num_features, num_classes, num_views, hidden_dim)\n    \n    # Use Adam optimizer with the specified learning rate and weight decay\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n\n    # Train and test the model\n    train_model(model, data, optimizer, epochs=2000, early_stopping_patience=100)\n    test_model(model, data)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T16:53:04.511747Z","iopub.execute_input":"2024-08-12T16:53:04.512705Z","iopub.status.idle":"2024-08-12T16:53:28.413786Z","shell.execute_reply.started":"2024-08-12T16:53:04.512667Z","shell.execute_reply":"2024-08-12T16:53:28.412577Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.7988, Train Accuracy: 0.1417, Validation Loss: 1.7637, Validation Accuracy: 0.3060\nEpoch 2, Loss: 1.7441, Train Accuracy: 0.2417, Validation Loss: 1.7156, Validation Accuracy: 0.3880\nEpoch 3, Loss: 1.6648, Train Accuracy: 0.3750, Validation Loss: 1.6520, Validation Accuracy: 0.4880\nEpoch 4, Loss: 1.5810, Train Accuracy: 0.4500, Validation Loss: 1.5770, Validation Accuracy: 0.5720\nEpoch 5, Loss: 1.4979, Train Accuracy: 0.4167, Validation Loss: 1.5054, Validation Accuracy: 0.6260\nEpoch 6, Loss: 1.4429, Train Accuracy: 0.4167, Validation Loss: 1.4391, Validation Accuracy: 0.6740\nEpoch 7, Loss: 1.3084, Train Accuracy: 0.4667, Validation Loss: 1.3790, Validation Accuracy: 0.7100\nEpoch 8, Loss: 1.2223, Train Accuracy: 0.5333, Validation Loss: 1.3226, Validation Accuracy: 0.7120\nEpoch 9, Loss: 1.1668, Train Accuracy: 0.5333, Validation Loss: 1.2673, Validation Accuracy: 0.7160\nEpoch 10, Loss: 1.1263, Train Accuracy: 0.5667, Validation Loss: 1.2153, Validation Accuracy: 0.7220\nEpoch 11, Loss: 1.0811, Train Accuracy: 0.6000, Validation Loss: 1.1745, Validation Accuracy: 0.7260\nEpoch 12, Loss: 0.9930, Train Accuracy: 0.5917, Validation Loss: 1.1414, Validation Accuracy: 0.7220\nEpoch 13, Loss: 0.9905, Train Accuracy: 0.6250, Validation Loss: 1.1188, Validation Accuracy: 0.7180\nEpoch 14, Loss: 0.9539, Train Accuracy: 0.6000, Validation Loss: 1.1063, Validation Accuracy: 0.7020\nEpoch 15, Loss: 0.9795, Train Accuracy: 0.6083, Validation Loss: 1.1008, Validation Accuracy: 0.6920\nEpoch 16, Loss: 0.8037, Train Accuracy: 0.7000, Validation Loss: 1.0989, Validation Accuracy: 0.6840\nEpoch 17, Loss: 0.8487, Train Accuracy: 0.5917, Validation Loss: 1.0957, Validation Accuracy: 0.6760\nEpoch 18, Loss: 0.7392, Train Accuracy: 0.6500, Validation Loss: 1.0829, Validation Accuracy: 0.6740\nEpoch 19, Loss: 0.8339, Train Accuracy: 0.6167, Validation Loss: 1.0640, Validation Accuracy: 0.6780\nEpoch 20, Loss: 0.6801, Train Accuracy: 0.6833, Validation Loss: 1.0460, Validation Accuracy: 0.6740\nEpoch 21, Loss: 0.8001, Train Accuracy: 0.6583, Validation Loss: 1.0309, Validation Accuracy: 0.6760\nEpoch 22, Loss: 0.7895, Train Accuracy: 0.6333, Validation Loss: 1.0124, Validation Accuracy: 0.6740\nEpoch 23, Loss: 0.7153, Train Accuracy: 0.6667, Validation Loss: 0.9988, Validation Accuracy: 0.6820\nEpoch 24, Loss: 0.7850, Train Accuracy: 0.6750, Validation Loss: 0.9895, Validation Accuracy: 0.6860\nEpoch 25, Loss: 0.7298, Train Accuracy: 0.6833, Validation Loss: 0.9839, Validation Accuracy: 0.6860\nEpoch 26, Loss: 0.7105, Train Accuracy: 0.6333, Validation Loss: 0.9825, Validation Accuracy: 0.6860\nEpoch 27, Loss: 0.6882, Train Accuracy: 0.6583, Validation Loss: 0.9830, Validation Accuracy: 0.6880\nEpoch 28, Loss: 0.7941, Train Accuracy: 0.6250, Validation Loss: 0.9844, Validation Accuracy: 0.6880\nEpoch 29, Loss: 0.6417, Train Accuracy: 0.6917, Validation Loss: 0.9890, Validation Accuracy: 0.6880\nEpoch 30, Loss: 0.6804, Train Accuracy: 0.6750, Validation Loss: 0.9924, Validation Accuracy: 0.6880\nEpoch 31, Loss: 0.6828, Train Accuracy: 0.6167, Validation Loss: 0.9968, Validation Accuracy: 0.6860\nEpoch 32, Loss: 0.7155, Train Accuracy: 0.6833, Validation Loss: 1.0009, Validation Accuracy: 0.6800\nEpoch 33, Loss: 0.6856, Train Accuracy: 0.6417, Validation Loss: 1.0044, Validation Accuracy: 0.6800\nEpoch 34, Loss: 0.6708, Train Accuracy: 0.6417, Validation Loss: 1.0137, Validation Accuracy: 0.6680\nEpoch 35, Loss: 0.7209, Train Accuracy: 0.6500, Validation Loss: 1.0226, Validation Accuracy: 0.6640\nEpoch 36, Loss: 0.6978, Train Accuracy: 0.6667, Validation Loss: 1.0157, Validation Accuracy: 0.6660\nEpoch 37, Loss: 0.6517, Train Accuracy: 0.6500, Validation Loss: 1.0037, Validation Accuracy: 0.6740\nEpoch 38, Loss: 0.6414, Train Accuracy: 0.6750, Validation Loss: 0.9969, Validation Accuracy: 0.6740\nEpoch 39, Loss: 0.6291, Train Accuracy: 0.6500, Validation Loss: 0.9886, Validation Accuracy: 0.6760\nEpoch 40, Loss: 0.7221, Train Accuracy: 0.5833, Validation Loss: 0.9785, Validation Accuracy: 0.6800\nEpoch 41, Loss: 0.6600, Train Accuracy: 0.6750, Validation Loss: 0.9690, Validation Accuracy: 0.6840\nEpoch 42, Loss: 0.6173, Train Accuracy: 0.7250, Validation Loss: 0.9606, Validation Accuracy: 0.6900\nEpoch 43, Loss: 0.6478, Train Accuracy: 0.7000, Validation Loss: 0.9579, Validation Accuracy: 0.6860\nEpoch 44, Loss: 0.6782, Train Accuracy: 0.6500, Validation Loss: 0.9538, Validation Accuracy: 0.6880\nEpoch 45, Loss: 0.6306, Train Accuracy: 0.6833, Validation Loss: 0.9510, Validation Accuracy: 0.6900\nEpoch 46, Loss: 0.7180, Train Accuracy: 0.6500, Validation Loss: 0.9528, Validation Accuracy: 0.6900\nEpoch 47, Loss: 0.7647, Train Accuracy: 0.6083, Validation Loss: 0.9547, Validation Accuracy: 0.6900\nEpoch 48, Loss: 0.6752, Train Accuracy: 0.6833, Validation Loss: 0.9568, Validation Accuracy: 0.6880\nEpoch 49, Loss: 0.6372, Train Accuracy: 0.6750, Validation Loss: 0.9593, Validation Accuracy: 0.6860\nEpoch 50, Loss: 0.6905, Train Accuracy: 0.6667, Validation Loss: 0.9584, Validation Accuracy: 0.6840\nEpoch 51, Loss: 0.6673, Train Accuracy: 0.6583, Validation Loss: 0.9601, Validation Accuracy: 0.6840\nEpoch 52, Loss: 0.6267, Train Accuracy: 0.6833, Validation Loss: 0.9607, Validation Accuracy: 0.6840\nEpoch 53, Loss: 0.5355, Train Accuracy: 0.7083, Validation Loss: 0.9614, Validation Accuracy: 0.6820\nEpoch 54, Loss: 0.7192, Train Accuracy: 0.6083, Validation Loss: 0.9659, Validation Accuracy: 0.6800\nEpoch 55, Loss: 0.6560, Train Accuracy: 0.6333, Validation Loss: 0.9702, Validation Accuracy: 0.6780\nEpoch 56, Loss: 0.5817, Train Accuracy: 0.7000, Validation Loss: 0.9736, Validation Accuracy: 0.6720\nEpoch 57, Loss: 0.7234, Train Accuracy: 0.6333, Validation Loss: 0.9691, Validation Accuracy: 0.6820\nEpoch 58, Loss: 0.6299, Train Accuracy: 0.6667, Validation Loss: 0.9611, Validation Accuracy: 0.6840\nEpoch 59, Loss: 0.6867, Train Accuracy: 0.6417, Validation Loss: 0.9532, Validation Accuracy: 0.6860\nEpoch 60, Loss: 0.7121, Train Accuracy: 0.6333, Validation Loss: 0.9488, Validation Accuracy: 0.6860\nEpoch 61, Loss: 0.6683, Train Accuracy: 0.6667, Validation Loss: 0.9476, Validation Accuracy: 0.6940\nEpoch 62, Loss: 0.6501, Train Accuracy: 0.6750, Validation Loss: 0.9495, Validation Accuracy: 0.6940\nEpoch 63, Loss: 0.5648, Train Accuracy: 0.7583, Validation Loss: 0.9509, Validation Accuracy: 0.6880\nEpoch 64, Loss: 0.6696, Train Accuracy: 0.6250, Validation Loss: 0.9530, Validation Accuracy: 0.6880\nEpoch 65, Loss: 0.6361, Train Accuracy: 0.6833, Validation Loss: 0.9549, Validation Accuracy: 0.6880\nEpoch 66, Loss: 0.6599, Train Accuracy: 0.6500, Validation Loss: 0.9562, Validation Accuracy: 0.6900\nEpoch 67, Loss: 0.6455, Train Accuracy: 0.6333, Validation Loss: 0.9600, Validation Accuracy: 0.6880\nEpoch 68, Loss: 0.6367, Train Accuracy: 0.6917, Validation Loss: 0.9629, Validation Accuracy: 0.6820\nEpoch 69, Loss: 0.7214, Train Accuracy: 0.5833, Validation Loss: 0.9661, Validation Accuracy: 0.6800\nEpoch 70, Loss: 0.6499, Train Accuracy: 0.6500, Validation Loss: 0.9732, Validation Accuracy: 0.6740\nEpoch 71, Loss: 0.6610, Train Accuracy: 0.7083, Validation Loss: 0.9793, Validation Accuracy: 0.6760\nEpoch 72, Loss: 0.6161, Train Accuracy: 0.6583, Validation Loss: 0.9827, Validation Accuracy: 0.6740\nEpoch 73, Loss: 0.7242, Train Accuracy: 0.6167, Validation Loss: 0.9847, Validation Accuracy: 0.6660\nEpoch 74, Loss: 0.6210, Train Accuracy: 0.6500, Validation Loss: 0.9828, Validation Accuracy: 0.6720\nEpoch 75, Loss: 0.5619, Train Accuracy: 0.6917, Validation Loss: 0.9790, Validation Accuracy: 0.6720\nEpoch 76, Loss: 0.6572, Train Accuracy: 0.6583, Validation Loss: 0.9762, Validation Accuracy: 0.6780\nEpoch 77, Loss: 0.6156, Train Accuracy: 0.6667, Validation Loss: 0.9725, Validation Accuracy: 0.6840\nEpoch 78, Loss: 0.6885, Train Accuracy: 0.6417, Validation Loss: 0.9636, Validation Accuracy: 0.6840\nEpoch 79, Loss: 0.6738, Train Accuracy: 0.6417, Validation Loss: 0.9562, Validation Accuracy: 0.6880\nEpoch 80, Loss: 0.5684, Train Accuracy: 0.6833, Validation Loss: 0.9485, Validation Accuracy: 0.6880\nEpoch 81, Loss: 0.7880, Train Accuracy: 0.6250, Validation Loss: 0.9456, Validation Accuracy: 0.6880\nEpoch 82, Loss: 0.5897, Train Accuracy: 0.7167, Validation Loss: 0.9465, Validation Accuracy: 0.6900\nEpoch 83, Loss: 0.5764, Train Accuracy: 0.6917, Validation Loss: 0.9501, Validation Accuracy: 0.6940\nEpoch 84, Loss: 0.7915, Train Accuracy: 0.5583, Validation Loss: 0.9565, Validation Accuracy: 0.6940\nEpoch 85, Loss: 0.6188, Train Accuracy: 0.7000, Validation Loss: 0.9618, Validation Accuracy: 0.6960\nEpoch 86, Loss: 0.5611, Train Accuracy: 0.6833, Validation Loss: 0.9681, Validation Accuracy: 0.6920\nEpoch 87, Loss: 0.6219, Train Accuracy: 0.7083, Validation Loss: 0.9713, Validation Accuracy: 0.6880\nEpoch 88, Loss: 0.6213, Train Accuracy: 0.7417, Validation Loss: 0.9713, Validation Accuracy: 0.6860\nEpoch 89, Loss: 0.6694, Train Accuracy: 0.6750, Validation Loss: 0.9666, Validation Accuracy: 0.6880\nEpoch 90, Loss: 0.6374, Train Accuracy: 0.7000, Validation Loss: 0.9584, Validation Accuracy: 0.6880\nEpoch 91, Loss: 0.7348, Train Accuracy: 0.6083, Validation Loss: 0.9505, Validation Accuracy: 0.6880\nEpoch 92, Loss: 0.5647, Train Accuracy: 0.6833, Validation Loss: 0.9454, Validation Accuracy: 0.6980\nEpoch 93, Loss: 0.8093, Train Accuracy: 0.5250, Validation Loss: 0.9458, Validation Accuracy: 0.6940\nEpoch 94, Loss: 0.6739, Train Accuracy: 0.6417, Validation Loss: 0.9484, Validation Accuracy: 0.6860\nEpoch 95, Loss: 0.6470, Train Accuracy: 0.6917, Validation Loss: 0.9513, Validation Accuracy: 0.6840\nEpoch 96, Loss: 0.7187, Train Accuracy: 0.6250, Validation Loss: 0.9555, Validation Accuracy: 0.6880\nEpoch 97, Loss: 0.6463, Train Accuracy: 0.6750, Validation Loss: 0.9600, Validation Accuracy: 0.6920\nEpoch 98, Loss: 0.6413, Train Accuracy: 0.6333, Validation Loss: 0.9635, Validation Accuracy: 0.6880\nEpoch 99, Loss: 0.6600, Train Accuracy: 0.6583, Validation Loss: 0.9653, Validation Accuracy: 0.6940\nEpoch 100, Loss: 0.6999, Train Accuracy: 0.6000, Validation Loss: 0.9623, Validation Accuracy: 0.6940\nEpoch 101, Loss: 0.6634, Train Accuracy: 0.6667, Validation Loss: 0.9606, Validation Accuracy: 0.6940\nEpoch 102, Loss: 0.5583, Train Accuracy: 0.7083, Validation Loss: 0.9618, Validation Accuracy: 0.6920\nEpoch 103, Loss: 0.7250, Train Accuracy: 0.5833, Validation Loss: 0.9620, Validation Accuracy: 0.6920\nEpoch 104, Loss: 0.5646, Train Accuracy: 0.6667, Validation Loss: 0.9604, Validation Accuracy: 0.6920\nEpoch 105, Loss: 0.6907, Train Accuracy: 0.6417, Validation Loss: 0.9584, Validation Accuracy: 0.6920\nEpoch 106, Loss: 0.5813, Train Accuracy: 0.6667, Validation Loss: 0.9588, Validation Accuracy: 0.6940\nEpoch 107, Loss: 0.7515, Train Accuracy: 0.6250, Validation Loss: 0.9591, Validation Accuracy: 0.6940\nEpoch 108, Loss: 0.6430, Train Accuracy: 0.6500, Validation Loss: 0.9610, Validation Accuracy: 0.6900\nEpoch 109, Loss: 0.6794, Train Accuracy: 0.6167, Validation Loss: 0.9661, Validation Accuracy: 0.6900\nEpoch 110, Loss: 0.6225, Train Accuracy: 0.6583, Validation Loss: 0.9732, Validation Accuracy: 0.6860\nEpoch 111, Loss: 0.5833, Train Accuracy: 0.7250, Validation Loss: 0.9774, Validation Accuracy: 0.6860\nEpoch 112, Loss: 0.7034, Train Accuracy: 0.5917, Validation Loss: 0.9807, Validation Accuracy: 0.6820\nEpoch 113, Loss: 0.5868, Train Accuracy: 0.7167, Validation Loss: 0.9804, Validation Accuracy: 0.6780\nEpoch 114, Loss: 0.6136, Train Accuracy: 0.6917, Validation Loss: 0.9802, Validation Accuracy: 0.6800\nEpoch 115, Loss: 0.6663, Train Accuracy: 0.6250, Validation Loss: 0.9760, Validation Accuracy: 0.6840\nEpoch 116, Loss: 0.6214, Train Accuracy: 0.6750, Validation Loss: 0.9718, Validation Accuracy: 0.6840\nEpoch 117, Loss: 0.7083, Train Accuracy: 0.6333, Validation Loss: 0.9648, Validation Accuracy: 0.6880\nEpoch 118, Loss: 0.5868, Train Accuracy: 0.7167, Validation Loss: 0.9587, Validation Accuracy: 0.6880\nEpoch 119, Loss: 0.6271, Train Accuracy: 0.6500, Validation Loss: 0.9541, Validation Accuracy: 0.6880\nEpoch 120, Loss: 0.6203, Train Accuracy: 0.6250, Validation Loss: 0.9497, Validation Accuracy: 0.6880\nEpoch 121, Loss: 0.6800, Train Accuracy: 0.6333, Validation Loss: 0.9480, Validation Accuracy: 0.6880\nEpoch 122, Loss: 0.7210, Train Accuracy: 0.6250, Validation Loss: 0.9475, Validation Accuracy: 0.6880\nEpoch 123, Loss: 0.5305, Train Accuracy: 0.7250, Validation Loss: 0.9495, Validation Accuracy: 0.6880\nEpoch 124, Loss: 0.6608, Train Accuracy: 0.6500, Validation Loss: 0.9527, Validation Accuracy: 0.6920\nEpoch 125, Loss: 0.7703, Train Accuracy: 0.5833, Validation Loss: 0.9574, Validation Accuracy: 0.6900\nEpoch 126, Loss: 0.6324, Train Accuracy: 0.6500, Validation Loss: 0.9592, Validation Accuracy: 0.6860\nEpoch 127, Loss: 0.7130, Train Accuracy: 0.6417, Validation Loss: 0.9601, Validation Accuracy: 0.6820\nEpoch 128, Loss: 0.6135, Train Accuracy: 0.6667, Validation Loss: 0.9622, Validation Accuracy: 0.6800\nEpoch 129, Loss: 0.6065, Train Accuracy: 0.6833, Validation Loss: 0.9610, Validation Accuracy: 0.6840\nEpoch 130, Loss: 0.6833, Train Accuracy: 0.6583, Validation Loss: 0.9568, Validation Accuracy: 0.6840\nEpoch 131, Loss: 0.6854, Train Accuracy: 0.6417, Validation Loss: 0.9513, Validation Accuracy: 0.6860\nEpoch 132, Loss: 0.5909, Train Accuracy: 0.7417, Validation Loss: 0.9472, Validation Accuracy: 0.6900\nEpoch 133, Loss: 0.5629, Train Accuracy: 0.6917, Validation Loss: 0.9452, Validation Accuracy: 0.6960\nEpoch 134, Loss: 0.5083, Train Accuracy: 0.7167, Validation Loss: 0.9515, Validation Accuracy: 0.6940\nEpoch 135, Loss: 0.6918, Train Accuracy: 0.6417, Validation Loss: 0.9606, Validation Accuracy: 0.6900\nEpoch 136, Loss: 0.5925, Train Accuracy: 0.6583, Validation Loss: 0.9728, Validation Accuracy: 0.6900\nEpoch 137, Loss: 0.6613, Train Accuracy: 0.6583, Validation Loss: 0.9828, Validation Accuracy: 0.6860\nEpoch 138, Loss: 0.6104, Train Accuracy: 0.6833, Validation Loss: 0.9959, Validation Accuracy: 0.6860\nEpoch 139, Loss: 0.6671, Train Accuracy: 0.6500, Validation Loss: 1.0032, Validation Accuracy: 0.6880\nEpoch 140, Loss: 0.7503, Train Accuracy: 0.5333, Validation Loss: 1.0077, Validation Accuracy: 0.6820\nEpoch 141, Loss: 0.7083, Train Accuracy: 0.6500, Validation Loss: 1.0007, Validation Accuracy: 0.6800\nEpoch 142, Loss: 0.6465, Train Accuracy: 0.6167, Validation Loss: 0.9876, Validation Accuracy: 0.6760\nEpoch 143, Loss: 0.5626, Train Accuracy: 0.7167, Validation Loss: 0.9788, Validation Accuracy: 0.6780\nEpoch 144, Loss: 0.6069, Train Accuracy: 0.6750, Validation Loss: 0.9672, Validation Accuracy: 0.6900\nEpoch 145, Loss: 0.6548, Train Accuracy: 0.6500, Validation Loss: 0.9584, Validation Accuracy: 0.6900\nEpoch 146, Loss: 0.7284, Train Accuracy: 0.5500, Validation Loss: 0.9519, Validation Accuracy: 0.6900\nEpoch 147, Loss: 0.6393, Train Accuracy: 0.6667, Validation Loss: 0.9478, Validation Accuracy: 0.6900\nEpoch 148, Loss: 0.6437, Train Accuracy: 0.6667, Validation Loss: 0.9482, Validation Accuracy: 0.6940\nEpoch 149, Loss: 0.7172, Train Accuracy: 0.5833, Validation Loss: 0.9511, Validation Accuracy: 0.6860\nEpoch 150, Loss: 0.6692, Train Accuracy: 0.6250, Validation Loss: 0.9571, Validation Accuracy: 0.6820\nEpoch 151, Loss: 0.5944, Train Accuracy: 0.7333, Validation Loss: 0.9641, Validation Accuracy: 0.6840\nEpoch 152, Loss: 0.5431, Train Accuracy: 0.7000, Validation Loss: 0.9722, Validation Accuracy: 0.6820\nEpoch 153, Loss: 0.5552, Train Accuracy: 0.7167, Validation Loss: 0.9778, Validation Accuracy: 0.6880\nEpoch 154, Loss: 0.6212, Train Accuracy: 0.6750, Validation Loss: 0.9782, Validation Accuracy: 0.6880\nEpoch 155, Loss: 0.6705, Train Accuracy: 0.6000, Validation Loss: 0.9794, Validation Accuracy: 0.6880\nEpoch 156, Loss: 0.7159, Train Accuracy: 0.6417, Validation Loss: 0.9776, Validation Accuracy: 0.6840\nEpoch 157, Loss: 0.5386, Train Accuracy: 0.7500, Validation Loss: 0.9792, Validation Accuracy: 0.6800\nEpoch 158, Loss: 0.6839, Train Accuracy: 0.6500, Validation Loss: 0.9775, Validation Accuracy: 0.6820\nEpoch 159, Loss: 0.5679, Train Accuracy: 0.7417, Validation Loss: 0.9733, Validation Accuracy: 0.6800\nEpoch 160, Loss: 0.6725, Train Accuracy: 0.5917, Validation Loss: 0.9678, Validation Accuracy: 0.6800\nEpoch 161, Loss: 0.6386, Train Accuracy: 0.6667, Validation Loss: 0.9618, Validation Accuracy: 0.6820\nEpoch 162, Loss: 0.6379, Train Accuracy: 0.6667, Validation Loss: 0.9595, Validation Accuracy: 0.6800\nEpoch 163, Loss: 0.7283, Train Accuracy: 0.6000, Validation Loss: 0.9605, Validation Accuracy: 0.6840\nEpoch 164, Loss: 0.6709, Train Accuracy: 0.6417, Validation Loss: 0.9624, Validation Accuracy: 0.6840\nEpoch 165, Loss: 0.7372, Train Accuracy: 0.6500, Validation Loss: 0.9653, Validation Accuracy: 0.6900\nEpoch 166, Loss: 0.5842, Train Accuracy: 0.6917, Validation Loss: 0.9714, Validation Accuracy: 0.6860\nEpoch 167, Loss: 0.6335, Train Accuracy: 0.6750, Validation Loss: 0.9790, Validation Accuracy: 0.6880\nEpoch 168, Loss: 0.6914, Train Accuracy: 0.6000, Validation Loss: 0.9867, Validation Accuracy: 0.6860\nEpoch 169, Loss: 0.6637, Train Accuracy: 0.6250, Validation Loss: 0.9944, Validation Accuracy: 0.6820\nEpoch 170, Loss: 0.6246, Train Accuracy: 0.6833, Validation Loss: 0.9965, Validation Accuracy: 0.6940\nEpoch 171, Loss: 0.6944, Train Accuracy: 0.6250, Validation Loss: 0.9964, Validation Accuracy: 0.6980\nEpoch 172, Loss: 0.5423, Train Accuracy: 0.6833, Validation Loss: 0.9946, Validation Accuracy: 0.6880\nEpoch 173, Loss: 0.5856, Train Accuracy: 0.6917, Validation Loss: 0.9905, Validation Accuracy: 0.6880\nEpoch 174, Loss: 0.6056, Train Accuracy: 0.7083, Validation Loss: 0.9841, Validation Accuracy: 0.6820\nEpoch 175, Loss: 0.6809, Train Accuracy: 0.6583, Validation Loss: 0.9797, Validation Accuracy: 0.6820\nEpoch 176, Loss: 0.6278, Train Accuracy: 0.6333, Validation Loss: 0.9757, Validation Accuracy: 0.6800\nEpoch 177, Loss: 0.7697, Train Accuracy: 0.5583, Validation Loss: 0.9791, Validation Accuracy: 0.6760\nEpoch 178, Loss: 0.6667, Train Accuracy: 0.6417, Validation Loss: 0.9832, Validation Accuracy: 0.6780\nEpoch 179, Loss: 0.6727, Train Accuracy: 0.6083, Validation Loss: 0.9840, Validation Accuracy: 0.6760\nEpoch 180, Loss: 0.5558, Train Accuracy: 0.7083, Validation Loss: 0.9793, Validation Accuracy: 0.6800\nEpoch 181, Loss: 0.6996, Train Accuracy: 0.6083, Validation Loss: 0.9722, Validation Accuracy: 0.6840\nEpoch 182, Loss: 0.7099, Train Accuracy: 0.6167, Validation Loss: 0.9632, Validation Accuracy: 0.6900\nEpoch 183, Loss: 0.5354, Train Accuracy: 0.7917, Validation Loss: 0.9559, Validation Accuracy: 0.6880\nEpoch 184, Loss: 0.5816, Train Accuracy: 0.7000, Validation Loss: 0.9521, Validation Accuracy: 0.6880\nEpoch 185, Loss: 0.5334, Train Accuracy: 0.7083, Validation Loss: 0.9521, Validation Accuracy: 0.6920\nEpoch 186, Loss: 0.7643, Train Accuracy: 0.5833, Validation Loss: 0.9523, Validation Accuracy: 0.6920\nEpoch 187, Loss: 0.7891, Train Accuracy: 0.5833, Validation Loss: 0.9573, Validation Accuracy: 0.6960\nEpoch 188, Loss: 0.5946, Train Accuracy: 0.6667, Validation Loss: 0.9591, Validation Accuracy: 0.7000\nEpoch 189, Loss: 0.6000, Train Accuracy: 0.6500, Validation Loss: 0.9610, Validation Accuracy: 0.6960\nEpoch 190, Loss: 0.5105, Train Accuracy: 0.7000, Validation Loss: 0.9635, Validation Accuracy: 0.6920\nEpoch 191, Loss: 0.6640, Train Accuracy: 0.6417, Validation Loss: 0.9670, Validation Accuracy: 0.6940\nEpoch 192, Loss: 0.6603, Train Accuracy: 0.6833, Validation Loss: 0.9744, Validation Accuracy: 0.6880\nEpoch 193, Loss: 0.5641, Train Accuracy: 0.7083, Validation Loss: 0.9792, Validation Accuracy: 0.6820\nEpoch 194, Loss: 0.6136, Train Accuracy: 0.6500, Validation Loss: 0.9825, Validation Accuracy: 0.6760\nEpoch 195, Loss: 0.5960, Train Accuracy: 0.6750, Validation Loss: 0.9875, Validation Accuracy: 0.6760\nEpoch 196, Loss: 0.5849, Train Accuracy: 0.6750, Validation Loss: 0.9877, Validation Accuracy: 0.6720\nEpoch 197, Loss: 0.6338, Train Accuracy: 0.6417, Validation Loss: 0.9867, Validation Accuracy: 0.6680\nEpoch 198, Loss: 0.6929, Train Accuracy: 0.6167, Validation Loss: 0.9850, Validation Accuracy: 0.6720\nEpoch 199, Loss: 0.6447, Train Accuracy: 0.6333, Validation Loss: 0.9849, Validation Accuracy: 0.6700\nEpoch 200, Loss: 0.6658, Train Accuracy: 0.6250, Validation Loss: 0.9847, Validation Accuracy: 0.6740\nEpoch 201, Loss: 0.6124, Train Accuracy: 0.6417, Validation Loss: 0.9826, Validation Accuracy: 0.6820\nEpoch 202, Loss: 0.6013, Train Accuracy: 0.6750, Validation Loss: 0.9800, Validation Accuracy: 0.6840\nEpoch 203, Loss: 0.6679, Train Accuracy: 0.6000, Validation Loss: 0.9760, Validation Accuracy: 0.6940\nEpoch 204, Loss: 0.5917, Train Accuracy: 0.6583, Validation Loss: 0.9701, Validation Accuracy: 0.6880\nEpoch 205, Loss: 0.5743, Train Accuracy: 0.6917, Validation Loss: 0.9648, Validation Accuracy: 0.6900\nEpoch 206, Loss: 0.5909, Train Accuracy: 0.6833, Validation Loss: 0.9603, Validation Accuracy: 0.6880\nEpoch 207, Loss: 0.6596, Train Accuracy: 0.6917, Validation Loss: 0.9563, Validation Accuracy: 0.6940\nEpoch 208, Loss: 0.6048, Train Accuracy: 0.6917, Validation Loss: 0.9537, Validation Accuracy: 0.6980\nEpoch 209, Loss: 0.6349, Train Accuracy: 0.6333, Validation Loss: 0.9550, Validation Accuracy: 0.7080\nEpoch 210, Loss: 0.6425, Train Accuracy: 0.6750, Validation Loss: 0.9605, Validation Accuracy: 0.7020\nEpoch 211, Loss: 0.6618, Train Accuracy: 0.6583, Validation Loss: 0.9682, Validation Accuracy: 0.6960\nEpoch 212, Loss: 0.6638, Train Accuracy: 0.6250, Validation Loss: 0.9781, Validation Accuracy: 0.6960\nEpoch 213, Loss: 0.5826, Train Accuracy: 0.6500, Validation Loss: 0.9896, Validation Accuracy: 0.6920\nEpoch 214, Loss: 0.6307, Train Accuracy: 0.6417, Validation Loss: 0.9940, Validation Accuracy: 0.6920\nEpoch 215, Loss: 0.6521, Train Accuracy: 0.6417, Validation Loss: 0.9950, Validation Accuracy: 0.6840\nEpoch 216, Loss: 0.6911, Train Accuracy: 0.6167, Validation Loss: 0.9945, Validation Accuracy: 0.6820\nEpoch 217, Loss: 0.5954, Train Accuracy: 0.6583, Validation Loss: 0.9937, Validation Accuracy: 0.6840\nEpoch 218, Loss: 0.6236, Train Accuracy: 0.6417, Validation Loss: 0.9949, Validation Accuracy: 0.6820\nEpoch 219, Loss: 0.6083, Train Accuracy: 0.6833, Validation Loss: 0.9941, Validation Accuracy: 0.6800\nEpoch 220, Loss: 0.6643, Train Accuracy: 0.6167, Validation Loss: 0.9878, Validation Accuracy: 0.6900\nEpoch 221, Loss: 0.7594, Train Accuracy: 0.6083, Validation Loss: 0.9784, Validation Accuracy: 0.6900\nEpoch 222, Loss: 0.7046, Train Accuracy: 0.6000, Validation Loss: 0.9713, Validation Accuracy: 0.6940\nEpoch 223, Loss: 0.6619, Train Accuracy: 0.6250, Validation Loss: 0.9680, Validation Accuracy: 0.7000\nEpoch 224, Loss: 0.6197, Train Accuracy: 0.6417, Validation Loss: 0.9661, Validation Accuracy: 0.7040\nEpoch 225, Loss: 0.6367, Train Accuracy: 0.6417, Validation Loss: 0.9647, Validation Accuracy: 0.7000\nEpoch 226, Loss: 0.5830, Train Accuracy: 0.7000, Validation Loss: 0.9675, Validation Accuracy: 0.7000\nEpoch 227, Loss: 0.5159, Train Accuracy: 0.7250, Validation Loss: 0.9682, Validation Accuracy: 0.7040\nEpoch 228, Loss: 0.5527, Train Accuracy: 0.7000, Validation Loss: 0.9703, Validation Accuracy: 0.6960\nEpoch 229, Loss: 0.6122, Train Accuracy: 0.6583, Validation Loss: 0.9741, Validation Accuracy: 0.6920\nEpoch 230, Loss: 0.5409, Train Accuracy: 0.7167, Validation Loss: 0.9736, Validation Accuracy: 0.6860\nEpoch 231, Loss: 0.5780, Train Accuracy: 0.7000, Validation Loss: 0.9710, Validation Accuracy: 0.6960\nEpoch 232, Loss: 0.7183, Train Accuracy: 0.6417, Validation Loss: 0.9674, Validation Accuracy: 0.6860\nEpoch 233, Loss: 0.6452, Train Accuracy: 0.6083, Validation Loss: 0.9652, Validation Accuracy: 0.6880\nEarly stopping triggered.\nBest model loaded from best_model.pth\nTest Accuracy: 0.6870\n","output_type":"stream"}]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Assume all_datasets dictionary is already populated as in the previous steps\n    dataset_name = 'Pubmed'  # Change this to Citeseer or Pubmed as needed\n    data = load_processed_data_from_dict(all_datasets[dataset_name])\n\n    # Model configuration based on the paper's setup\n    num_features = data.x.size(1)\n    num_classes = int(data.y.max()) + 1\n    hidden_dim = 16  # Set the hidden dimension to 16 as per the paper\n    num_views = 3\n\n    # Initialize the model with the specified architecture and dimensions\n    model = MAGCN(num_features, num_classes, num_views, hidden_dim)\n    \n    # Use Adam optimizer with the specified learning rate and weight decay\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n\n    # Train and test the model\n    train_model(model, data, optimizer, epochs=2000, early_stopping_patience=100)\n    test_model(model, data)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T16:59:01.976101Z","iopub.execute_input":"2024-08-12T16:59:01.976938Z","iopub.status.idle":"2024-08-12T17:01:02.470376Z","shell.execute_reply.started":"2024-08-12T16:59:01.976905Z","shell.execute_reply":"2024-08-12T17:01:02.469233Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.0977, Train Accuracy: 0.3333, Validation Loss: 1.0944, Validation Accuracy: 0.5120\nEpoch 2, Loss: 1.0945, Train Accuracy: 0.4667, Validation Loss: 1.0900, Validation Accuracy: 0.5640\nEpoch 3, Loss: 1.0893, Train Accuracy: 0.3667, Validation Loss: 1.0851, Validation Accuracy: 0.6580\nEpoch 4, Loss: 1.0769, Train Accuracy: 0.5000, Validation Loss: 1.0780, Validation Accuracy: 0.6900\nEpoch 5, Loss: 1.0683, Train Accuracy: 0.4333, Validation Loss: 1.0713, Validation Accuracy: 0.6760\nEpoch 6, Loss: 1.0612, Train Accuracy: 0.5000, Validation Loss: 1.0634, Validation Accuracy: 0.6520\nEpoch 7, Loss: 1.0559, Train Accuracy: 0.5500, Validation Loss: 1.0563, Validation Accuracy: 0.6640\nEpoch 8, Loss: 1.0441, Train Accuracy: 0.4667, Validation Loss: 1.0502, Validation Accuracy: 0.6740\nEpoch 9, Loss: 1.0269, Train Accuracy: 0.6167, Validation Loss: 1.0450, Validation Accuracy: 0.6780\nEpoch 10, Loss: 1.0200, Train Accuracy: 0.6500, Validation Loss: 1.0404, Validation Accuracy: 0.6580\nEpoch 11, Loss: 1.0247, Train Accuracy: 0.6000, Validation Loss: 1.0355, Validation Accuracy: 0.6520\nEpoch 12, Loss: 0.9957, Train Accuracy: 0.6333, Validation Loss: 1.0295, Validation Accuracy: 0.6460\nEpoch 13, Loss: 0.9886, Train Accuracy: 0.6167, Validation Loss: 1.0218, Validation Accuracy: 0.6480\nEpoch 14, Loss: 0.9589, Train Accuracy: 0.6333, Validation Loss: 1.0147, Validation Accuracy: 0.6500\nEpoch 15, Loss: 0.9840, Train Accuracy: 0.6000, Validation Loss: 1.0070, Validation Accuracy: 0.6520\nEpoch 16, Loss: 0.9484, Train Accuracy: 0.7167, Validation Loss: 0.9979, Validation Accuracy: 0.6660\nEpoch 17, Loss: 0.9630, Train Accuracy: 0.6333, Validation Loss: 0.9898, Validation Accuracy: 0.6680\nEpoch 18, Loss: 0.9390, Train Accuracy: 0.7333, Validation Loss: 0.9792, Validation Accuracy: 0.6840\nEpoch 19, Loss: 0.9199, Train Accuracy: 0.6333, Validation Loss: 0.9683, Validation Accuracy: 0.6900\nEpoch 20, Loss: 0.9104, Train Accuracy: 0.7333, Validation Loss: 0.9531, Validation Accuracy: 0.7080\nEpoch 21, Loss: 0.9008, Train Accuracy: 0.6500, Validation Loss: 0.9394, Validation Accuracy: 0.7160\nEpoch 22, Loss: 0.8629, Train Accuracy: 0.7167, Validation Loss: 0.9253, Validation Accuracy: 0.7180\nEpoch 23, Loss: 0.8428, Train Accuracy: 0.7167, Validation Loss: 0.9119, Validation Accuracy: 0.7240\nEpoch 24, Loss: 0.8788, Train Accuracy: 0.7000, Validation Loss: 0.8988, Validation Accuracy: 0.7260\nEpoch 25, Loss: 0.8503, Train Accuracy: 0.6833, Validation Loss: 0.8869, Validation Accuracy: 0.7280\nEpoch 26, Loss: 0.8416, Train Accuracy: 0.6500, Validation Loss: 0.8760, Validation Accuracy: 0.7320\nEpoch 27, Loss: 0.8478, Train Accuracy: 0.6833, Validation Loss: 0.8652, Validation Accuracy: 0.7560\nEpoch 28, Loss: 0.8196, Train Accuracy: 0.7000, Validation Loss: 0.8564, Validation Accuracy: 0.7580\nEpoch 29, Loss: 0.7503, Train Accuracy: 0.7833, Validation Loss: 0.8469, Validation Accuracy: 0.7580\nEpoch 30, Loss: 0.7470, Train Accuracy: 0.8167, Validation Loss: 0.8318, Validation Accuracy: 0.7640\nEpoch 31, Loss: 0.7898, Train Accuracy: 0.5833, Validation Loss: 0.8191, Validation Accuracy: 0.7620\nEpoch 32, Loss: 0.7546, Train Accuracy: 0.7333, Validation Loss: 0.8072, Validation Accuracy: 0.7640\nEpoch 33, Loss: 0.7229, Train Accuracy: 0.7500, Validation Loss: 0.7971, Validation Accuracy: 0.7660\nEpoch 34, Loss: 0.7332, Train Accuracy: 0.7167, Validation Loss: 0.7878, Validation Accuracy: 0.7700\nEpoch 35, Loss: 0.6647, Train Accuracy: 0.7833, Validation Loss: 0.7773, Validation Accuracy: 0.7660\nEpoch 36, Loss: 0.6893, Train Accuracy: 0.7500, Validation Loss: 0.7665, Validation Accuracy: 0.7520\nEpoch 37, Loss: 0.7201, Train Accuracy: 0.6833, Validation Loss: 0.7579, Validation Accuracy: 0.7520\nEpoch 38, Loss: 0.6838, Train Accuracy: 0.7833, Validation Loss: 0.7491, Validation Accuracy: 0.7440\nEpoch 39, Loss: 0.6477, Train Accuracy: 0.7833, Validation Loss: 0.7407, Validation Accuracy: 0.7500\nEpoch 40, Loss: 0.6431, Train Accuracy: 0.7167, Validation Loss: 0.7371, Validation Accuracy: 0.7480\nEpoch 41, Loss: 0.6206, Train Accuracy: 0.7333, Validation Loss: 0.7363, Validation Accuracy: 0.7460\nEpoch 42, Loss: 0.6713, Train Accuracy: 0.8333, Validation Loss: 0.7317, Validation Accuracy: 0.7440\nEpoch 43, Loss: 0.6269, Train Accuracy: 0.7333, Validation Loss: 0.7272, Validation Accuracy: 0.7380\nEpoch 44, Loss: 0.6777, Train Accuracy: 0.7333, Validation Loss: 0.7233, Validation Accuracy: 0.7420\nEpoch 45, Loss: 0.6089, Train Accuracy: 0.7500, Validation Loss: 0.7201, Validation Accuracy: 0.7560\nEpoch 46, Loss: 0.6686, Train Accuracy: 0.7333, Validation Loss: 0.7211, Validation Accuracy: 0.7580\nEpoch 47, Loss: 0.5991, Train Accuracy: 0.7667, Validation Loss: 0.7177, Validation Accuracy: 0.7660\nEpoch 48, Loss: 0.6568, Train Accuracy: 0.7167, Validation Loss: 0.7094, Validation Accuracy: 0.7720\nEpoch 49, Loss: 0.6107, Train Accuracy: 0.7167, Validation Loss: 0.6974, Validation Accuracy: 0.7720\nEpoch 50, Loss: 0.6436, Train Accuracy: 0.6833, Validation Loss: 0.6860, Validation Accuracy: 0.7800\nEpoch 51, Loss: 0.5959, Train Accuracy: 0.7333, Validation Loss: 0.6749, Validation Accuracy: 0.7700\nEpoch 52, Loss: 0.6229, Train Accuracy: 0.6333, Validation Loss: 0.6652, Validation Accuracy: 0.7780\nEpoch 53, Loss: 0.5304, Train Accuracy: 0.7833, Validation Loss: 0.6548, Validation Accuracy: 0.7760\nEpoch 54, Loss: 0.5982, Train Accuracy: 0.7000, Validation Loss: 0.6474, Validation Accuracy: 0.7860\nEpoch 55, Loss: 0.6021, Train Accuracy: 0.7500, Validation Loss: 0.6418, Validation Accuracy: 0.7920\nEpoch 56, Loss: 0.6234, Train Accuracy: 0.7333, Validation Loss: 0.6373, Validation Accuracy: 0.7900\nEpoch 57, Loss: 0.6231, Train Accuracy: 0.7333, Validation Loss: 0.6338, Validation Accuracy: 0.7940\nEpoch 58, Loss: 0.5585, Train Accuracy: 0.7167, Validation Loss: 0.6323, Validation Accuracy: 0.7900\nEpoch 59, Loss: 0.5787, Train Accuracy: 0.6833, Validation Loss: 0.6315, Validation Accuracy: 0.7820\nEpoch 60, Loss: 0.5121, Train Accuracy: 0.8000, Validation Loss: 0.6322, Validation Accuracy: 0.7780\nEpoch 61, Loss: 0.4660, Train Accuracy: 0.8667, Validation Loss: 0.6304, Validation Accuracy: 0.7780\nEpoch 62, Loss: 0.5306, Train Accuracy: 0.7667, Validation Loss: 0.6278, Validation Accuracy: 0.7800\nEpoch 63, Loss: 0.5256, Train Accuracy: 0.8167, Validation Loss: 0.6231, Validation Accuracy: 0.7820\nEpoch 64, Loss: 0.5439, Train Accuracy: 0.7667, Validation Loss: 0.6186, Validation Accuracy: 0.7900\nEpoch 65, Loss: 0.5014, Train Accuracy: 0.8167, Validation Loss: 0.6152, Validation Accuracy: 0.7880\nEpoch 66, Loss: 0.5719, Train Accuracy: 0.6500, Validation Loss: 0.6131, Validation Accuracy: 0.7900\nEpoch 67, Loss: 0.4647, Train Accuracy: 0.7667, Validation Loss: 0.6124, Validation Accuracy: 0.7880\nEpoch 68, Loss: 0.4696, Train Accuracy: 0.7667, Validation Loss: 0.6130, Validation Accuracy: 0.7940\nEpoch 69, Loss: 0.5406, Train Accuracy: 0.7333, Validation Loss: 0.6132, Validation Accuracy: 0.7920\nEpoch 70, Loss: 0.5013, Train Accuracy: 0.7833, Validation Loss: 0.6139, Validation Accuracy: 0.7920\nEpoch 71, Loss: 0.4685, Train Accuracy: 0.8000, Validation Loss: 0.6142, Validation Accuracy: 0.7900\nEpoch 72, Loss: 0.4695, Train Accuracy: 0.7833, Validation Loss: 0.6146, Validation Accuracy: 0.7920\nEpoch 73, Loss: 0.5482, Train Accuracy: 0.8000, Validation Loss: 0.6165, Validation Accuracy: 0.7880\nEpoch 74, Loss: 0.4561, Train Accuracy: 0.8167, Validation Loss: 0.6190, Validation Accuracy: 0.7820\nEpoch 75, Loss: 0.4089, Train Accuracy: 0.8167, Validation Loss: 0.6183, Validation Accuracy: 0.7860\nEpoch 76, Loss: 0.4967, Train Accuracy: 0.7167, Validation Loss: 0.6177, Validation Accuracy: 0.7880\nEpoch 77, Loss: 0.4417, Train Accuracy: 0.7500, Validation Loss: 0.6174, Validation Accuracy: 0.7820\nEpoch 78, Loss: 0.4394, Train Accuracy: 0.7833, Validation Loss: 0.6166, Validation Accuracy: 0.7840\nEpoch 79, Loss: 0.4865, Train Accuracy: 0.8000, Validation Loss: 0.6138, Validation Accuracy: 0.7860\nEpoch 80, Loss: 0.4428, Train Accuracy: 0.8000, Validation Loss: 0.6107, Validation Accuracy: 0.7840\nEpoch 81, Loss: 0.4213, Train Accuracy: 0.8167, Validation Loss: 0.6102, Validation Accuracy: 0.7800\nEpoch 82, Loss: 0.4175, Train Accuracy: 0.7833, Validation Loss: 0.6095, Validation Accuracy: 0.7820\nEpoch 83, Loss: 0.4578, Train Accuracy: 0.8167, Validation Loss: 0.6075, Validation Accuracy: 0.7840\nEpoch 84, Loss: 0.4225, Train Accuracy: 0.7667, Validation Loss: 0.6050, Validation Accuracy: 0.7840\nEpoch 85, Loss: 0.4134, Train Accuracy: 0.8167, Validation Loss: 0.6039, Validation Accuracy: 0.7720\nEpoch 86, Loss: 0.4621, Train Accuracy: 0.7667, Validation Loss: 0.6035, Validation Accuracy: 0.7740\nEpoch 87, Loss: 0.3786, Train Accuracy: 0.8667, Validation Loss: 0.6011, Validation Accuracy: 0.7820\nEpoch 88, Loss: 0.4811, Train Accuracy: 0.7667, Validation Loss: 0.5957, Validation Accuracy: 0.7880\nEpoch 89, Loss: 0.5394, Train Accuracy: 0.7000, Validation Loss: 0.5919, Validation Accuracy: 0.7840\nEpoch 90, Loss: 0.4623, Train Accuracy: 0.8000, Validation Loss: 0.5917, Validation Accuracy: 0.7840\nEpoch 91, Loss: 0.4738, Train Accuracy: 0.7500, Validation Loss: 0.5963, Validation Accuracy: 0.7860\nEpoch 92, Loss: 0.3996, Train Accuracy: 0.8167, Validation Loss: 0.6018, Validation Accuracy: 0.7920\nEpoch 93, Loss: 0.4395, Train Accuracy: 0.8333, Validation Loss: 0.6004, Validation Accuracy: 0.7920\nEpoch 94, Loss: 0.3325, Train Accuracy: 0.8333, Validation Loss: 0.5988, Validation Accuracy: 0.7860\nEpoch 95, Loss: 0.3917, Train Accuracy: 0.8500, Validation Loss: 0.5951, Validation Accuracy: 0.7780\nEpoch 96, Loss: 0.3999, Train Accuracy: 0.7667, Validation Loss: 0.5927, Validation Accuracy: 0.7880\nEpoch 97, Loss: 0.3743, Train Accuracy: 0.8000, Validation Loss: 0.5936, Validation Accuracy: 0.7880\nEpoch 98, Loss: 0.3999, Train Accuracy: 0.8500, Validation Loss: 0.5994, Validation Accuracy: 0.7780\nEpoch 99, Loss: 0.4323, Train Accuracy: 0.8000, Validation Loss: 0.6055, Validation Accuracy: 0.7820\nEpoch 100, Loss: 0.4228, Train Accuracy: 0.8000, Validation Loss: 0.6082, Validation Accuracy: 0.7840\nEpoch 101, Loss: 0.4478, Train Accuracy: 0.7667, Validation Loss: 0.6033, Validation Accuracy: 0.7820\nEpoch 102, Loss: 0.3944, Train Accuracy: 0.7500, Validation Loss: 0.6002, Validation Accuracy: 0.7760\nEpoch 103, Loss: 0.3944, Train Accuracy: 0.8000, Validation Loss: 0.5972, Validation Accuracy: 0.7840\nEpoch 104, Loss: 0.4778, Train Accuracy: 0.7667, Validation Loss: 0.5946, Validation Accuracy: 0.7840\nEpoch 105, Loss: 0.3359, Train Accuracy: 0.8667, Validation Loss: 0.5912, Validation Accuracy: 0.7900\nEpoch 106, Loss: 0.4552, Train Accuracy: 0.7167, Validation Loss: 0.5894, Validation Accuracy: 0.7880\nEpoch 107, Loss: 0.3653, Train Accuracy: 0.8500, Validation Loss: 0.5873, Validation Accuracy: 0.7860\nEpoch 108, Loss: 0.4442, Train Accuracy: 0.7500, Validation Loss: 0.5831, Validation Accuracy: 0.7880\nEpoch 109, Loss: 0.4231, Train Accuracy: 0.8000, Validation Loss: 0.5787, Validation Accuracy: 0.7960\nEpoch 110, Loss: 0.4195, Train Accuracy: 0.8167, Validation Loss: 0.5774, Validation Accuracy: 0.7960\nEpoch 111, Loss: 0.3921, Train Accuracy: 0.8333, Validation Loss: 0.5803, Validation Accuracy: 0.7840\nEpoch 112, Loss: 0.4688, Train Accuracy: 0.6833, Validation Loss: 0.5876, Validation Accuracy: 0.7800\nEpoch 113, Loss: 0.3992, Train Accuracy: 0.8167, Validation Loss: 0.5931, Validation Accuracy: 0.7820\nEpoch 114, Loss: 0.3401, Train Accuracy: 0.8500, Validation Loss: 0.5986, Validation Accuracy: 0.7800\nEpoch 115, Loss: 0.3621, Train Accuracy: 0.8000, Validation Loss: 0.6011, Validation Accuracy: 0.7760\nEpoch 116, Loss: 0.3676, Train Accuracy: 0.8000, Validation Loss: 0.5989, Validation Accuracy: 0.7800\nEpoch 117, Loss: 0.3739, Train Accuracy: 0.8000, Validation Loss: 0.5989, Validation Accuracy: 0.7800\nEpoch 118, Loss: 0.4798, Train Accuracy: 0.7500, Validation Loss: 0.5943, Validation Accuracy: 0.7880\nEpoch 119, Loss: 0.4355, Train Accuracy: 0.7000, Validation Loss: 0.5899, Validation Accuracy: 0.7860\nEpoch 120, Loss: 0.3985, Train Accuracy: 0.8500, Validation Loss: 0.5862, Validation Accuracy: 0.7880\nEpoch 121, Loss: 0.4519, Train Accuracy: 0.7833, Validation Loss: 0.5831, Validation Accuracy: 0.7860\nEpoch 122, Loss: 0.4028, Train Accuracy: 0.8167, Validation Loss: 0.5779, Validation Accuracy: 0.7880\nEpoch 123, Loss: 0.4003, Train Accuracy: 0.8167, Validation Loss: 0.5738, Validation Accuracy: 0.7900\nEpoch 124, Loss: 0.4087, Train Accuracy: 0.7333, Validation Loss: 0.5711, Validation Accuracy: 0.7840\nEpoch 125, Loss: 0.4422, Train Accuracy: 0.8000, Validation Loss: 0.5716, Validation Accuracy: 0.7860\nEpoch 126, Loss: 0.3790, Train Accuracy: 0.8667, Validation Loss: 0.5774, Validation Accuracy: 0.7820\nEpoch 127, Loss: 0.4429, Train Accuracy: 0.7333, Validation Loss: 0.5833, Validation Accuracy: 0.7700\nEpoch 128, Loss: 0.3476, Train Accuracy: 0.8500, Validation Loss: 0.5843, Validation Accuracy: 0.7720\nEpoch 129, Loss: 0.4562, Train Accuracy: 0.7667, Validation Loss: 0.5849, Validation Accuracy: 0.7780\nEpoch 130, Loss: 0.4503, Train Accuracy: 0.7500, Validation Loss: 0.5903, Validation Accuracy: 0.7800\nEpoch 131, Loss: 0.3513, Train Accuracy: 0.8500, Validation Loss: 0.5950, Validation Accuracy: 0.7860\nEpoch 132, Loss: 0.4944, Train Accuracy: 0.7500, Validation Loss: 0.5984, Validation Accuracy: 0.7800\nEpoch 133, Loss: 0.4502, Train Accuracy: 0.7333, Validation Loss: 0.5971, Validation Accuracy: 0.7820\nEpoch 134, Loss: 0.3729, Train Accuracy: 0.8333, Validation Loss: 0.5958, Validation Accuracy: 0.7800\nEpoch 135, Loss: 0.4252, Train Accuracy: 0.8000, Validation Loss: 0.5933, Validation Accuracy: 0.7860\nEpoch 136, Loss: 0.4202, Train Accuracy: 0.7833, Validation Loss: 0.5878, Validation Accuracy: 0.7860\nEpoch 137, Loss: 0.3841, Train Accuracy: 0.6833, Validation Loss: 0.5850, Validation Accuracy: 0.7920\nEpoch 138, Loss: 0.4768, Train Accuracy: 0.7500, Validation Loss: 0.5832, Validation Accuracy: 0.7840\nEpoch 139, Loss: 0.4443, Train Accuracy: 0.7667, Validation Loss: 0.5806, Validation Accuracy: 0.7760\nEpoch 140, Loss: 0.4230, Train Accuracy: 0.7500, Validation Loss: 0.5795, Validation Accuracy: 0.7740\nEpoch 141, Loss: 0.3751, Train Accuracy: 0.8333, Validation Loss: 0.5808, Validation Accuracy: 0.7740\nEpoch 142, Loss: 0.4550, Train Accuracy: 0.7167, Validation Loss: 0.5802, Validation Accuracy: 0.7740\nEpoch 143, Loss: 0.4004, Train Accuracy: 0.7833, Validation Loss: 0.5810, Validation Accuracy: 0.7740\nEpoch 144, Loss: 0.3632, Train Accuracy: 0.8167, Validation Loss: 0.5796, Validation Accuracy: 0.7740\nEpoch 145, Loss: 0.3916, Train Accuracy: 0.8500, Validation Loss: 0.5753, Validation Accuracy: 0.7760\nEpoch 146, Loss: 0.4905, Train Accuracy: 0.7500, Validation Loss: 0.5723, Validation Accuracy: 0.7860\nEpoch 147, Loss: 0.3939, Train Accuracy: 0.7167, Validation Loss: 0.5704, Validation Accuracy: 0.7920\nEpoch 148, Loss: 0.3582, Train Accuracy: 0.7833, Validation Loss: 0.5697, Validation Accuracy: 0.7920\nEpoch 149, Loss: 0.4259, Train Accuracy: 0.7667, Validation Loss: 0.5684, Validation Accuracy: 0.7920\nEpoch 150, Loss: 0.4279, Train Accuracy: 0.8000, Validation Loss: 0.5672, Validation Accuracy: 0.7900\nEpoch 151, Loss: 0.3433, Train Accuracy: 0.8333, Validation Loss: 0.5672, Validation Accuracy: 0.7900\nEpoch 152, Loss: 0.2927, Train Accuracy: 0.9333, Validation Loss: 0.5668, Validation Accuracy: 0.7880\nEpoch 153, Loss: 0.4615, Train Accuracy: 0.7333, Validation Loss: 0.5697, Validation Accuracy: 0.7820\nEpoch 154, Loss: 0.3362, Train Accuracy: 0.8333, Validation Loss: 0.5720, Validation Accuracy: 0.7800\nEpoch 155, Loss: 0.3960, Train Accuracy: 0.7833, Validation Loss: 0.5746, Validation Accuracy: 0.7780\nEpoch 156, Loss: 0.4322, Train Accuracy: 0.7833, Validation Loss: 0.5795, Validation Accuracy: 0.7740\nEpoch 157, Loss: 0.3831, Train Accuracy: 0.8500, Validation Loss: 0.5780, Validation Accuracy: 0.7740\nEpoch 158, Loss: 0.4048, Train Accuracy: 0.8167, Validation Loss: 0.5750, Validation Accuracy: 0.7780\nEpoch 159, Loss: 0.3929, Train Accuracy: 0.8333, Validation Loss: 0.5734, Validation Accuracy: 0.7800\nEpoch 160, Loss: 0.4811, Train Accuracy: 0.7500, Validation Loss: 0.5712, Validation Accuracy: 0.7820\nEpoch 161, Loss: 0.2918, Train Accuracy: 0.8667, Validation Loss: 0.5690, Validation Accuracy: 0.7820\nEpoch 162, Loss: 0.3998, Train Accuracy: 0.7667, Validation Loss: 0.5692, Validation Accuracy: 0.7880\nEpoch 163, Loss: 0.3587, Train Accuracy: 0.7833, Validation Loss: 0.5708, Validation Accuracy: 0.7860\nEpoch 164, Loss: 0.2881, Train Accuracy: 0.8500, Validation Loss: 0.5728, Validation Accuracy: 0.7860\nEpoch 165, Loss: 0.4102, Train Accuracy: 0.8500, Validation Loss: 0.5731, Validation Accuracy: 0.7840\nEpoch 166, Loss: 0.3839, Train Accuracy: 0.8167, Validation Loss: 0.5731, Validation Accuracy: 0.7820\nEpoch 167, Loss: 0.3696, Train Accuracy: 0.8167, Validation Loss: 0.5728, Validation Accuracy: 0.7840\nEpoch 168, Loss: 0.4127, Train Accuracy: 0.7833, Validation Loss: 0.5720, Validation Accuracy: 0.7840\nEpoch 169, Loss: 0.3241, Train Accuracy: 0.7833, Validation Loss: 0.5710, Validation Accuracy: 0.7840\nEpoch 170, Loss: 0.3274, Train Accuracy: 0.8667, Validation Loss: 0.5702, Validation Accuracy: 0.7820\nEpoch 171, Loss: 0.3972, Train Accuracy: 0.7833, Validation Loss: 0.5704, Validation Accuracy: 0.7840\nEpoch 172, Loss: 0.4250, Train Accuracy: 0.7833, Validation Loss: 0.5714, Validation Accuracy: 0.7840\nEpoch 173, Loss: 0.4565, Train Accuracy: 0.7667, Validation Loss: 0.5726, Validation Accuracy: 0.7860\nEpoch 174, Loss: 0.3409, Train Accuracy: 0.8500, Validation Loss: 0.5745, Validation Accuracy: 0.7880\nEpoch 175, Loss: 0.3684, Train Accuracy: 0.8500, Validation Loss: 0.5777, Validation Accuracy: 0.7860\nEpoch 176, Loss: 0.4316, Train Accuracy: 0.7333, Validation Loss: 0.5826, Validation Accuracy: 0.7860\nEpoch 177, Loss: 0.3875, Train Accuracy: 0.7667, Validation Loss: 0.5891, Validation Accuracy: 0.7800\nEpoch 178, Loss: 0.4037, Train Accuracy: 0.7667, Validation Loss: 0.5953, Validation Accuracy: 0.7800\nEpoch 179, Loss: 0.4055, Train Accuracy: 0.8333, Validation Loss: 0.5942, Validation Accuracy: 0.7800\nEpoch 180, Loss: 0.3416, Train Accuracy: 0.9000, Validation Loss: 0.5850, Validation Accuracy: 0.7880\nEpoch 181, Loss: 0.3855, Train Accuracy: 0.7833, Validation Loss: 0.5774, Validation Accuracy: 0.7860\nEpoch 182, Loss: 0.4002, Train Accuracy: 0.7833, Validation Loss: 0.5725, Validation Accuracy: 0.7880\nEpoch 183, Loss: 0.3882, Train Accuracy: 0.8000, Validation Loss: 0.5672, Validation Accuracy: 0.7740\nEpoch 184, Loss: 0.4873, Train Accuracy: 0.7333, Validation Loss: 0.5649, Validation Accuracy: 0.7700\nEpoch 185, Loss: 0.3831, Train Accuracy: 0.8333, Validation Loss: 0.5647, Validation Accuracy: 0.7740\nEpoch 186, Loss: 0.2930, Train Accuracy: 0.8667, Validation Loss: 0.5658, Validation Accuracy: 0.7760\nEpoch 187, Loss: 0.5281, Train Accuracy: 0.6667, Validation Loss: 0.5670, Validation Accuracy: 0.7840\nEpoch 188, Loss: 0.3866, Train Accuracy: 0.8333, Validation Loss: 0.5662, Validation Accuracy: 0.7860\nEpoch 189, Loss: 0.3853, Train Accuracy: 0.8500, Validation Loss: 0.5670, Validation Accuracy: 0.7860\nEpoch 190, Loss: 0.3787, Train Accuracy: 0.7667, Validation Loss: 0.5715, Validation Accuracy: 0.7900\nEpoch 191, Loss: 0.4486, Train Accuracy: 0.7833, Validation Loss: 0.5747, Validation Accuracy: 0.7900\nEpoch 192, Loss: 0.4257, Train Accuracy: 0.7833, Validation Loss: 0.5743, Validation Accuracy: 0.7900\nEpoch 193, Loss: 0.4368, Train Accuracy: 0.7333, Validation Loss: 0.5735, Validation Accuracy: 0.7840\nEpoch 194, Loss: 0.3467, Train Accuracy: 0.8000, Validation Loss: 0.5711, Validation Accuracy: 0.7880\nEpoch 195, Loss: 0.3817, Train Accuracy: 0.8500, Validation Loss: 0.5658, Validation Accuracy: 0.7860\nEpoch 196, Loss: 0.3770, Train Accuracy: 0.7833, Validation Loss: 0.5629, Validation Accuracy: 0.7740\nEpoch 197, Loss: 0.4174, Train Accuracy: 0.7667, Validation Loss: 0.5621, Validation Accuracy: 0.7740\nEpoch 198, Loss: 0.4003, Train Accuracy: 0.8833, Validation Loss: 0.5634, Validation Accuracy: 0.7800\nEpoch 199, Loss: 0.3506, Train Accuracy: 0.8167, Validation Loss: 0.5635, Validation Accuracy: 0.7820\nEpoch 200, Loss: 0.3953, Train Accuracy: 0.8167, Validation Loss: 0.5643, Validation Accuracy: 0.7740\nEpoch 201, Loss: 0.4587, Train Accuracy: 0.7833, Validation Loss: 0.5637, Validation Accuracy: 0.7820\nEpoch 202, Loss: 0.3583, Train Accuracy: 0.8000, Validation Loss: 0.5650, Validation Accuracy: 0.7820\nEpoch 203, Loss: 0.3102, Train Accuracy: 0.8500, Validation Loss: 0.5674, Validation Accuracy: 0.7880\nEpoch 204, Loss: 0.4807, Train Accuracy: 0.7500, Validation Loss: 0.5701, Validation Accuracy: 0.7880\nEpoch 205, Loss: 0.3311, Train Accuracy: 0.7667, Validation Loss: 0.5720, Validation Accuracy: 0.7900\nEpoch 206, Loss: 0.4279, Train Accuracy: 0.7667, Validation Loss: 0.5723, Validation Accuracy: 0.7880\nEpoch 207, Loss: 0.3436, Train Accuracy: 0.8000, Validation Loss: 0.5714, Validation Accuracy: 0.7880\nEpoch 208, Loss: 0.4372, Train Accuracy: 0.8000, Validation Loss: 0.5716, Validation Accuracy: 0.7900\nEpoch 209, Loss: 0.3020, Train Accuracy: 0.8833, Validation Loss: 0.5695, Validation Accuracy: 0.7880\nEpoch 210, Loss: 0.4898, Train Accuracy: 0.7500, Validation Loss: 0.5690, Validation Accuracy: 0.7860\nEpoch 211, Loss: 0.3396, Train Accuracy: 0.8000, Validation Loss: 0.5696, Validation Accuracy: 0.7840\nEpoch 212, Loss: 0.3317, Train Accuracy: 0.7833, Validation Loss: 0.5728, Validation Accuracy: 0.7840\nEpoch 213, Loss: 0.4296, Train Accuracy: 0.7500, Validation Loss: 0.5759, Validation Accuracy: 0.7840\nEpoch 214, Loss: 0.3517, Train Accuracy: 0.8000, Validation Loss: 0.5751, Validation Accuracy: 0.7800\nEpoch 215, Loss: 0.3650, Train Accuracy: 0.8333, Validation Loss: 0.5734, Validation Accuracy: 0.7800\nEpoch 216, Loss: 0.4978, Train Accuracy: 0.7167, Validation Loss: 0.5743, Validation Accuracy: 0.7800\nEpoch 217, Loss: 0.4252, Train Accuracy: 0.8167, Validation Loss: 0.5752, Validation Accuracy: 0.7800\nEpoch 218, Loss: 0.3532, Train Accuracy: 0.8167, Validation Loss: 0.5744, Validation Accuracy: 0.7800\nEpoch 219, Loss: 0.3983, Train Accuracy: 0.7667, Validation Loss: 0.5718, Validation Accuracy: 0.7800\nEpoch 220, Loss: 0.3539, Train Accuracy: 0.8333, Validation Loss: 0.5686, Validation Accuracy: 0.7800\nEpoch 221, Loss: 0.3169, Train Accuracy: 0.8167, Validation Loss: 0.5678, Validation Accuracy: 0.7820\nEpoch 222, Loss: 0.4305, Train Accuracy: 0.7667, Validation Loss: 0.5687, Validation Accuracy: 0.7820\nEpoch 223, Loss: 0.4624, Train Accuracy: 0.6667, Validation Loss: 0.5697, Validation Accuracy: 0.7800\nEpoch 224, Loss: 0.3492, Train Accuracy: 0.7833, Validation Loss: 0.5719, Validation Accuracy: 0.7820\nEpoch 225, Loss: 0.2945, Train Accuracy: 0.8667, Validation Loss: 0.5731, Validation Accuracy: 0.7820\nEpoch 226, Loss: 0.3921, Train Accuracy: 0.8167, Validation Loss: 0.5750, Validation Accuracy: 0.7800\nEpoch 227, Loss: 0.3945, Train Accuracy: 0.7500, Validation Loss: 0.5783, Validation Accuracy: 0.7780\nEpoch 228, Loss: 0.4102, Train Accuracy: 0.8000, Validation Loss: 0.5822, Validation Accuracy: 0.7760\nEpoch 229, Loss: 0.3927, Train Accuracy: 0.8167, Validation Loss: 0.5842, Validation Accuracy: 0.7740\nEpoch 230, Loss: 0.4338, Train Accuracy: 0.6667, Validation Loss: 0.5855, Validation Accuracy: 0.7740\nEpoch 231, Loss: 0.3948, Train Accuracy: 0.8000, Validation Loss: 0.5783, Validation Accuracy: 0.7760\nEpoch 232, Loss: 0.3847, Train Accuracy: 0.7833, Validation Loss: 0.5740, Validation Accuracy: 0.7760\nEpoch 233, Loss: 0.3846, Train Accuracy: 0.7500, Validation Loss: 0.5720, Validation Accuracy: 0.7740\nEpoch 234, Loss: 0.4661, Train Accuracy: 0.7500, Validation Loss: 0.5679, Validation Accuracy: 0.7740\nEpoch 235, Loss: 0.3951, Train Accuracy: 0.7833, Validation Loss: 0.5638, Validation Accuracy: 0.7760\nEpoch 236, Loss: 0.3666, Train Accuracy: 0.8667, Validation Loss: 0.5596, Validation Accuracy: 0.7800\nEpoch 237, Loss: 0.4346, Train Accuracy: 0.8000, Validation Loss: 0.5586, Validation Accuracy: 0.7820\nEpoch 238, Loss: 0.4357, Train Accuracy: 0.7500, Validation Loss: 0.5598, Validation Accuracy: 0.7840\nEpoch 239, Loss: 0.3882, Train Accuracy: 0.8667, Validation Loss: 0.5611, Validation Accuracy: 0.7780\nEpoch 240, Loss: 0.4511, Train Accuracy: 0.7333, Validation Loss: 0.5632, Validation Accuracy: 0.7840\nEpoch 241, Loss: 0.4839, Train Accuracy: 0.6833, Validation Loss: 0.5679, Validation Accuracy: 0.7920\nEpoch 242, Loss: 0.3238, Train Accuracy: 0.8333, Validation Loss: 0.5725, Validation Accuracy: 0.7880\nEpoch 243, Loss: 0.3298, Train Accuracy: 0.9000, Validation Loss: 0.5760, Validation Accuracy: 0.7860\nEpoch 244, Loss: 0.4344, Train Accuracy: 0.7667, Validation Loss: 0.5781, Validation Accuracy: 0.7820\nEpoch 245, Loss: 0.3774, Train Accuracy: 0.7500, Validation Loss: 0.5771, Validation Accuracy: 0.7800\nEpoch 246, Loss: 0.3283, Train Accuracy: 0.8167, Validation Loss: 0.5772, Validation Accuracy: 0.7800\nEpoch 247, Loss: 0.3436, Train Accuracy: 0.8167, Validation Loss: 0.5731, Validation Accuracy: 0.7820\nEpoch 248, Loss: 0.4654, Train Accuracy: 0.7500, Validation Loss: 0.5694, Validation Accuracy: 0.7800\nEpoch 249, Loss: 0.4077, Train Accuracy: 0.7833, Validation Loss: 0.5677, Validation Accuracy: 0.7820\nEpoch 250, Loss: 0.4216, Train Accuracy: 0.7500, Validation Loss: 0.5665, Validation Accuracy: 0.7820\nEpoch 251, Loss: 0.3434, Train Accuracy: 0.8000, Validation Loss: 0.5663, Validation Accuracy: 0.7800\nEpoch 252, Loss: 0.4579, Train Accuracy: 0.7833, Validation Loss: 0.5657, Validation Accuracy: 0.7800\nEpoch 253, Loss: 0.4896, Train Accuracy: 0.7667, Validation Loss: 0.5665, Validation Accuracy: 0.7740\nEpoch 254, Loss: 0.3633, Train Accuracy: 0.8000, Validation Loss: 0.5679, Validation Accuracy: 0.7700\nEpoch 255, Loss: 0.4055, Train Accuracy: 0.8000, Validation Loss: 0.5719, Validation Accuracy: 0.7740\nEpoch 256, Loss: 0.3697, Train Accuracy: 0.8333, Validation Loss: 0.5750, Validation Accuracy: 0.7840\nEpoch 257, Loss: 0.3497, Train Accuracy: 0.7500, Validation Loss: 0.5771, Validation Accuracy: 0.7800\nEpoch 258, Loss: 0.4799, Train Accuracy: 0.7333, Validation Loss: 0.5790, Validation Accuracy: 0.7780\nEpoch 259, Loss: 0.3918, Train Accuracy: 0.7333, Validation Loss: 0.5775, Validation Accuracy: 0.7780\nEpoch 260, Loss: 0.3356, Train Accuracy: 0.8333, Validation Loss: 0.5730, Validation Accuracy: 0.7800\nEpoch 261, Loss: 0.3894, Train Accuracy: 0.7167, Validation Loss: 0.5705, Validation Accuracy: 0.7800\nEpoch 262, Loss: 0.3720, Train Accuracy: 0.7667, Validation Loss: 0.5681, Validation Accuracy: 0.7780\nEpoch 263, Loss: 0.3795, Train Accuracy: 0.8167, Validation Loss: 0.5646, Validation Accuracy: 0.7760\nEpoch 264, Loss: 0.4488, Train Accuracy: 0.7667, Validation Loss: 0.5589, Validation Accuracy: 0.7720\nEpoch 265, Loss: 0.4215, Train Accuracy: 0.8000, Validation Loss: 0.5537, Validation Accuracy: 0.7720\nEpoch 266, Loss: 0.4165, Train Accuracy: 0.8500, Validation Loss: 0.5507, Validation Accuracy: 0.7680\nEpoch 267, Loss: 0.3821, Train Accuracy: 0.8000, Validation Loss: 0.5492, Validation Accuracy: 0.7740\nEpoch 268, Loss: 0.3530, Train Accuracy: 0.8167, Validation Loss: 0.5485, Validation Accuracy: 0.7700\nEpoch 269, Loss: 0.4266, Train Accuracy: 0.7833, Validation Loss: 0.5507, Validation Accuracy: 0.7720\nEpoch 270, Loss: 0.3747, Train Accuracy: 0.7667, Validation Loss: 0.5541, Validation Accuracy: 0.7780\nEpoch 271, Loss: 0.2930, Train Accuracy: 0.8667, Validation Loss: 0.5567, Validation Accuracy: 0.7820\nEpoch 272, Loss: 0.3899, Train Accuracy: 0.7833, Validation Loss: 0.5597, Validation Accuracy: 0.7880\nEpoch 273, Loss: 0.3293, Train Accuracy: 0.8167, Validation Loss: 0.5635, Validation Accuracy: 0.7880\nEpoch 274, Loss: 0.4242, Train Accuracy: 0.7167, Validation Loss: 0.5664, Validation Accuracy: 0.7840\nEpoch 275, Loss: 0.4143, Train Accuracy: 0.8000, Validation Loss: 0.5693, Validation Accuracy: 0.7820\nEpoch 276, Loss: 0.3778, Train Accuracy: 0.8000, Validation Loss: 0.5705, Validation Accuracy: 0.7840\nEpoch 277, Loss: 0.3918, Train Accuracy: 0.7333, Validation Loss: 0.5694, Validation Accuracy: 0.7760\nEpoch 278, Loss: 0.4230, Train Accuracy: 0.7500, Validation Loss: 0.5689, Validation Accuracy: 0.7740\nEpoch 279, Loss: 0.3012, Train Accuracy: 0.8667, Validation Loss: 0.5681, Validation Accuracy: 0.7780\nEpoch 280, Loss: 0.4025, Train Accuracy: 0.7667, Validation Loss: 0.5673, Validation Accuracy: 0.7740\nEpoch 281, Loss: 0.4805, Train Accuracy: 0.7500, Validation Loss: 0.5614, Validation Accuracy: 0.7720\nEpoch 282, Loss: 0.4246, Train Accuracy: 0.7500, Validation Loss: 0.5576, Validation Accuracy: 0.7800\nEpoch 283, Loss: 0.5005, Train Accuracy: 0.7333, Validation Loss: 0.5602, Validation Accuracy: 0.7800\nEpoch 284, Loss: 0.5055, Train Accuracy: 0.7333, Validation Loss: 0.5616, Validation Accuracy: 0.7720\nEpoch 285, Loss: 0.4188, Train Accuracy: 0.7667, Validation Loss: 0.5598, Validation Accuracy: 0.7740\nEpoch 286, Loss: 0.3993, Train Accuracy: 0.7833, Validation Loss: 0.5590, Validation Accuracy: 0.7780\nEpoch 287, Loss: 0.4185, Train Accuracy: 0.7667, Validation Loss: 0.5589, Validation Accuracy: 0.7820\nEpoch 288, Loss: 0.3585, Train Accuracy: 0.7667, Validation Loss: 0.5594, Validation Accuracy: 0.7820\nEpoch 289, Loss: 0.3463, Train Accuracy: 0.7833, Validation Loss: 0.5640, Validation Accuracy: 0.7800\nEpoch 290, Loss: 0.3923, Train Accuracy: 0.7667, Validation Loss: 0.5688, Validation Accuracy: 0.7800\nEpoch 291, Loss: 0.4009, Train Accuracy: 0.7833, Validation Loss: 0.5717, Validation Accuracy: 0.7780\nEpoch 292, Loss: 0.3321, Train Accuracy: 0.7833, Validation Loss: 0.5746, Validation Accuracy: 0.7780\nEpoch 293, Loss: 0.3208, Train Accuracy: 0.8167, Validation Loss: 0.5716, Validation Accuracy: 0.7760\nEpoch 294, Loss: 0.3986, Train Accuracy: 0.8667, Validation Loss: 0.5650, Validation Accuracy: 0.7840\nEpoch 295, Loss: 0.4151, Train Accuracy: 0.8000, Validation Loss: 0.5630, Validation Accuracy: 0.7800\nEpoch 296, Loss: 0.2793, Train Accuracy: 0.8667, Validation Loss: 0.5633, Validation Accuracy: 0.7740\nEpoch 297, Loss: 0.3717, Train Accuracy: 0.7500, Validation Loss: 0.5620, Validation Accuracy: 0.7760\nEpoch 298, Loss: 0.4095, Train Accuracy: 0.7833, Validation Loss: 0.5594, Validation Accuracy: 0.7720\nEpoch 299, Loss: 0.3612, Train Accuracy: 0.8000, Validation Loss: 0.5547, Validation Accuracy: 0.7800\nEpoch 300, Loss: 0.3727, Train Accuracy: 0.7333, Validation Loss: 0.5537, Validation Accuracy: 0.7840\nEpoch 301, Loss: 0.3981, Train Accuracy: 0.7667, Validation Loss: 0.5536, Validation Accuracy: 0.7820\nEpoch 302, Loss: 0.3559, Train Accuracy: 0.8167, Validation Loss: 0.5556, Validation Accuracy: 0.7780\nEpoch 303, Loss: 0.3041, Train Accuracy: 0.8333, Validation Loss: 0.5588, Validation Accuracy: 0.7780\nEpoch 304, Loss: 0.3478, Train Accuracy: 0.7667, Validation Loss: 0.5586, Validation Accuracy: 0.7760\nEpoch 305, Loss: 0.4427, Train Accuracy: 0.7333, Validation Loss: 0.5553, Validation Accuracy: 0.7780\nEpoch 306, Loss: 0.3085, Train Accuracy: 0.8000, Validation Loss: 0.5516, Validation Accuracy: 0.7800\nEpoch 307, Loss: 0.3078, Train Accuracy: 0.9000, Validation Loss: 0.5494, Validation Accuracy: 0.7740\nEpoch 308, Loss: 0.3681, Train Accuracy: 0.7833, Validation Loss: 0.5517, Validation Accuracy: 0.7800\nEpoch 309, Loss: 0.3999, Train Accuracy: 0.8333, Validation Loss: 0.5570, Validation Accuracy: 0.7860\nEpoch 310, Loss: 0.3594, Train Accuracy: 0.8000, Validation Loss: 0.5613, Validation Accuracy: 0.7900\nEpoch 311, Loss: 0.4188, Train Accuracy: 0.7500, Validation Loss: 0.5612, Validation Accuracy: 0.7840\nEpoch 312, Loss: 0.3188, Train Accuracy: 0.8667, Validation Loss: 0.5598, Validation Accuracy: 0.7800\nEpoch 313, Loss: 0.4678, Train Accuracy: 0.7000, Validation Loss: 0.5588, Validation Accuracy: 0.7800\nEpoch 314, Loss: 0.3477, Train Accuracy: 0.7500, Validation Loss: 0.5605, Validation Accuracy: 0.7700\nEpoch 315, Loss: 0.3580, Train Accuracy: 0.8167, Validation Loss: 0.5599, Validation Accuracy: 0.7740\nEpoch 316, Loss: 0.4327, Train Accuracy: 0.7000, Validation Loss: 0.5573, Validation Accuracy: 0.7720\nEpoch 317, Loss: 0.3220, Train Accuracy: 0.8333, Validation Loss: 0.5556, Validation Accuracy: 0.7860\nEpoch 318, Loss: 0.3224, Train Accuracy: 0.8000, Validation Loss: 0.5569, Validation Accuracy: 0.7740\nEpoch 319, Loss: 0.3991, Train Accuracy: 0.8167, Validation Loss: 0.5621, Validation Accuracy: 0.7800\nEpoch 320, Loss: 0.3898, Train Accuracy: 0.8333, Validation Loss: 0.5661, Validation Accuracy: 0.7800\nEpoch 321, Loss: 0.3486, Train Accuracy: 0.7667, Validation Loss: 0.5682, Validation Accuracy: 0.7820\nEpoch 322, Loss: 0.3662, Train Accuracy: 0.8000, Validation Loss: 0.5698, Validation Accuracy: 0.7860\nEpoch 323, Loss: 0.4586, Train Accuracy: 0.7000, Validation Loss: 0.5703, Validation Accuracy: 0.7860\nEpoch 324, Loss: 0.3080, Train Accuracy: 0.8333, Validation Loss: 0.5718, Validation Accuracy: 0.7820\nEpoch 325, Loss: 0.3455, Train Accuracy: 0.8000, Validation Loss: 0.5720, Validation Accuracy: 0.7860\nEpoch 326, Loss: 0.3646, Train Accuracy: 0.8167, Validation Loss: 0.5688, Validation Accuracy: 0.7840\nEpoch 327, Loss: 0.4154, Train Accuracy: 0.8000, Validation Loss: 0.5662, Validation Accuracy: 0.7780\nEpoch 328, Loss: 0.3653, Train Accuracy: 0.8000, Validation Loss: 0.5646, Validation Accuracy: 0.7780\nEpoch 329, Loss: 0.3904, Train Accuracy: 0.7667, Validation Loss: 0.5621, Validation Accuracy: 0.7780\nEpoch 330, Loss: 0.3766, Train Accuracy: 0.7667, Validation Loss: 0.5615, Validation Accuracy: 0.7780\nEpoch 331, Loss: 0.3737, Train Accuracy: 0.8167, Validation Loss: 0.5627, Validation Accuracy: 0.7780\nEpoch 332, Loss: 0.3745, Train Accuracy: 0.7500, Validation Loss: 0.5644, Validation Accuracy: 0.7800\nEpoch 333, Loss: 0.3507, Train Accuracy: 0.7667, Validation Loss: 0.5666, Validation Accuracy: 0.7900\nEpoch 334, Loss: 0.3714, Train Accuracy: 0.7667, Validation Loss: 0.5667, Validation Accuracy: 0.7840\nEpoch 335, Loss: 0.3716, Train Accuracy: 0.8500, Validation Loss: 0.5637, Validation Accuracy: 0.7800\nEpoch 336, Loss: 0.3438, Train Accuracy: 0.8167, Validation Loss: 0.5636, Validation Accuracy: 0.7820\nEpoch 337, Loss: 0.3048, Train Accuracy: 0.8500, Validation Loss: 0.5642, Validation Accuracy: 0.7820\nEpoch 338, Loss: 0.3441, Train Accuracy: 0.8167, Validation Loss: 0.5634, Validation Accuracy: 0.7800\nEpoch 339, Loss: 0.3627, Train Accuracy: 0.8000, Validation Loss: 0.5605, Validation Accuracy: 0.7760\nEpoch 340, Loss: 0.4438, Train Accuracy: 0.7333, Validation Loss: 0.5558, Validation Accuracy: 0.7780\nEpoch 341, Loss: 0.3646, Train Accuracy: 0.7500, Validation Loss: 0.5538, Validation Accuracy: 0.7760\nEpoch 342, Loss: 0.4202, Train Accuracy: 0.7167, Validation Loss: 0.5537, Validation Accuracy: 0.7740\nEpoch 343, Loss: 0.3637, Train Accuracy: 0.7333, Validation Loss: 0.5556, Validation Accuracy: 0.7780\nEpoch 344, Loss: 0.3296, Train Accuracy: 0.8167, Validation Loss: 0.5591, Validation Accuracy: 0.7780\nEpoch 345, Loss: 0.3471, Train Accuracy: 0.8000, Validation Loss: 0.5629, Validation Accuracy: 0.7780\nEpoch 346, Loss: 0.3440, Train Accuracy: 0.8333, Validation Loss: 0.5683, Validation Accuracy: 0.7800\nEpoch 347, Loss: 0.4059, Train Accuracy: 0.7000, Validation Loss: 0.5745, Validation Accuracy: 0.7760\nEpoch 348, Loss: 0.2343, Train Accuracy: 0.9000, Validation Loss: 0.5783, Validation Accuracy: 0.7760\nEpoch 349, Loss: 0.3557, Train Accuracy: 0.8167, Validation Loss: 0.5797, Validation Accuracy: 0.7800\nEpoch 350, Loss: 0.4607, Train Accuracy: 0.7833, Validation Loss: 0.5722, Validation Accuracy: 0.7780\nEpoch 351, Loss: 0.3782, Train Accuracy: 0.7500, Validation Loss: 0.5622, Validation Accuracy: 0.7800\nEpoch 352, Loss: 0.3823, Train Accuracy: 0.8833, Validation Loss: 0.5534, Validation Accuracy: 0.7760\nEpoch 353, Loss: 0.3877, Train Accuracy: 0.7833, Validation Loss: 0.5500, Validation Accuracy: 0.7700\nEpoch 354, Loss: 0.3396, Train Accuracy: 0.7833, Validation Loss: 0.5486, Validation Accuracy: 0.7720\nEpoch 355, Loss: 0.4846, Train Accuracy: 0.6833, Validation Loss: 0.5485, Validation Accuracy: 0.7760\nEpoch 356, Loss: 0.3852, Train Accuracy: 0.7667, Validation Loss: 0.5504, Validation Accuracy: 0.7720\nEpoch 357, Loss: 0.3951, Train Accuracy: 0.8167, Validation Loss: 0.5533, Validation Accuracy: 0.7760\nEpoch 358, Loss: 0.3159, Train Accuracy: 0.8167, Validation Loss: 0.5576, Validation Accuracy: 0.7800\nEpoch 359, Loss: 0.3256, Train Accuracy: 0.8167, Validation Loss: 0.5646, Validation Accuracy: 0.7820\nEpoch 360, Loss: 0.4001, Train Accuracy: 0.7500, Validation Loss: 0.5710, Validation Accuracy: 0.7800\nEpoch 361, Loss: 0.2849, Train Accuracy: 0.8333, Validation Loss: 0.5768, Validation Accuracy: 0.7760\nEpoch 362, Loss: 0.3657, Train Accuracy: 0.8167, Validation Loss: 0.5775, Validation Accuracy: 0.7860\nEpoch 363, Loss: 0.2961, Train Accuracy: 0.8000, Validation Loss: 0.5744, Validation Accuracy: 0.7840\nEpoch 364, Loss: 0.3245, Train Accuracy: 0.8833, Validation Loss: 0.5711, Validation Accuracy: 0.7780\nEpoch 365, Loss: 0.3009, Train Accuracy: 0.8167, Validation Loss: 0.5681, Validation Accuracy: 0.7780\nEpoch 366, Loss: 0.2987, Train Accuracy: 0.8333, Validation Loss: 0.5646, Validation Accuracy: 0.7820\nEpoch 367, Loss: 0.3819, Train Accuracy: 0.7833, Validation Loss: 0.5616, Validation Accuracy: 0.7780\nEpoch 368, Loss: 0.3525, Train Accuracy: 0.8167, Validation Loss: 0.5609, Validation Accuracy: 0.7760\nEarly stopping triggered.\nBest model loaded from best_model.pth\nTest Accuracy: 0.7890\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}